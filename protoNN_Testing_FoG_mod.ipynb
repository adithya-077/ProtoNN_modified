{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kwwtn5iEQJ1M"
   },
   "source": [
    "# ProtoNN in Tensorflow\n",
    "\n",
    "This is a simple notebook that illustrates the usage of Tensorflow implementation of ProtoNN. We are using the USPS dataset. Please refer to `fetch_usps.py` for more details on downloading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:06:10.223951Z",
     "start_time": "2018-08-15T13:06:09.303454Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "dJBVr2b7QJ1R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nadit\\anaconda3\\envs\\ProtoNN\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT license.\n",
    "\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#sys.path.insert(0, '../../')\n",
    "# from edgeml.trainer.protoNNTrainer import ProtoNNTrainer\n",
    "# from edgeml.graph.protoNN import ProtoNN\n",
    "# import edgeml.utils as utils\n",
    "# import helpermethods as helper\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "sys.path.append(r\"E:\\programming\\practice\\research\\optimized code\\EdgeML\\examples\\tf\\ProtoNN\")\n",
    "import helpermethods as helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETER\n",
    "hyper = {'REG_W': 2.0012967740413277e-06,\n",
    " 'REG_B': 2.0861588167171992e-05,\n",
    " 'REG_Z': 2.0478935633536105e-05,\n",
    " 'SPAR_W': 0.8367487762320901,\n",
    " 'SPAR_B': 0.9791350117492328,\n",
    " 'SPAR_Z': 0.9505125522850648,\n",
    " 'LEARNING_RATE': 0.00012654644856451654,\n",
    " 'NUM_EPOCHS': 355}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mxqvfwWQtQ-s"
   },
   "source": [
    "# Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cT-KokQSQiS6"
   },
   "outputs": [],
   "source": [
    "#helper methods\n",
    "sys.path.insert(0, '../')\n",
    "import argparse\n",
    "\n",
    "\n",
    "def getModelSize(matrixList, sparcityList, expected=True, bytesPerVar=4):\n",
    "    '''\n",
    "    expected: Expected size according to the parameters set. The number of\n",
    "        zeros could actually be more than that is required to satisfy the\n",
    "        sparsity constraint.\n",
    "    '''\n",
    "    nnzList, sizeList, isSparseList = [], [], []\n",
    "    hasSparse = False\n",
    "    for i in range(len(matrixList)):\n",
    "        A, s = matrixList[i], sparcityList[i]\n",
    "        assert A.ndim == 2\n",
    "        assert s >= 0\n",
    "        assert s <= 1\n",
    "        nnz, size, sparse = countnnZ(A, s, bytesPerVar=bytesPerVar)\n",
    "        nnzList.append(nnz)\n",
    "        sizeList.append(size)\n",
    "        hasSparse = (hasSparse or sparse)\n",
    "\n",
    "    totalnnZ = np.sum(nnzList)\n",
    "    totalSize = np.sum(sizeList)\n",
    "    if expected:\n",
    "        return totalnnZ, totalSize, hasSparse\n",
    "    numNonZero = 0\n",
    "    totalSize = 0\n",
    "    hasSparse = False\n",
    "    for i in range(len(matrixList)):\n",
    "        A, s = matrixList[i], sparcityList[i]\n",
    "        numNonZero_ = np.count_nonzero(A)\n",
    "        numNonZero += numNonZero_\n",
    "        hasSparse = (hasSparse or (s < 0.5))\n",
    "        if s <= 0.5:\n",
    "            totalSize += numNonZero_ * 2 * bytesPerVar\n",
    "        else:\n",
    "            totalSize += A.size * bytesPerVar\n",
    "    return numNonZero, totalSize, hasSparse\n",
    "\n",
    "\n",
    "def getGamma(gammaInit, projectionDim, dataDim, numPrototypes, x_train):\n",
    "    if gammaInit is None:\n",
    "        print(\"Using median heuristic to estimate gamma.\")\n",
    "        gamma, W, B = medianHeuristic(x_train, projectionDim,\n",
    "                                            numPrototypes)\n",
    "        print(\"Gamma estimate is: %f\" % gamma)\n",
    "        return W, B, gamma\n",
    "    return None, None, gammaInit\n",
    "\n",
    "\n",
    "def preprocessData(dataDir,w):\n",
    "    '''\n",
    "    Loads data from the dataDir and does some initial preprocessing\n",
    "    steps. Data is assumed to be contained in two files,\n",
    "    train.npy and test.npy. Each containing a 2D numpy array of dimension\n",
    "    [numberOfExamples, numberOfFeatures + 1]. The first column of each\n",
    "    matrix is assumed to contain label information.\n",
    "\n",
    "    For an N-Class problem, we assume the labels are integers from 0 through\n",
    "    N-1.\n",
    "    '''\n",
    "    # Uncomment for usual training data\n",
    "    # train = np.load(dataDir + '/train_'+str(w)+'.npy')\n",
    "    # test = np.load(dataDir + '/test_'+str(w)+'.npy')\n",
    "    # Uncomment for time domain training data\n",
    "    train = np.load(dataDir + '/ttrain_'+str(w)+'.npy')\n",
    "    test = np.load(dataDir + '/ttest_'+str(w)+'.npy')\n",
    "    # Uncomment for 1 sensordrop training data\n",
    "    # train = np.load(dataDir + '/train_'+str(w)+'.npy')\n",
    "    # test = np.load(dataDir + '/test_'+str(w)+'.npy')\n",
    "\n",
    "    dataDimension = int(train.shape[1]) - 1\n",
    "    x_train = train[:, 1:dataDimension + 1]\n",
    "    y_train_ = train[:, 0]\n",
    "    x_test = test[:, 1:dataDimension + 1]\n",
    "    y_test_ = test[:, 0]\n",
    "\n",
    "    numClasses = max(y_train_) - min(y_train_) + 1\n",
    "    numClasses = max(numClasses, max(y_test_) - min(y_test_) + 1)\n",
    "    numClasses = int(numClasses)\n",
    "\n",
    "    # mean-var\n",
    "    mean = np.mean(x_train, 0)\n",
    "    std = np.std(x_train, 0)\n",
    "    std[std[:] < 0.000001] = 1\n",
    "    x_train = (x_train - mean) / std\n",
    "    x_test = (x_test - mean) / std\n",
    "\n",
    "    # one hot y-train\n",
    "    lab = y_train_.astype('uint8')\n",
    "    lab = np.array(lab) - min(lab)\n",
    "    lab_ = np.zeros((x_train.shape[0], numClasses))\n",
    "    lab_[np.arange(x_train.shape[0]), lab] = 1\n",
    "    y_train = lab_\n",
    "\n",
    "    # one hot y-test\n",
    "    lab = y_test_.astype('uint8')\n",
    "    lab = np.array(lab) - min(lab)\n",
    "    lab_ = np.zeros((x_test.shape[0], numClasses))\n",
    "    lab_[np.arange(x_test.shape[0]), lab] = 1\n",
    "    y_test = lab_\n",
    "\n",
    "    return dataDimension, numClasses, x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "def getProtoNNArgs():\n",
    "    def checkIntPos(value):\n",
    "        ivalue = int(value)\n",
    "        if ivalue <= 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid positive int value\" % value)\n",
    "        return ivalue\n",
    "\n",
    "    def checkIntNneg(value):\n",
    "        ivalue = int(value)\n",
    "        if ivalue < 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid non-neg int value\" % value)\n",
    "        return ivalue\n",
    "\n",
    "    def checkFloatNneg(value):\n",
    "        fvalue = float(value)\n",
    "        if fvalue < 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid non-neg float value\" % value)\n",
    "        return fvalue\n",
    "\n",
    "    def checkFloatPos(value):\n",
    "        fvalue = float(value)\n",
    "        if fvalue <= 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid positive float value\" % value)\n",
    "        return fvalue\n",
    "\n",
    "    '''\n",
    "    Parse protoNN commandline arguments\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Hyperparameters for ProtoNN Algorithm')\n",
    "\n",
    "    msg = 'Data directory containing train and test data. The '\n",
    "    msg += 'data is assumed to be saved as 2-D numpy matrices with '\n",
    "    msg += 'names `train.npy` and `test.npy`, of dimensions\\n'\n",
    "    msg += '\\t[numberOfInstances, numberOfFeatures + 1].\\n'\n",
    "    msg += 'The first column of each file is assumed to contain label information.'\n",
    "    msg += ' For a N-class problem, labels are assumed to be integers from 0 to'\n",
    "    msg += ' N-1 (inclusive).'\n",
    "    parser.add_argument('-d', '--data-dir', required=True, help=msg)\n",
    "    parser.add_argument('-l', '--projection-dim', type=checkIntPos, default=10,\n",
    "                        help='Projection Dimension.')\n",
    "    parser.add_argument('-p', '--num-prototypes', type=checkIntPos, default=20,\n",
    "                        help='Number of prototypes.')\n",
    "    parser.add_argument('-g', '--gamma', type=checkFloatPos, default=None,\n",
    "                        help='Gamma for Gaussian kernel. If not provided, ' +\n",
    "                        'median heuristic will be used to estimate gamma.')\n",
    "\n",
    "    parser.add_argument('-e', '--epochs', type=checkIntPos, default=100,\n",
    "                        help='Total training epochs.')\n",
    "    parser.add_argument('-b', '--batch-size', type=checkIntPos, default=32,\n",
    "                        help='Batch size for each pass.')\n",
    "    parser.add_argument('-r', '--learning-rate', type=checkFloatPos,\n",
    "                        default=0.001,\n",
    "                        help='Initial Learning rate for ADAM Optimizer.')\n",
    "\n",
    "    parser.add_argument('-rW', type=float, default=0.000,\n",
    "                        help='Coefficient for l2 regularizer for predictor' +\n",
    "                        ' parameter W ' + '(default = 0.0).')\n",
    "    parser.add_argument('-rB', type=float, default=0.00,\n",
    "                        help='Coefficient for l2 regularizer for predictor' +\n",
    "                        ' parameter B ' + '(default = 0.0).')\n",
    "    parser.add_argument('-rZ', type=float, default=0.00,\n",
    "                        help='Coefficient for l2 regularizer for predictor' +\n",
    "                        'parameter Z ' +\n",
    "                        '(default = 0.0).')\n",
    "\n",
    "    parser.add_argument('-sW', type=float, default=1.000,\n",
    "                        help='Sparsity constraint for predictor parameter W ' +\n",
    "                        '(default = 1.0, i.e. dense matrix).')\n",
    "    parser.add_argument('-sB', type=float, default=1.00,\n",
    "                        help='Sparsity constraint for predictor parameter B ' +\n",
    "                        '(default = 1.0, i.e. dense matrix).')\n",
    "    parser.add_argument('-sZ', type=float, default=1.00,\n",
    "                        help='Sparsity constraint for predictor parameter Z ' +\n",
    "                        '(default = 1.0, i.e. dense matrix).')\n",
    "    parser.add_argument('-pS', '--print-step', type=int, default=200,\n",
    "                        help='The number of update steps between print ' +\n",
    "                        'calls to console.')\n",
    "    parser.add_argument('-vS', '--val-step', type=int, default=3,\n",
    "                        help='The number of epochs between validation' +\n",
    "                        'performance evaluation')\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ou1MfKhYtMdT"
   },
   "source": [
    "# Utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jDVo_0JiRSi9"
   },
   "outputs": [],
   "source": [
    "#utils\n",
    "import scipy.cluster\n",
    "import scipy.spatial\n",
    "import os\n",
    "\n",
    "\n",
    "def medianHeuristic(data, projectionDimension, numPrototypes, W_init=None):\n",
    "    '''\n",
    "    This method can be used to estimate gamma for ProtoNN. An approximation to\n",
    "    median heuristic is used here.\n",
    "    1. First the data is collapsed into the projectionDimension by W_init. If\n",
    "    W_init is not provided, it is initialized from a random normal(0, 1). Hence\n",
    "    data normalization is essential.\n",
    "    2. Prototype are computed by running a  k-means clustering on the projected\n",
    "    data.\n",
    "    3. The median distance is then estimated by calculating median distance\n",
    "    between prototypes and projected data points.\n",
    "\n",
    "    data needs to be [-1, numFeats]\n",
    "    If using this method to initialize gamma, please use the W and B as well.\n",
    "\n",
    "    TODO: Return estimate of Z (prototype labels) based on cluster centroids\n",
    "    andand labels\n",
    "\n",
    "    TODO: Clustering fails due to singularity error if projecting upwards\n",
    "\n",
    "    W [dxd_cap]\n",
    "    B [d_cap, m]\n",
    "    returns gamma, W, B\n",
    "    '''\n",
    "    assert data.ndim == 2\n",
    "    X = data\n",
    "    featDim = data.shape[1]\n",
    "    if projectionDimension > featDim:\n",
    "        print(\"Warning: Projection dimension > feature dimension. Gamma\")\n",
    "        print(\"\\t estimation due to median heuristic could fail.\")\n",
    "        print(\"\\tTo retain the projection dataDimension, provide\")\n",
    "        print(\"\\ta value for gamma.\")\n",
    "\n",
    "    if W_init is None:\n",
    "        W_init = np.random.normal(size=[featDim, projectionDimension])\n",
    "    W = W_init\n",
    "    XW = np.matmul(X, W)\n",
    "    assert XW.shape[1] == projectionDimension\n",
    "    assert XW.shape[0] == len(X)\n",
    "    # Requires [N x d_cap] data matrix of N observations of d_cap-dimension and\n",
    "    # the number of centroids m. Returns, [n x d_cap] centroids and\n",
    "    # elementwise center information.\n",
    "    B, centers = scipy.cluster.vq.kmeans2(XW, numPrototypes)\n",
    "    # Requires two matrices. Number of observations x dimension of observation\n",
    "    # space. Distances[i,j] is the distance between XW[i] and B[j]\n",
    "    distances = scipy.spatial.distance.cdist(XW, B, metric='euclidean')\n",
    "    distances = np.reshape(distances, [-1])\n",
    "    gamma = np.median(distances)\n",
    "    gamma = 1 / (2.5 * gamma)\n",
    "    return gamma.astype('float32'), W.astype('float32'), B.T.astype('float32')\n",
    "\n",
    "\n",
    "def multiClassHingeLoss(logits, label, batch_th):\n",
    "    '''\n",
    "    MultiClassHingeLoss to match C++ Version - No TF internal version\n",
    "    '''\n",
    "    flatLogits = tf.reshape(logits, [-1, ])\n",
    "    label_ = tf.argmax(label, 1)\n",
    "\n",
    "    correctId = tf.range(0, batch_th) * label.shape[1] + label_\n",
    "    correctLogit = tf.gather(flatLogits, correctId)\n",
    "\n",
    "    maxLabel = tf.argmax(logits, 1)\n",
    "    top2, _ = tf.nn.top_k(logits, k=2, sorted=True)\n",
    "\n",
    "    wrongMaxLogit = tf.where(\n",
    "        tf.equal(maxLabel, label_), top2[:, 1], top2[:, 0])\n",
    "\n",
    "    return tf.reduce_mean(tf.nn.relu(1. + wrongMaxLogit - correctLogit))\n",
    "\n",
    "\n",
    "def crossEntropyLoss(logits, label):\n",
    "    '''\n",
    "    Cross Entropy loss for MultiClass case in joint training for\n",
    "    faster convergence\n",
    "    '''\n",
    "    return tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                   labels=tf.stop_gradient(label)))\n",
    "\n",
    "\n",
    "def mean_absolute_error(logits, label):\n",
    "    '''\n",
    "    Function to compute the mean absolute error.\n",
    "    '''\n",
    "    return tf.reduce_mean(tf.abs(tf.subtract(logits, label)))\n",
    "\n",
    "\n",
    "def hardThreshold(A, s):\n",
    "    '''\n",
    "    Hard thresholding function on Tensor A with sparsity s\n",
    "    '''\n",
    "    A_ = np.copy(A)\n",
    "    A_ = A_.ravel()\n",
    "    if len(A_) > 0:\n",
    "        th = np.percentile(np.abs(A_), (1 - s) * 100.0, interpolation='higher')\n",
    "        A_[np.abs(A_) < th] = 0.0\n",
    "    A_ = A_.reshape(A.shape)\n",
    "    return A_\n",
    "\n",
    "\n",
    "def copySupport(src, dest):\n",
    "    '''\n",
    "    copy support of src tensor to dest tensor\n",
    "    '''\n",
    "    support = np.nonzero(src)\n",
    "    dest_ = dest\n",
    "    dest = np.zeros(dest_.shape)\n",
    "    dest[support] = dest_[support]\n",
    "    return dest\n",
    "\n",
    "\n",
    "def countnnZ(A, s, bytesPerVar=4):\n",
    "    '''\n",
    "    Returns # of non-zeros and representative size of the tensor\n",
    "    Uses dense for s >= 0.5 - 4 byte\n",
    "    Else uses sparse - 8 byte\n",
    "    '''\n",
    "    params = 1\n",
    "    hasSparse = False\n",
    "    for i in range(0, len(A.shape)):\n",
    "        params *= int(A.shape[i])\n",
    "    if s < 0.5:\n",
    "        nnZ = np.ceil(params * s)\n",
    "        hasSparse = True\n",
    "        return nnZ, nnZ * 2 * bytesPerVar, hasSparse\n",
    "    else:\n",
    "        nnZ = params\n",
    "        return nnZ, nnZ * bytesPerVar, hasSparse\n",
    "\n",
    "\n",
    "def getConfusionMatrix(predicted, target, numClasses):\n",
    "    '''\n",
    "    Returns a confusion matrix for a multiclass classification\n",
    "    problem. `predicted` is a 1-D array of integers representing\n",
    "    the predicted classes and `target` is the target classes.\n",
    "\n",
    "    confusion[i][j]: Number of elements of class j\n",
    "        predicted as class i\n",
    "    Labels are assumed to be in range(0, numClasses)\n",
    "    Use`printFormattedConfusionMatrix` to echo the confusion matrix\n",
    "    in a user friendly form.\n",
    "    '''\n",
    "    assert(predicted.ndim == 1)\n",
    "    assert(target.ndim == 1)\n",
    "    arr = np.zeros([numClasses, numClasses])\n",
    "\n",
    "    for i in range(len(predicted)):\n",
    "        arr[predicted[i]][target[i]] += 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "def printFormattedConfusionMatrix(matrix):\n",
    "    '''\n",
    "    Given a 2D confusion matrix, prints it in a human readable way.\n",
    "    The confusion matrix is expected to be a 2D numpy array with\n",
    "    square dimensions\n",
    "    '''\n",
    "    assert(matrix.ndim == 2)\n",
    "    assert(matrix.shape[0] == matrix.shape[1])\n",
    "    RECALL = 'Recall'\n",
    "    PRECISION = 'PRECISION'\n",
    "    print(\"|%s|\" % ('True->'), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%7d|\" % i, end='')\n",
    "    print(\"%s|\" % 'Precision')\n",
    "\n",
    "    print(\"|%s|\" % ('-' * len(RECALL)), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%s|\" % ('-' * 7), end='')\n",
    "    print(\"%s|\" % ('-' * len(PRECISION)))\n",
    "\n",
    "    precisionlist = np.sum(matrix, axis=1)\n",
    "    recalllist = np.sum(matrix, axis=0)\n",
    "    precisionlist = [matrix[i][i] / x if x !=\n",
    "                     0 else -1 for i, x in enumerate(precisionlist)]\n",
    "    recalllist = [matrix[i][i] / x if x !=\n",
    "                  0 else -1 for i, x in enumerate(recalllist)]\n",
    "    for i in range(matrix.shape[0]):\n",
    "        # len recall = 6\n",
    "        print(\"|%6d|\" % (i), end='')\n",
    "        for j in range(matrix.shape[0]):\n",
    "            print(\"%7d|\" % (matrix[i][j]), end='')\n",
    "        print(\"%s\" % (\" \" * (len(PRECISION) - 7)), end='')\n",
    "        if precisionlist[i] != -1:\n",
    "            print(\"%1.5f|\" % precisionlist[i])\n",
    "        else:\n",
    "            print(\"%7s|\" % \"nan\")\n",
    "\n",
    "    print(\"|%s|\" % ('-' * len(RECALL)), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%s|\" % ('-' * 7), end='')\n",
    "    print(\"%s|\" % ('-' * len(PRECISION)))\n",
    "    print(\"|%s|\" % ('Recall'), end='')\n",
    "\n",
    "    for i in range(matrix.shape[0]):\n",
    "        if recalllist[i] != -1:\n",
    "            print(\"%1.5f|\" % (recalllist[i]), end='')\n",
    "        else:\n",
    "            print(\"%7s|\" % \"nan\", end='')\n",
    "\n",
    "    print('%s|' % (' ' * len(PRECISION)))\n",
    "\n",
    "\n",
    "def getPrecisionRecall(cmatrix, label=1):\n",
    "    trueP = cmatrix[label][label]\n",
    "    denom = np.sum(cmatrix, axis=0)[label]\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    recall = trueP / denom\n",
    "    denom = np.sum(cmatrix, axis=1)[label]\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    precision = trueP / denom\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMacroPrecisionRecall(cmatrix):\n",
    "    # TP + FP\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    # TP + FN\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    precisionlist__ = [cmatrix[i][i] / x if x !=\n",
    "                       0 else 0 for i, x in enumerate(precisionlist)]\n",
    "    recalllist__ = [cmatrix[i][i] / x if x !=\n",
    "                    0 else 0 for i, x in enumerate(recalllist)]\n",
    "    precision = np.sum(precisionlist__)\n",
    "    precision /= len(precisionlist__)\n",
    "    recall = np.sum(recalllist__)\n",
    "    recall /= len(recalllist__)\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMicroPrecisionRecall(cmatrix):\n",
    "    # TP + FP\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    # TP + FN\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    num = 0.0\n",
    "    for i in range(len(cmatrix)):\n",
    "        num += cmatrix[i][i]\n",
    "\n",
    "    precision = num / np.sum(precisionlist)\n",
    "    recall = num / np.sum(recalllist)\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMacroMicroFScore(cmatrix):\n",
    "    '''\n",
    "    Returns macro and micro f-scores.\n",
    "    Refer: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.104.8244&rep=rep1&type=pdf\n",
    "    '''\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    precisionlist__ = [cmatrix[i][i] / x if x !=\n",
    "                       0 else 0 for i, x in enumerate(precisionlist)]\n",
    "    recalllist__ = [cmatrix[i][i] / x if x !=\n",
    "                    0 else 0 for i, x in enumerate(recalllist)]\n",
    "    macro = 0.0\n",
    "    for i in range(len(precisionlist)):\n",
    "        denom = precisionlist__[i] + recalllist__[i]\n",
    "        numer = precisionlist__[i] * recalllist__[i] * 2\n",
    "        if denom == 0:\n",
    "            denom = 1\n",
    "        macro += numer / denom\n",
    "    macro /= len(precisionlist)\n",
    "\n",
    "    num = 0.0\n",
    "    for i in range(len(precisionlist)):\n",
    "        num += cmatrix[i][i]\n",
    "\n",
    "    denom1 = np.sum(precisionlist)\n",
    "    denom2 = np.sum(recalllist)\n",
    "    pi = num / denom1\n",
    "    rho = num / denom2\n",
    "    denom = pi + rho\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    micro = 2 * pi * rho / denom\n",
    "    return macro, micro\n",
    "\n",
    "\n",
    "class GraphManager:\n",
    "    '''\n",
    "    Manages saving and restoring graphs. Designed to be used with EMI-RNN\n",
    "    though is general enough to be useful otherwise as well.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def checkpointModel(self, saver, sess, modelPrefix,\n",
    "                        globalStep=1000, redirFile=None):\n",
    "        saver.save(sess, modelPrefix, global_step=globalStep)\n",
    "        print('Model saved to %s, global_step %d' % (modelPrefix, globalStep),\n",
    "              file=redirFile)\n",
    "\n",
    "    def loadCheckpoint(self, sess, modelPrefix, globalStep,\n",
    "                       redirFile=None):\n",
    "        metaname = modelPrefix + '-%d.meta' % globalStep\n",
    "        basename = os.path.basename(metaname)\n",
    "        fileList = os.listdir(os.path.dirname(modelPrefix))\n",
    "        fileList = [x for x in fileList if x.startswith(basename)]\n",
    "        assert len(fileList) > 0, 'Checkpoint file not found'\n",
    "        msg = 'Too many or too few checkpoint files for globalStep: %d' % globalStep\n",
    "        assert len(fileList) is 1, msg\n",
    "        chkpt = basename + '/' + fileList[0]\n",
    "        saver = tf.train.import_meta_graph(metaname)\n",
    "        metaname = metaname[:-5]\n",
    "        saver.restore(sess, metaname)\n",
    "        graph = tf.get_default_graph()\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DAjSVSOFtFmm"
   },
   "source": [
    "# Model Trainer - ProtoNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bp5dEFiZR_sy"
   },
   "outputs": [],
   "source": [
    "#Trainer\n",
    "class ProtoNNTrainer:\n",
    "    def __init__(self, protoNNObj, regW, regB, regZ,\n",
    "                 sparcityW, sparcityB, sparcityZ,\n",
    "                 learningRate, X, Y, lossType='l2'):\n",
    "        '''\n",
    "        A wrapper for the various techniques used for training ProtoNN. This\n",
    "        subsumes both the responsibility of loss graph construction and\n",
    "        performing training. The original training routine that is part of the\n",
    "        C++ implementation of EdgeML used iterative hard thresholding (IHT),\n",
    "        gamma estimation through median heuristic and other tricks for\n",
    "        training ProtoNN. This module implements the same in Tensorflow\n",
    "        and python.\n",
    "\n",
    "        protoNNObj: An instance of ProtoNN class defining the forward\n",
    "            computation graph. The loss functions and training routines will be\n",
    "            attached to this instance.\n",
    "        regW, regB, regZ: Regularization constants for W, B, and\n",
    "            Z matrices of protoNN.\n",
    "        sparcityW, sparcityB, sparcityZ: Sparsity constraints\n",
    "            for W, B and Z matrices. A value between 0 (exclusive) and 1\n",
    "            (inclusive) is expected. A value of 1 indicates dense training.\n",
    "        learningRate: Initial learning rate for ADAM optimizer.\n",
    "        X, Y : Placeholders for data and labels.\n",
    "            X [-1, featureDimension]\n",
    "            Y [-1, num Labels]\n",
    "        lossType: ['l2', 'xentropy']\n",
    "        '''\n",
    "        self.protoNNObj = protoNNObj\n",
    "        self.__regW = regW\n",
    "        self.__regB = regB\n",
    "        self.__regZ = regZ\n",
    "        self.__sW = sparcityW\n",
    "        self.__sB = sparcityB\n",
    "        self.__sZ = sparcityZ\n",
    "        self.__lR = learningRate\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.sparseTraining = True\n",
    "        if (sparcityW == 1.0) and (sparcityB == 1.0) and (sparcityZ == 1.0):\n",
    "            self.sparseTraining = False\n",
    "            print(\"Sparse training disabled.\", file=sys.stderr)\n",
    "        # Define placeholders for sparse training\n",
    "        self.W_th = None\n",
    "        self.B_th = None\n",
    "        self.Z_th = None\n",
    "        self.__lossType = lossType\n",
    "        self.__validInit = False\n",
    "        self.__validInit = self.__validateInit()\n",
    "        self.__protoNNOut = protoNNObj(X, Y)\n",
    "        self.loss = self.__lossGraph()\n",
    "        self.trainStep = self.__trainGraph()\n",
    "        self.__hthOp = self.__getHardThresholdOp()\n",
    "        self.accuracy = protoNNObj.getAccuracyOp()\n",
    "\n",
    "    def __validateInit(self):\n",
    "        self.__validInit = False\n",
    "        msg = \"Sparsity value should be between\"\n",
    "        msg += \" 0 and 1 (both inclusive).\"\n",
    "        assert self.__sW >= 0. and self.__sW <= 1., 'W:' + msg\n",
    "        assert self.__sB >= 0. and self.__sB <= 1., 'B:' + msg\n",
    "        assert self.__sZ >= 0. and self.__sZ <= 1., 'Z:' + msg\n",
    "        d, dcap, m, L, _ = self.protoNNObj.getHyperParams()\n",
    "        msg = 'Y should be of dimension [-1, num labels/classes]'\n",
    "        msg += ' specified as part of ProtoNN object.'\n",
    "        assert (len(self.Y.shape)) == 2, msg\n",
    "        assert (self.Y.shape[1] == L), msg\n",
    "        msg = 'X should be of dimension [-1, featureDimension]'\n",
    "        msg += ' specified as part of ProtoNN object.'\n",
    "        assert (len(self.X.shape) == 2), msg\n",
    "        assert (self.X.shape[1] == d), msg\n",
    "        self.__validInit = True\n",
    "        msg = 'Values can be \\'l2\\', or \\'xentropy\\''\n",
    "        if self.__lossType not in ['l2', 'xentropy']:\n",
    "            raise ValueError(msg)\n",
    "        return True\n",
    "\n",
    "    def __lossGraph(self):\n",
    "        pnnOut = self.__protoNNOut\n",
    "        l1, l2, l3 = self.__regW, self.__regB, self.__regZ\n",
    "        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n",
    "        if self.__lossType == 'l2':\n",
    "            with tf.name_scope('protonn-l2-loss'):\n",
    "                loss_0 = tf.nn.l2_loss(self.Y - pnnOut)\n",
    "                reg = l1 * tf.nn.l2_loss(W) + l2 * tf.nn.l2_loss(B)\n",
    "                reg += l3 * tf.nn.l2_loss(Z)\n",
    "                loss = loss_0 + reg\n",
    "        elif self.__lossType == 'xentropy':\n",
    "            with tf.name_scope('protonn-xentropy-loss'):\n",
    "                loss_0 = tf.nn.softmax_cross_entropy_with_logits_v2(logits=pnnOut,\n",
    "                                                         labels=tf.stop_gradient(self.Y))\n",
    "                loss_0 = tf.reduce_mean(loss_0)\n",
    "                reg = l1 * tf.nn.l2_loss(W) + l2 * tf.nn.l2_loss(B)\n",
    "                reg += l3 * tf.nn.l2_loss(Z)\n",
    "                loss = loss_0 + reg\n",
    "        return loss\n",
    "\n",
    "    def __trainGraph(self):\n",
    "        with tf.name_scope('protonn-gradient-adam'):\n",
    "            trainStep = tf.train.AdamOptimizer(self.__lR)\n",
    "            trainStep = trainStep.minimize(self.loss)\n",
    "        return trainStep\n",
    "\n",
    "    def __getHardThresholdOp(self):\n",
    "        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n",
    "        self.W_th = tf.placeholder(tf.float32, name='W_th')\n",
    "        self.B_th = tf.placeholder(tf.float32, name='B_th')\n",
    "        self.Z_th = tf.placeholder(tf.float32, name='Z_th')\n",
    "        with tf.name_scope('hard-threshold-assignments'):\n",
    "            hard_thrsd_W = W.assign(self.W_th)\n",
    "            hard_thrsd_B = B.assign(self.B_th)\n",
    "            hard_thrsd_Z = Z.assign(self.Z_th)\n",
    "            hard_thrsd_op = tf.group(hard_thrsd_W, hard_thrsd_B, hard_thrsd_Z)\n",
    "        return hard_thrsd_op\n",
    "\n",
    "    def train(self, batchSize, totalEpochs, sess,\n",
    "              x_train, x_val, y_train, y_val, noInit=False,\n",
    "              redirFile=None, printStep=10, valStep=3):\n",
    "        '''\n",
    "        Performs dense training of ProtoNN followed by iterative hard\n",
    "        thresholding to enforce sparsity constraints.\n",
    "\n",
    "        batchSize: Batch size per update\n",
    "        totalEpochs: The number of epochs to run training for. One epoch is\n",
    "            defined as one pass over the entire training data.\n",
    "        sess: The Tensorflow session to use for running various graph\n",
    "            operators.\n",
    "        x_train, x_val, y_train, y_val: The numpy array containing train and\n",
    "            validation data. x data is assumed to in of shape [-1,\n",
    "            featureDimension] while y should have shape [-1, numberLabels].\n",
    "        noInit: By default, all the tensors of the computation graph are\n",
    "        initialized at the start of the training session. Set noInit=False to\n",
    "        disable this behaviour.\n",
    "        printStep: Number of batches between echoing of loss and train accuracy.\n",
    "        valStep: Number of epochs between evolutions on validation set.\n",
    "        '''\n",
    "        d, d_cap, m, L, gamma = self.protoNNObj.getHyperParams()\n",
    "        assert batchSize >= 1, 'Batch size should be positive integer'\n",
    "        assert totalEpochs >= 1, 'Total epochs should be positive integer'\n",
    "        assert x_train.ndim == 2, 'Expected training data to be of rank 2'\n",
    "        assert x_train.shape[1] == d, 'Expected x_train to be [-1, %d]' % d\n",
    "        assert x_val.ndim == 2, 'Expected validation data to be of rank 2'\n",
    "        assert x_val.shape[1] == d, 'Expected x_val to be [-1, %d]' % d\n",
    "        assert y_train.ndim == 2, 'Expected training labels to be of rank 2'\n",
    "        assert y_train.shape[1] == L, 'Expected y_train to be [-1, %d]' % L\n",
    "        assert y_val.ndim == 2, 'Expected validation labels to be of rank 2'\n",
    "        assert y_val.shape[1] == L, 'Expected y_val to be [-1, %d]' % L\n",
    "\n",
    "        # Numpy will throw asserts for arrays\n",
    "        if sess is None:\n",
    "            raise ValueError('sess must be valid Tensorflow session.')\n",
    "\n",
    "        trainNumBatches = int(np.ceil(len(x_train) / batchSize))\n",
    "        valNumBatches = int(np.ceil(len(x_val) / batchSize))\n",
    "        x_train_batches = np.array_split(x_train, trainNumBatches)\n",
    "        y_train_batches = np.array_split(y_train, trainNumBatches)\n",
    "        x_val_batches = np.array_split(x_val, valNumBatches)\n",
    "        y_val_batches = np.array_split(y_val, valNumBatches)\n",
    "        if not noInit:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        X, Y = self.X, self.Y\n",
    "        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n",
    "        for epoch in range(totalEpochs):\n",
    "            for i in range(len(x_train_batches)):\n",
    "                batch_x = x_train_batches[i]\n",
    "                batch_y = y_train_batches[i]\n",
    "                feed_dict = {\n",
    "                    X: batch_x,\n",
    "                    Y: batch_y\n",
    "                }\n",
    "                sess.run(self.trainStep, feed_dict=feed_dict)\n",
    "                if i % printStep == 0:\n",
    "                    loss, acc = sess.run([self.loss, self.accuracy],\n",
    "                                         feed_dict=feed_dict)\n",
    "                    msg = \"Epoch: %3d Batch: %3d\" % (epoch, i)\n",
    "                    msg += \" Loss: %3.5f Accuracy: %2.5f\" % (loss, acc)\n",
    "                    print(msg, file=redirFile)\n",
    "\n",
    "            # Perform Hard thresholding\n",
    "            if self.sparseTraining:\n",
    "                W_, B_, Z_ = sess.run([W, B, Z])\n",
    "                fd_thrsd = {\n",
    "                    self.W_th: hardThreshold(W_, self.__sW),\n",
    "                    self.B_th: hardThreshold(B_, self.__sB),\n",
    "                    self.Z_th: hardThreshold(Z_, self.__sZ)\n",
    "                }\n",
    "                sess.run(self.__hthOp, feed_dict=fd_thrsd)\n",
    "\n",
    "            if (epoch + 1) % valStep  == 0:\n",
    "                acc = 0.0\n",
    "                loss = 0.0\n",
    "                for j in range(len(x_val_batches)):\n",
    "                    batch_x = x_val_batches[j]\n",
    "                    batch_y = y_val_batches[j]\n",
    "                    feed_dict = {\n",
    "                        X: batch_x,\n",
    "                        Y: batch_y\n",
    "                    }\n",
    "                    acc_, loss_ = sess.run([self.accuracy, self.loss],\n",
    "                                           feed_dict=feed_dict)\n",
    "                    acc += acc_\n",
    "                    loss += loss_\n",
    "                acc /= len(y_val_batches)\n",
    "                loss /= len(y_val_batches)\n",
    "                print(\"Test Loss: %2.5f Accuracy: %2.5f\" % (loss, acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Z6ym4k_s9pS"
   },
   "source": [
    "# Model Graph - ProtoNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GRPFglKHSbu-"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ProtoNN:\n",
    "    def __init__(self, inputDimension, projectionDimension, numPrototypes,\n",
    "                 numOutputLabels, gamma,\n",
    "                 W = None, B = None, Z = None):\n",
    "        '''\n",
    "        Forward computation graph for ProtoNN.\n",
    "\n",
    "        inputDimension: Input data dimension or feature dimension.\n",
    "        projectionDimension: hyperparameter\n",
    "        numPrototypes: hyperparameter\n",
    "        numOutputLabels: The number of output labels or classes\n",
    "        W, B, Z: Numpy matrices that can be used to initialize\n",
    "            projection matrix(W), prototype matrix (B) and prototype labels\n",
    "            matrix (B).\n",
    "            Expected Dimensions:\n",
    "                W   inputDimension (d) x projectionDimension (d_cap)\n",
    "                B   projectionDimension (d_cap) x numPrototypes (m)\n",
    "                Z   numOutputLabels (L) x numPrototypes (m)\n",
    "        '''\n",
    "        with tf.name_scope('protoNN') as ns:\n",
    "            self.__nscope = ns\n",
    "        self.__d = inputDimension\n",
    "        self.__d_cap = projectionDimension\n",
    "        self.__m = numPrototypes\n",
    "        self.__L = numOutputLabels\n",
    "\n",
    "        self.__inW = W\n",
    "        self.__inB = B\n",
    "        self.__inZ = Z\n",
    "        self.__inGamma = gamma\n",
    "        self.W, self.B, self.Z = None, None, None\n",
    "        self.gamma = None\n",
    "\n",
    "        self.__validInit = False\n",
    "        self.__initWBZ()\n",
    "        self.__initGamma()\n",
    "        self.__validateInit()\n",
    "        self.protoNNOut = None\n",
    "        self.predictions = None\n",
    "        self.accuracy = None\n",
    "\n",
    "    def __validateInit(self):\n",
    "        self.__validInit = False\n",
    "        errmsg = \"Dimensions mismatch! Should be W[d, d_cap]\"\n",
    "        errmsg += \", B[d_cap, m] and Z[L, m]\"\n",
    "        d, d_cap, m, L, _ = self.getHyperParams()\n",
    "        assert self.W.shape[0] == d, errmsg\n",
    "        assert self.W.shape[1] == d_cap, errmsg\n",
    "        assert self.B.shape[0] == d_cap, errmsg\n",
    "        assert self.B.shape[1] == m, errmsg\n",
    "        assert self.Z.shape[0] == L, errmsg\n",
    "        assert self.Z.shape[1] == m, errmsg\n",
    "        self.__validInit = True\n",
    "\n",
    "    def __initWBZ(self):\n",
    "        with tf.name_scope(self.__nscope):\n",
    "            W = self.__inW\n",
    "            if W is None:\n",
    "                W = tf.random_normal_initializer()\n",
    "                W = W([self.__d, self.__d_cap])\n",
    "            self.W = tf.Variable(W, name='W', dtype=tf.float32)\n",
    "\n",
    "            B = self.__inB\n",
    "            if B is None:\n",
    "                B = tf.random_uniform_initializer()\n",
    "                B = B([self.__d_cap, self.__m])\n",
    "            self.B = tf.Variable(B, name='B', dtype=tf.float32)\n",
    "\n",
    "            Z = self.__inZ\n",
    "            if Z is None:\n",
    "                Z = tf.random_normal_initializer()\n",
    "                Z = Z([self.__L, self.__m])\n",
    "            Z = tf.Variable(Z, name='Z', dtype=tf.float32)\n",
    "            self.Z = Z\n",
    "        return self.W, self.B, self.Z\n",
    "\n",
    "    def __initGamma(self):\n",
    "        with tf.name_scope(self.__nscope):\n",
    "            gamma = self.__inGamma\n",
    "            self.gamma = tf.constant(gamma, name='gamma')\n",
    "\n",
    "    def getHyperParams(self):\n",
    "        '''\n",
    "        Returns the model hyperparameters:\n",
    "            [inputDimension, projectionDimension,\n",
    "            numPrototypes, numOutputLabels, gamma]\n",
    "        '''\n",
    "        d = self.__d\n",
    "        dcap = self.__d_cap\n",
    "        m = self.__m\n",
    "        L = self.__L\n",
    "        return d, dcap, m, L, self.gamma\n",
    "\n",
    "    def getModelMatrices(self):\n",
    "        '''\n",
    "        Returns Tensorflow tensors of the model matrices, which\n",
    "        can then be evaluated to obtain corresponding numpy arrays.\n",
    "\n",
    "        These can then be exported as part of other implementations of\n",
    "        ProtonNN, for instance a C++ implementation or pure python\n",
    "        implementation.\n",
    "        Returns\n",
    "            [ProjectionMatrix (W), prototypeMatrix (B),\n",
    "             prototypeLabelsMatrix (Z), gamma]\n",
    "        '''\n",
    "        return self.W, self.B, self.Z, self.gamma\n",
    "\n",
    "    def __call__(self, X, Y=None):\n",
    "        '''\n",
    "        This method is responsible for construction of the forward computation\n",
    "        graph. The end point of the computation graph, or in other words the\n",
    "        output operator for the forward computation is returned. Additionally,\n",
    "        if the argument Y is provided, a classification accuracy operator with\n",
    "        Y as target will also be created. For this, Y is assumed to in one-hot\n",
    "        encoded format and the class with the maximum prediction score is\n",
    "        compared to the encoded class in Y.  This accuracy operator is returned\n",
    "        by getAccuracyOp() method. If a different accuracyOp is required, it\n",
    "        can be defined by overriding the createAccOp(protoNNScoresOut, Y)\n",
    "        method.\n",
    "\n",
    "        X: Input tensor or placeholder of shape [-1, inputDimension]\n",
    "        Y: Optional tensor or placeholder for targets (labels or classes).\n",
    "            Expected shape is [-1, numOutputLabels].\n",
    "        returns: The forward computation outputs, self.protoNNOut\n",
    "        '''\n",
    "        # This should never execute\n",
    "        assert self.__validInit is True, \"Initialization failed!\"\n",
    "        if self.protoNNOut is not None:\n",
    "            return self.protoNNOut\n",
    "\n",
    "        W, B, Z, gamma = self.W, self.B, self.Z, self.gamma\n",
    "        with tf.name_scope(self.__nscope):\n",
    "            WX = tf.matmul(X, W)\n",
    "            # Convert WX to tensor so that broadcasting can work\n",
    "            dim = [-1, WX.shape.as_list()[1], 1]\n",
    "            WX = tf.reshape(WX, dim)\n",
    "            dim = [1, B.shape.as_list()[0], -1]\n",
    "            B = tf.reshape(B, dim)\n",
    "            l2sim = B - WX\n",
    "            l2sim = tf.pow(l2sim, 2)\n",
    "            l2sim = tf.reduce_sum(l2sim, 1, keepdims=True)\n",
    "            self.l2sim = l2sim\n",
    "            gammal2sim = (-1 * gamma * gamma) * l2sim\n",
    "            M = tf.exp(gammal2sim)\n",
    "            dim = [1] + Z.shape.as_list()\n",
    "            Z = tf.reshape(Z, dim)\n",
    "            y = tf.multiply(Z, M)\n",
    "            y = tf.reduce_sum(y, 2, name='protoNNScoreOut')\n",
    "            self.protoNNOut = y\n",
    "            self.predictions = tf.argmax(y, 1, name='protoNNPredictions')\n",
    "            if Y is not None:\n",
    "                self.createAccOp(self.protoNNOut, Y)\n",
    "        return y\n",
    "\n",
    "    def createAccOp(self, outputs, target):\n",
    "        '''\n",
    "        Define an accuracy operation on ProtoNN's output scores and targets.\n",
    "        Here a simple classification accuracy operator is defined. More\n",
    "        complicated operators (for multiple label problems and so forth) can be\n",
    "        defined by overriding this method\n",
    "        '''\n",
    "        assert self.predictions is not None\n",
    "        target = tf.argmax(target, 1)\n",
    "        correctPrediction = tf.equal(self.predictions, target)\n",
    "        acc = tf.reduce_mean(tf.cast(correctPrediction, tf.float32),\n",
    "                             name='protoNNAccuracy')\n",
    "        self.accuracy = acc\n",
    "\n",
    "    def getPredictionsOp(self):\n",
    "        '''\n",
    "        The predictions operator is defined as argmax(protoNNScores) for each\n",
    "        prediction.\n",
    "        '''\n",
    "        return self.predictions\n",
    "\n",
    "    def getAccuracyOp(self):\n",
    "        '''\n",
    "        returns accuracyOp as defined by createAccOp. It defaults to\n",
    "        multi-class classification accuracy.\n",
    "        '''\n",
    "        msg = \"Accuracy operator not defined in graph. Did you provide Y as an\"\n",
    "        msg += \" argument to _call_?\"\n",
    "        assert self.accuracy is not None, msg\n",
    "        return self.accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WINDOW 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uEBMKewPQJ1c"
   },
   "source": [
    "# Obtain Data\n",
    "\n",
    "It is assumed that the Daphnet data has already been downloaded,preprocessed and set up in subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Dimension:  423\n",
      "Num classes:  2\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = r\"./experiments\"\n",
    "windowLen = 'data_w1'\n",
    "out = preprocessData(DATA_DIR,windowLen)\n",
    "dataDimension = out[0]\n",
    "numClasses = out[1]\n",
    "x_train, y_train = out[2], out[3]\n",
    "x_test, y_test = out[4], out[5]\n",
    "print(\"Feature Dimension: \", dataDimension)\n",
    "print(\"Num classes: \", numClasses)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_DIR = r\"./experiments\"\n",
    "train, test = np.load(DATA_DIR + '/ttrain_data_w1.npy'), np.load(DATA_DIR + '/ttest_data_w1.npy')\n",
    "x_train, y_train = train[:, 1:], train[:, 0]\n",
    "x_test, y_test = test[:, 1:], test[:, 0]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "numClasses = max(y_train) - min(y_train) + 1\n",
    "numClasses = max(numClasses, max(y_test) - min(y_test) + 1)\n",
    "numClasses = int(numClasses)\n",
    "\n",
    "y_train = helper.to_onehot(y_train, numClasses)\n",
    "y_test = helper.to_onehot(y_test, numClasses)\n",
    "y_val = helper.to_onehot(y_val, numClasses)\n",
    "\n",
    "dataDimension = x_train.shape[1]\n",
    "numClasses = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1u6oX8eJQJ2N"
   },
   "source": [
    "# Model Parameters\n",
    "\n",
    "Note that ProtoNN is very sensitive to the value of the hyperparameter $\\gamma$, here stored in valiable `GAMMA`. If `GAMMA` is set to `None`, median heuristic will be used to estimate a good value of $\\gamma$ through the `helper.getGamma()` method. This method also returns the corresponding `W` and `B` matrices which should be used to initialize ProtoNN (as is done here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:06:10.279204Z",
     "start_time": "2018-08-15T13:06:10.272880Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "UaduZ1vJQJ2P"
   },
   "outputs": [],
   "source": [
    "PROJECTION_DIM = 5 #d^\n",
    "NUM_PROTOTYPES = 40 #m\n",
    "REG_W = 0.000005\n",
    "REG_B = 0.0\n",
    "REG_Z = 0.00005\n",
    "SPAR_W = 1.0\n",
    "SPAR_B = 0.8\n",
    "SPAR_Z = 0.8\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 600\n",
    "BATCH_SIZE = 2048\n",
    "GAMMA = 0.007586"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:06:10.307632Z",
     "start_time": "2018-08-15T13:06:10.280955Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1003,
     "status": "ok",
     "timestamp": 1567154485603,
     "user": {
      "displayName": "Gokul Hari",
      "photoUrl": "",
      "userId": "16159457985484250305"
     },
     "user_tz": -330
    },
    "id": "teqlUPhLQJ2W",
    "outputId": "e7e7f7f2-9ddb-448b-9539-65a1a2dc1c03"
   },
   "outputs": [],
   "source": [
    "W, B, gamma = getGamma(GAMMA, PROJECTION_DIM, dataDimension,\n",
    "                       NUM_PROTOTYPES, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007586"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, dataDimension], name='X')\n",
    "Y = tf.placeholder(tf.float32, [None, numClasses], name='Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, dataDimension], name='X')\n",
    "Y = tf.placeholder(tf.float32, [None, numClasses], name='Y')\n",
    "def objective(trial,x_train, x_test, y_train, y_test, x_val, y_val):\n",
    "    \n",
    "    W, B, gamma = getGamma(GAMMA, PROJECTION_DIM, dataDimension,\n",
    "                       NUM_PROTOTYPES, x_train)\n",
    "    # Inside the optimization function, you use the 'trial' object to suggest hyperparameters\n",
    "    REG_W = trial.suggest_float('REG_W', 2e-6, 5e-6)\n",
    "    REG_B = trial.suggest_float('REG_B', 0.0, 0.01)\n",
    "    REG_Z = trial.suggest_float('REG_Z', 2e-5, 5e-5)\n",
    "    SPAR_W = trial.suggest_float('SPAR_W', 0.5, 1.0)\n",
    "    SPAR_B = trial.suggest_float('SPAR_B', 0.5, 1.0)\n",
    "    SPAR_Z = trial.suggest_float('SPAR_Z', 0.5, 1.0)\n",
    "    loss = trial.suggest_categorical('loss', ['l2', 'xentropy'])\n",
    "        \n",
    "    LEARNING_RATE = trial.suggest_float('LEARNING_RATE', 1e-4, 1e-3)\n",
    "    NUM_EPOCHS = trial.suggest_int('NUM_EPOCHS', 200, 600)\n",
    "\n",
    "    # Set the suggested hyperparameters in the trainer\n",
    "    protoNN = ProtoNN(dataDimension, PROJECTION_DIM,\n",
    "                  NUM_PROTOTYPES, numClasses,\n",
    "                  gamma, W=W, B=B)\n",
    "    \n",
    "    trainer = ProtoNNTrainer(protoNN, REG_W, REG_B, REG_Z,\n",
    "                         SPAR_W, SPAR_B, SPAR_Z,\n",
    "                         LEARNING_RATE, X, Y, lossType=loss)\n",
    "    \n",
    "    # Call your ProtoNN trainer function or use it as needed\n",
    "    sess = tf.Session()\n",
    "    \n",
    "        \n",
    "    trainer.train(BATCH_SIZE, NUM_EPOCHS, sess, x_train, x_test, y_train, y_test,printStep=600, valStep=10)\n",
    "    pred = sess.run(protoNN.predictions, feed_dict={X: x_val, Y: y_val})\n",
    "    # W, B, Z are tensorflow graph nodes\n",
    "    W, B, Z, _ = protoNN.getModelMatrices()\n",
    "    matrixList = sess.run([W, B, Z])\n",
    "    sparcityList = [SPAR_W, SPAR_B, SPAR_Z]                       \n",
    "    nnz, size, sparse = getModelSize(matrixList, sparcityList)\n",
    "    y_val = np.argmax(y_val,axis=1)\n",
    "    sensitivity = confusion_matrix(y_val,pred)[1][1]/(confusion_matrix(y_val,pred)[1][1] + confusion_matrix(y_val,pred)[1][0])\n",
    "    specificity = confusion_matrix(y_val,pred)[0][0]/(confusion_matrix(y_val,pred)[0][0] + confusion_matrix(y_val,pred)[0][1])\n",
    "\n",
    "    alpha = trial.suggest_float('alpha', 0.0, 1.0)\n",
    "\n",
    "    f1 = (2*sensitivity*specificity)/(sensitivity+specificity)\n",
    "    mcc = matthews_corrcoef(y_val, pred)\n",
    "    return alpha * f1 + (1 - alpha) * mcc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-05 15:53:06,239] A new study created in memory with name: no-name-25f8a899-991d-4b05-8513-eefe96805523\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction='maximize', pruner = optuna.pruners.MedianPruner())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nadit\\anaconda3\\envs\\ProtoNN\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch:   0 Batch:   0 Loss: 32135.26367 Accuracy: 0.49768\n",
      "Epoch:   1 Batch:   0 Loss: 32359.42383 Accuracy: 0.49768\n",
      "Epoch:   2 Batch:   0 Loss: 31790.16016 Accuracy: 0.49768\n",
      "Epoch:   3 Batch:   0 Loss: 31228.30664 Accuracy: 0.49768\n",
      "Epoch:   4 Batch:   0 Loss: 30675.18359 Accuracy: 0.49768\n",
      "Epoch:   5 Batch:   0 Loss: 30131.35938 Accuracy: 0.49768\n",
      "Epoch:   6 Batch:   0 Loss: 29597.03711 Accuracy: 0.49768\n",
      "Epoch:   7 Batch:   0 Loss: 29072.27539 Accuracy: 0.49768\n",
      "Epoch:   8 Batch:   0 Loss: 28557.02930 Accuracy: 0.49768\n",
      "Epoch:   9 Batch:   0 Loss: 28051.22070 Accuracy: 0.49768\n",
      "Test Loss: 25997.18066 Accuracy: 0.49987\n",
      "Epoch:  10 Batch:   0 Loss: 27554.69727 Accuracy: 0.49768\n",
      "Epoch:  11 Batch:   0 Loss: 27067.28711 Accuracy: 0.49768\n",
      "Epoch:  12 Batch:   0 Loss: 26588.84180 Accuracy: 0.49768\n",
      "Epoch:  13 Batch:   0 Loss: 26119.19531 Accuracy: 0.49768\n",
      "Epoch:  14 Batch:   0 Loss: 25658.15625 Accuracy: 0.49768\n",
      "Epoch:  15 Batch:   0 Loss: 25205.55859 Accuracy: 0.49768\n",
      "Epoch:  16 Batch:   0 Loss: 24761.25586 Accuracy: 0.49768\n",
      "Epoch:  17 Batch:   0 Loss: 24325.09180 Accuracy: 0.49768\n",
      "Epoch:  18 Batch:   0 Loss: 23896.87305 Accuracy: 0.49768\n",
      "Epoch:  19 Batch:   0 Loss: 23476.46094 Accuracy: 0.49768\n",
      "Test Loss: 21717.82666 Accuracy: 0.49987\n",
      "Epoch:  20 Batch:   0 Loss: 23063.70117 Accuracy: 0.49768\n",
      "Epoch:  21 Batch:   0 Loss: 22658.46680 Accuracy: 0.49768\n",
      "Epoch:  22 Batch:   0 Loss: 22260.55664 Accuracy: 0.49768\n",
      "Epoch:  23 Batch:   0 Loss: 21869.84766 Accuracy: 0.49768\n",
      "Epoch:  24 Batch:   0 Loss: 21486.20703 Accuracy: 0.49768\n",
      "Epoch:  25 Batch:   0 Loss: 21109.50000 Accuracy: 0.49768\n",
      "Epoch:  26 Batch:   0 Loss: 20739.55859 Accuracy: 0.49768\n",
      "Epoch:  27 Batch:   0 Loss: 20376.24414 Accuracy: 0.49768\n",
      "Epoch:  28 Batch:   0 Loss: 20019.46094 Accuracy: 0.49768\n",
      "Epoch:  29 Batch:   0 Loss: 19669.03125 Accuracy: 0.49768\n",
      "Test Loss: 18162.50732 Accuracy: 0.49987\n",
      "Epoch:  30 Batch:   0 Loss: 19324.83398 Accuracy: 0.49768\n",
      "Epoch:  31 Batch:   0 Loss: 18986.76758 Accuracy: 0.49768\n",
      "Epoch:  32 Batch:   0 Loss: 18654.69141 Accuracy: 0.49768\n",
      "Epoch:  33 Batch:   0 Loss: 18328.47656 Accuracy: 0.49768\n",
      "Epoch:  34 Batch:   0 Loss: 18008.02344 Accuracy: 0.49768\n",
      "Epoch:  35 Batch:   0 Loss: 17693.19531 Accuracy: 0.49768\n",
      "Epoch:  36 Batch:   0 Loss: 17383.87500 Accuracy: 0.49768\n",
      "Epoch:  37 Batch:   0 Loss: 17079.97070 Accuracy: 0.49768\n",
      "Epoch:  38 Batch:   0 Loss: 16781.36719 Accuracy: 0.49768\n",
      "Epoch:  39 Batch:   0 Loss: 16487.93750 Accuracy: 0.49768\n",
      "Test Loss: 15197.37354 Accuracy: 0.49987\n",
      "Epoch:  40 Batch:   0 Loss: 16199.61035 Accuracy: 0.49768\n",
      "Epoch:  41 Batch:   0 Loss: 15916.24023 Accuracy: 0.49768\n",
      "Epoch:  42 Batch:   0 Loss: 15637.77051 Accuracy: 0.49768\n",
      "Epoch:  43 Batch:   0 Loss: 15364.07520 Accuracy: 0.49768\n",
      "Epoch:  44 Batch:   0 Loss: 15095.07422 Accuracy: 0.49768\n",
      "Epoch:  45 Batch:   0 Loss: 14830.67578 Accuracy: 0.49768\n",
      "Epoch:  46 Batch:   0 Loss: 14570.77246 Accuracy: 0.49768\n",
      "Epoch:  47 Batch:   0 Loss: 14315.30566 Accuracy: 0.49768\n",
      "Epoch:  48 Batch:   0 Loss: 14064.15625 Accuracy: 0.49768\n",
      "Epoch:  49 Batch:   0 Loss: 13817.28027 Accuracy: 0.49768\n",
      "Test Loss: 12712.53589 Accuracy: 0.49987\n",
      "Epoch:  50 Batch:   0 Loss: 13574.55566 Accuracy: 0.49768\n",
      "Epoch:  51 Batch:   0 Loss: 13335.94824 Accuracy: 0.49768\n",
      "Epoch:  52 Batch:   0 Loss: 13101.33789 Accuracy: 0.49768\n",
      "Epoch:  53 Batch:   0 Loss: 12870.68750 Accuracy: 0.49768\n",
      "Epoch:  54 Batch:   0 Loss: 12643.89746 Accuracy: 0.49768\n",
      "Epoch:  55 Batch:   0 Loss: 12420.91602 Accuracy: 0.49768\n",
      "Epoch:  56 Batch:   0 Loss: 12201.66504 Accuracy: 0.49768\n",
      "Epoch:  57 Batch:   0 Loss: 11986.07910 Accuracy: 0.49768\n",
      "Epoch:  58 Batch:   0 Loss: 11774.10156 Accuracy: 0.49768\n",
      "Epoch:  59 Batch:   0 Loss: 11565.64941 Accuracy: 0.49768\n",
      "Test Loss: 10621.83350 Accuracy: 0.49987\n",
      "Epoch:  60 Batch:   0 Loss: 11360.69141 Accuracy: 0.49768\n",
      "Epoch:  61 Batch:   0 Loss: 11159.14551 Accuracy: 0.49768\n",
      "Epoch:  62 Batch:   0 Loss: 10960.95215 Accuracy: 0.49768\n",
      "Epoch:  63 Batch:   0 Loss: 10766.07812 Accuracy: 0.49768\n",
      "Epoch:  64 Batch:   0 Loss: 10574.44043 Accuracy: 0.49768\n",
      "Epoch:  65 Batch:   0 Loss: 10385.99414 Accuracy: 0.49768\n",
      "Epoch:  66 Batch:   0 Loss: 10200.69922 Accuracy: 0.49768\n",
      "Epoch:  67 Batch:   0 Loss: 10018.49121 Accuracy: 0.49768\n",
      "Epoch:  68 Batch:   0 Loss: 9839.31543 Accuracy: 0.49768\n",
      "Epoch:  69 Batch:   0 Loss: 9663.13281 Accuracy: 0.49768\n",
      "Test Loss: 8859.38208 Accuracy: 0.49987\n",
      "Epoch:  70 Batch:   0 Loss: 9489.89551 Accuracy: 0.49768\n",
      "Epoch:  71 Batch:   0 Loss: 9319.55664 Accuracy: 0.49768\n",
      "Epoch:  72 Batch:   0 Loss: 9152.05664 Accuracy: 0.49768\n",
      "Epoch:  73 Batch:   0 Loss: 8987.36816 Accuracy: 0.49768\n",
      "Epoch:  74 Batch:   0 Loss: 8825.43750 Accuracy: 0.49768\n",
      "Epoch:  75 Batch:   0 Loss: 8666.23242 Accuracy: 0.49768\n",
      "Epoch:  76 Batch:   0 Loss: 8509.70410 Accuracy: 0.49768\n",
      "Epoch:  77 Batch:   0 Loss: 8355.80176 Accuracy: 0.49768\n",
      "Epoch:  78 Batch:   0 Loss: 8204.49219 Accuracy: 0.49768\n",
      "Epoch:  79 Batch:   0 Loss: 8055.74170 Accuracy: 0.49768\n",
      "Test Loss: 7374.27734 Accuracy: 0.49987\n",
      "Epoch:  80 Batch:   0 Loss: 7909.49805 Accuracy: 0.49768\n",
      "Epoch:  81 Batch:   0 Loss: 7765.73779 Accuracy: 0.49768\n",
      "Epoch:  82 Batch:   0 Loss: 7624.41260 Accuracy: 0.49768\n",
      "Epoch:  83 Batch:   0 Loss: 7485.49170 Accuracy: 0.49768\n",
      "Epoch:  84 Batch:   0 Loss: 7348.93652 Accuracy: 0.49768\n",
      "Epoch:  85 Batch:   0 Loss: 7214.71338 Accuracy: 0.49768\n",
      "Epoch:  86 Batch:   0 Loss: 7082.77832 Accuracy: 0.49768\n",
      "Epoch:  87 Batch:   0 Loss: 6953.10303 Accuracy: 0.49768\n",
      "Epoch:  88 Batch:   0 Loss: 6825.65674 Accuracy: 0.49768\n",
      "Epoch:  89 Batch:   0 Loss: 6700.40137 Accuracy: 0.49768\n",
      "Test Loss: 6125.71167 Accuracy: 0.49987\n",
      "Epoch:  90 Batch:   0 Loss: 6577.30176 Accuracy: 0.49768\n",
      "Epoch:  91 Batch:   0 Loss: 6456.32617 Accuracy: 0.49768\n",
      "Epoch:  92 Batch:   0 Loss: 6337.44189 Accuracy: 0.49768\n",
      "Epoch:  93 Batch:   0 Loss: 6220.62646 Accuracy: 0.49768\n",
      "Epoch:  94 Batch:   0 Loss: 6105.84229 Accuracy: 0.49768\n",
      "Epoch:  95 Batch:   0 Loss: 5993.05664 Accuracy: 0.49768\n",
      "Epoch:  96 Batch:   0 Loss: 5882.24658 Accuracy: 0.49768\n",
      "Epoch:  97 Batch:   0 Loss: 5773.37500 Accuracy: 0.49768\n",
      "Epoch:  98 Batch:   0 Loss: 5666.40771 Accuracy: 0.49768\n",
      "Epoch:  99 Batch:   0 Loss: 5561.32129 Accuracy: 0.49768\n",
      "Test Loss: 5079.64355 Accuracy: 0.49987\n",
      "Epoch: 100 Batch:   0 Loss: 5458.08984 Accuracy: 0.49768\n",
      "Epoch: 101 Batch:   0 Loss: 5356.68994 Accuracy: 0.49768\n",
      "Epoch: 102 Batch:   0 Loss: 5257.08447 Accuracy: 0.49768\n",
      "Epoch: 103 Batch:   0 Loss: 5159.24707 Accuracy: 0.49768\n",
      "Epoch: 104 Batch:   0 Loss: 5063.14941 Accuracy: 0.49768\n",
      "Epoch: 105 Batch:   0 Loss: 4968.77295 Accuracy: 0.49768\n",
      "Epoch: 106 Batch:   0 Loss: 4876.08740 Accuracy: 0.49768\n",
      "Epoch: 107 Batch:   0 Loss: 4785.05811 Accuracy: 0.49768\n",
      "Epoch: 108 Batch:   0 Loss: 4695.67236 Accuracy: 0.49768\n",
      "Epoch: 109 Batch:   0 Loss: 4607.89941 Accuracy: 0.49768\n",
      "Test Loss: 4206.90314 Accuracy: 0.49987\n",
      "Epoch: 110 Batch:   0 Loss: 4521.70996 Accuracy: 0.49768\n",
      "Epoch: 111 Batch:   0 Loss: 4437.08838 Accuracy: 0.49768\n",
      "Epoch: 112 Batch:   0 Loss: 4354.00684 Accuracy: 0.49768\n",
      "Epoch: 113 Batch:   0 Loss: 4272.43604 Accuracy: 0.49768\n",
      "Epoch: 114 Batch:   0 Loss: 4192.36035 Accuracy: 0.49768\n",
      "Epoch: 115 Batch:   0 Loss: 4113.75000 Accuracy: 0.49768\n",
      "Epoch: 116 Batch:   0 Loss: 4036.58887 Accuracy: 0.49768\n",
      "Epoch: 117 Batch:   0 Loss: 3960.84863 Accuracy: 0.49768\n",
      "Epoch: 118 Batch:   0 Loss: 3886.50830 Accuracy: 0.49768\n",
      "Epoch: 119 Batch:   0 Loss: 3813.54834 Accuracy: 0.49768\n",
      "Test Loss: 3482.15356 Accuracy: 0.49987\n",
      "Epoch: 120 Batch:   0 Loss: 3741.94409 Accuracy: 0.49768\n",
      "Epoch: 121 Batch:   0 Loss: 3671.67603 Accuracy: 0.49768\n",
      "Epoch: 122 Batch:   0 Loss: 3602.72119 Accuracy: 0.49768\n",
      "Epoch: 123 Batch:   0 Loss: 3535.05981 Accuracy: 0.49768\n",
      "Epoch: 124 Batch:   0 Loss: 3468.67383 Accuracy: 0.49768\n",
      "Epoch: 125 Batch:   0 Loss: 3403.53516 Accuracy: 0.49768\n",
      "Epoch: 126 Batch:   0 Loss: 3339.63208 Accuracy: 0.49768\n",
      "Epoch: 127 Batch:   0 Loss: 3276.93970 Accuracy: 0.49768\n",
      "Epoch: 128 Batch:   0 Loss: 3215.44141 Accuracy: 0.49768\n",
      "Epoch: 129 Batch:   0 Loss: 3155.12012 Accuracy: 0.49768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2883.37137 Accuracy: 0.49987\n",
      "Epoch: 130 Batch:   0 Loss: 3095.94824 Accuracy: 0.49768\n",
      "Epoch: 131 Batch:   0 Loss: 3037.91602 Accuracy: 0.49768\n",
      "Epoch: 132 Batch:   0 Loss: 2981.00415 Accuracy: 0.49768\n",
      "Epoch: 133 Batch:   0 Loss: 2925.18799 Accuracy: 0.49768\n",
      "Epoch: 134 Batch:   0 Loss: 2870.45312 Accuracy: 0.49768\n",
      "Epoch: 135 Batch:   0 Loss: 2816.78638 Accuracy: 0.49768\n",
      "Epoch: 136 Batch:   0 Loss: 2764.16577 Accuracy: 0.49768\n",
      "Epoch: 137 Batch:   0 Loss: 2712.56982 Accuracy: 0.49768\n",
      "Epoch: 138 Batch:   0 Loss: 2661.98804 Accuracy: 0.49768\n",
      "Epoch: 139 Batch:   0 Loss: 2612.40552 Accuracy: 0.49768\n",
      "Test Loss: 2391.40504 Accuracy: 0.49987\n",
      "Epoch: 140 Batch:   0 Loss: 2563.80176 Accuracy: 0.49768\n",
      "Epoch: 141 Batch:   0 Loss: 2516.16187 Accuracy: 0.49768\n",
      "Epoch: 142 Batch:   0 Loss: 2469.46558 Accuracy: 0.49768\n",
      "Epoch: 143 Batch:   0 Loss: 2423.70117 Accuracy: 0.49768\n",
      "Epoch: 144 Batch:   0 Loss: 2378.85229 Accuracy: 0.49768\n",
      "Epoch: 145 Batch:   0 Loss: 2334.90479 Accuracy: 0.49768\n",
      "Epoch: 146 Batch:   0 Loss: 2291.84351 Accuracy: 0.49768\n",
      "Epoch: 147 Batch:   0 Loss: 2249.65186 Accuracy: 0.49768\n",
      "Epoch: 148 Batch:   0 Loss: 2208.31738 Accuracy: 0.49768\n",
      "Epoch: 149 Batch:   0 Loss: 2167.82422 Accuracy: 0.49768\n",
      "Test Loss: 1989.64345 Accuracy: 0.49987\n",
      "Epoch: 150 Batch:   0 Loss: 2128.15698 Accuracy: 0.49768\n",
      "Epoch: 151 Batch:   0 Loss: 2089.30273 Accuracy: 0.49768\n",
      "Epoch: 152 Batch:   0 Loss: 2051.24658 Accuracy: 0.49768\n",
      "Epoch: 153 Batch:   0 Loss: 2013.97400 Accuracy: 0.49768\n",
      "Epoch: 154 Batch:   0 Loss: 1977.47168 Accuracy: 0.49768\n",
      "Epoch: 155 Batch:   0 Loss: 1941.72729 Accuracy: 0.49768\n",
      "Epoch: 156 Batch:   0 Loss: 1906.72937 Accuracy: 0.49768\n",
      "Epoch: 157 Batch:   0 Loss: 1872.46204 Accuracy: 0.49768\n",
      "Epoch: 158 Batch:   0 Loss: 1838.91321 Accuracy: 0.49768\n",
      "Epoch: 159 Batch:   0 Loss: 1806.07117 Accuracy: 0.49768\n",
      "Test Loss: 1663.71518 Accuracy: 0.49987\n",
      "Epoch: 160 Batch:   0 Loss: 1773.92505 Accuracy: 0.49768\n",
      "Epoch: 161 Batch:   0 Loss: 1742.45959 Accuracy: 0.49768\n",
      "Epoch: 162 Batch:   0 Loss: 1711.66333 Accuracy: 0.49768\n",
      "Epoch: 163 Batch:   0 Loss: 1681.52466 Accuracy: 0.49768\n",
      "Epoch: 164 Batch:   0 Loss: 1652.03296 Accuracy: 0.49768\n",
      "Epoch: 165 Batch:   0 Loss: 1623.17590 Accuracy: 0.49768\n",
      "Epoch: 166 Batch:   0 Loss: 1594.94214 Accuracy: 0.49768\n",
      "Epoch: 167 Batch:   0 Loss: 1567.31799 Accuracy: 0.49768\n",
      "Epoch: 168 Batch:   0 Loss: 1540.29578 Accuracy: 0.49768\n",
      "Epoch: 169 Batch:   0 Loss: 1513.86414 Accuracy: 0.49768\n",
      "Test Loss: 1401.20823 Accuracy: 0.49987\n",
      "Epoch: 170 Batch:   0 Loss: 1488.01038 Accuracy: 0.49768\n",
      "Epoch: 171 Batch:   0 Loss: 1462.72595 Accuracy: 0.49768\n",
      "Epoch: 172 Batch:   0 Loss: 1437.99963 Accuracy: 0.49768\n",
      "Epoch: 173 Batch:   0 Loss: 1413.82166 Accuracy: 0.49768\n",
      "Epoch: 174 Batch:   0 Loss: 1390.18164 Accuracy: 0.49768\n",
      "Epoch: 175 Batch:   0 Loss: 1367.06812 Accuracy: 0.49768\n",
      "Epoch: 176 Batch:   0 Loss: 1344.47363 Accuracy: 0.49768\n",
      "Epoch: 177 Batch:   0 Loss: 1322.38562 Accuracy: 0.49768\n",
      "Epoch: 178 Batch:   0 Loss: 1300.79810 Accuracy: 0.49768\n",
      "Epoch: 179 Batch:   0 Loss: 1279.69751 Accuracy: 0.49768\n",
      "Test Loss: 1191.43749 Accuracy: 0.49987\n",
      "Epoch: 180 Batch:   0 Loss: 1259.07800 Accuracy: 0.49768\n",
      "Epoch: 181 Batch:   0 Loss: 1238.92981 Accuracy: 0.49768\n",
      "Epoch: 182 Batch:   0 Loss: 1219.24097 Accuracy: 0.49768\n",
      "Epoch: 183 Batch:   0 Loss: 1200.00732 Accuracy: 0.49768\n",
      "Epoch: 184 Batch:   0 Loss: 1181.21692 Accuracy: 0.49768\n",
      "Epoch: 185 Batch:   0 Loss: 1162.86108 Accuracy: 0.49768\n",
      "Epoch: 186 Batch:   0 Loss: 1144.93433 Accuracy: 0.49768\n",
      "Epoch: 187 Batch:   0 Loss: 1127.42651 Accuracy: 0.49768\n",
      "Epoch: 188 Batch:   0 Loss: 1110.32751 Accuracy: 0.49768\n",
      "Epoch: 189 Batch:   0 Loss: 1093.63171 Accuracy: 0.49768\n",
      "Test Loss: 1025.21696 Accuracy: 0.49987\n",
      "Epoch: 190 Batch:   0 Loss: 1077.33057 Accuracy: 0.49768\n",
      "Epoch: 191 Batch:   0 Loss: 1061.41614 Accuracy: 0.49768\n",
      "Epoch: 192 Batch:   0 Loss: 1045.88257 Accuracy: 0.49768\n",
      "Epoch: 193 Batch:   0 Loss: 1030.71973 Accuracy: 0.49768\n",
      "Epoch: 194 Batch:   0 Loss: 1015.91974 Accuracy: 0.49768\n",
      "Epoch: 195 Batch:   0 Loss: 1001.47614 Accuracy: 0.49768\n",
      "Epoch: 196 Batch:   0 Loss: 987.38055 Accuracy: 0.49768\n",
      "Epoch: 197 Batch:   0 Loss: 973.62775 Accuracy: 0.49768\n",
      "Epoch: 198 Batch:   0 Loss: 960.21008 Accuracy: 0.49768\n",
      "Epoch: 199 Batch:   0 Loss: 947.12109 Accuracy: 0.49768\n",
      "Test Loss: 894.69467 Accuracy: 0.49987\n",
      "Epoch: 200 Batch:   0 Loss: 934.35413 Accuracy: 0.49768\n",
      "Epoch: 201 Batch:   0 Loss: 921.90314 Accuracy: 0.49768\n",
      "Epoch: 202 Batch:   0 Loss: 909.76056 Accuracy: 0.49768\n",
      "Epoch: 203 Batch:   0 Loss: 897.92017 Accuracy: 0.49768\n",
      "Epoch: 204 Batch:   0 Loss: 886.37537 Accuracy: 0.49768\n",
      "Epoch: 205 Batch:   0 Loss: 875.11932 Accuracy: 0.49768\n",
      "Epoch: 206 Batch:   0 Loss: 864.14624 Accuracy: 0.49768\n",
      "Epoch: 207 Batch:   0 Loss: 853.45020 Accuracy: 0.49768\n",
      "Epoch: 208 Batch:   0 Loss: 843.02576 Accuracy: 0.49768\n",
      "Epoch: 209 Batch:   0 Loss: 832.86664 Accuracy: 0.49768\n",
      "Test Loss: 793.19903 Accuracy: 0.49987\n",
      "Epoch: 210 Batch:   0 Loss: 822.96649 Accuracy: 0.49768\n",
      "Epoch: 211 Batch:   0 Loss: 813.32062 Accuracy: 0.49768\n",
      "Epoch: 212 Batch:   0 Loss: 803.92377 Accuracy: 0.49768\n",
      "Epoch: 213 Batch:   0 Loss: 794.76941 Accuracy: 0.49768\n",
      "Epoch: 214 Batch:   0 Loss: 785.85303 Accuracy: 0.49768\n",
      "Epoch: 215 Batch:   0 Loss: 777.16846 Accuracy: 0.49768\n",
      "Epoch: 216 Batch:   0 Loss: 768.71106 Accuracy: 0.49768\n",
      "Epoch: 217 Batch:   0 Loss: 760.47559 Accuracy: 0.49768\n",
      "Epoch: 218 Batch:   0 Loss: 752.45740 Accuracy: 0.49768\n",
      "Epoch: 219 Batch:   0 Loss: 744.65143 Accuracy: 0.49768\n",
      "Test Loss: 715.07320 Accuracy: 0.49987\n",
      "Epoch: 220 Batch:   0 Loss: 737.05341 Accuracy: 0.49768\n",
      "Epoch: 221 Batch:   0 Loss: 729.65765 Accuracy: 0.49768\n",
      "Epoch: 222 Batch:   0 Loss: 722.46002 Accuracy: 0.49768\n",
      "Epoch: 223 Batch:   0 Loss: 715.45538 Accuracy: 0.49768\n",
      "Epoch: 224 Batch:   0 Loss: 708.64001 Accuracy: 0.49768\n",
      "Epoch: 225 Batch:   0 Loss: 702.00916 Accuracy: 0.49768\n",
      "Epoch: 226 Batch:   0 Loss: 695.55859 Accuracy: 0.49768\n",
      "Epoch: 227 Batch:   0 Loss: 689.28424 Accuracy: 0.49768\n",
      "Epoch: 228 Batch:   0 Loss: 683.18152 Accuracy: 0.49768\n",
      "Epoch: 229 Batch:   0 Loss: 677.24640 Accuracy: 0.49768\n",
      "Test Loss: 655.57645 Accuracy: 0.49987\n",
      "Epoch: 230 Batch:   0 Loss: 671.47498 Accuracy: 0.49768\n",
      "Epoch: 231 Batch:   0 Loss: 665.86334 Accuracy: 0.49768\n",
      "Epoch: 232 Batch:   0 Loss: 660.40833 Accuracy: 0.49768\n",
      "Epoch: 233 Batch:   0 Loss: 655.10498 Accuracy: 0.49768\n",
      "Epoch: 234 Batch:   0 Loss: 649.95013 Accuracy: 0.49768\n",
      "Epoch: 235 Batch:   0 Loss: 644.94006 Accuracy: 0.49768\n",
      "Epoch: 236 Batch:   0 Loss: 640.07166 Accuracy: 0.49768\n",
      "Epoch: 237 Batch:   0 Loss: 635.34143 Accuracy: 0.49768\n",
      "Epoch: 238 Batch:   0 Loss: 630.74518 Accuracy: 0.49768\n",
      "Epoch: 239 Batch:   0 Loss: 626.27997 Accuracy: 0.49768\n",
      "Test Loss: 610.76504 Accuracy: 0.49987\n",
      "Epoch: 240 Batch:   0 Loss: 621.94263 Accuracy: 0.49768\n",
      "Epoch: 241 Batch:   0 Loss: 617.73010 Accuracy: 0.49768\n",
      "Epoch: 242 Batch:   0 Loss: 613.63904 Accuracy: 0.49768\n",
      "Epoch: 243 Batch:   0 Loss: 609.66577 Accuracy: 0.49768\n",
      "Epoch: 244 Batch:   0 Loss: 605.80817 Accuracy: 0.49768\n",
      "Epoch: 245 Batch:   0 Loss: 602.06287 Accuracy: 0.49768\n",
      "Epoch: 246 Batch:   0 Loss: 598.42749 Accuracy: 0.49768\n",
      "Epoch: 247 Batch:   0 Loss: 594.89819 Accuracy: 0.49768\n",
      "Epoch: 248 Batch:   0 Loss: 591.47260 Accuracy: 0.49768\n",
      "Epoch: 249 Batch:   0 Loss: 588.14807 Accuracy: 0.49768\n",
      "Test Loss: 577.39506 Accuracy: 0.49987\n",
      "Epoch: 250 Batch:   0 Loss: 584.92230 Accuracy: 0.49768\n",
      "Epoch: 251 Batch:   0 Loss: 581.79236 Accuracy: 0.49768\n",
      "Epoch: 252 Batch:   0 Loss: 578.75574 Accuracy: 0.49768\n",
      "Epoch: 253 Batch:   0 Loss: 575.80975 Accuracy: 0.49768\n",
      "Epoch: 254 Batch:   0 Loss: 572.95215 Accuracy: 0.49768\n",
      "Epoch: 255 Batch:   0 Loss: 570.18066 Accuracy: 0.49768\n",
      "Epoch: 256 Batch:   0 Loss: 567.49280 Accuracy: 0.49768\n",
      "Epoch: 257 Batch:   0 Loss: 564.88605 Accuracy: 0.49768\n",
      "Epoch: 258 Batch:   0 Loss: 562.35864 Accuracy: 0.49768\n",
      "Epoch: 259 Batch:   0 Loss: 559.90851 Accuracy: 0.49717\n",
      "Test Loss: 552.82709 Accuracy: 0.49987\n",
      "Epoch: 260 Batch:   0 Loss: 557.53308 Accuracy: 0.49717\n",
      "Epoch: 261 Batch:   0 Loss: 555.23004 Accuracy: 0.49665\n",
      "Epoch: 262 Batch:   0 Loss: 552.99774 Accuracy: 0.49717\n",
      "Epoch: 263 Batch:   0 Loss: 550.83447 Accuracy: 0.49717\n",
      "Epoch: 264 Batch:   0 Loss: 548.73834 Accuracy: 0.49717\n",
      "Epoch: 265 Batch:   0 Loss: 546.70667 Accuracy: 0.49717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 266 Batch:   0 Loss: 544.73804 Accuracy: 0.49717\n",
      "Epoch: 267 Batch:   0 Loss: 542.83069 Accuracy: 0.49717\n",
      "Epoch: 268 Batch:   0 Loss: 540.98309 Accuracy: 0.49665\n",
      "Epoch: 269 Batch:   0 Loss: 539.19287 Accuracy: 0.49665\n",
      "Test Loss: 534.94272 Accuracy: 0.49961\n",
      "Epoch: 270 Batch:   0 Loss: 537.45929 Accuracy: 0.49665\n",
      "Epoch: 271 Batch:   0 Loss: 535.77985 Accuracy: 0.49717\n",
      "Epoch: 272 Batch:   0 Loss: 534.15332 Accuracy: 0.49717\n",
      "Epoch: 273 Batch:   0 Loss: 532.57812 Accuracy: 0.49717\n",
      "Epoch: 274 Batch:   0 Loss: 531.05249 Accuracy: 0.49665\n",
      "Epoch: 275 Batch:   0 Loss: 529.57520 Accuracy: 0.49665\n",
      "Epoch: 276 Batch:   0 Loss: 528.14502 Accuracy: 0.49665\n",
      "Epoch: 277 Batch:   0 Loss: 526.75995 Accuracy: 0.49665\n",
      "Epoch: 278 Batch:   0 Loss: 525.41925 Accuracy: 0.49665\n",
      "Epoch: 279 Batch:   0 Loss: 524.12109 Accuracy: 0.49717\n",
      "Test Loss: 522.06337 Accuracy: 0.49974\n",
      "Epoch: 280 Batch:   0 Loss: 522.86444 Accuracy: 0.49768\n",
      "Epoch: 281 Batch:   0 Loss: 521.64795 Accuracy: 0.49820\n",
      "Epoch: 282 Batch:   0 Loss: 520.47070 Accuracy: 0.49974\n",
      "Epoch: 283 Batch:   0 Loss: 519.33105 Accuracy: 0.50077\n",
      "Epoch: 284 Batch:   0 Loss: 518.22778 Accuracy: 0.50180\n",
      "Epoch: 285 Batch:   0 Loss: 517.16034 Accuracy: 0.50283\n",
      "Epoch: 286 Batch:   0 Loss: 516.12701 Accuracy: 0.50489\n",
      "Epoch: 287 Batch:   0 Loss: 515.12677 Accuracy: 0.50489\n",
      "Epoch: 288 Batch:   0 Loss: 514.15894 Accuracy: 0.50746\n",
      "Epoch: 289 Batch:   0 Loss: 513.22253 Accuracy: 0.50798\n",
      "Test Loss: 512.87954 Accuracy: 0.49961\n",
      "Epoch: 290 Batch:   0 Loss: 512.31592 Accuracy: 0.50901\n",
      "Epoch: 291 Batch:   0 Loss: 511.43857 Accuracy: 0.51055\n",
      "Epoch: 292 Batch:   0 Loss: 510.58963 Accuracy: 0.51158\n",
      "Epoch: 293 Batch:   0 Loss: 509.76782 Accuracy: 0.51261\n",
      "Epoch: 294 Batch:   0 Loss: 508.97266 Accuracy: 0.51518\n",
      "Epoch: 295 Batch:   0 Loss: 508.20303 Accuracy: 0.51724\n",
      "Epoch: 296 Batch:   0 Loss: 507.45825 Accuracy: 0.51827\n",
      "Epoch: 297 Batch:   0 Loss: 506.73749 Accuracy: 0.51981\n",
      "Epoch: 298 Batch:   0 Loss: 506.03955 Accuracy: 0.52187\n",
      "Epoch: 299 Batch:   0 Loss: 505.36017 Accuracy: 0.52342\n",
      "Test Loss: 506.38304 Accuracy: 0.49922\n",
      "Epoch: 300 Batch:   0 Loss: 504.70712 Accuracy: 0.52702\n",
      "Epoch: 301 Batch:   0 Loss: 504.07520 Accuracy: 0.52856\n",
      "Epoch: 302 Batch:   0 Loss: 503.46347 Accuracy: 0.53165\n",
      "Epoch: 303 Batch:   0 Loss: 502.87125 Accuracy: 0.53320\n",
      "Epoch: 304 Batch:   0 Loss: 502.29785 Accuracy: 0.53834\n",
      "Epoch: 305 Batch:   0 Loss: 501.74136 Accuracy: 0.54195\n",
      "Epoch: 306 Batch:   0 Loss: 501.20273 Accuracy: 0.54658\n",
      "Epoch: 307 Batch:   0 Loss: 500.68085 Accuracy: 0.55018\n",
      "Epoch: 308 Batch:   0 Loss: 500.17554 Accuracy: 0.55687\n",
      "Epoch: 309 Batch:   0 Loss: 499.68607 Accuracy: 0.55841\n",
      "Test Loss: 501.81496 Accuracy: 0.50825\n",
      "Epoch: 310 Batch:   0 Loss: 499.21182 Accuracy: 0.56202\n",
      "Epoch: 311 Batch:   0 Loss: 498.75256 Accuracy: 0.56613\n",
      "Epoch: 312 Batch:   0 Loss: 498.30530 Accuracy: 0.57128\n",
      "Epoch: 313 Batch:   0 Loss: 497.87366 Accuracy: 0.57385\n",
      "Epoch: 314 Batch:   0 Loss: 497.45529 Accuracy: 0.57694\n",
      "Epoch: 315 Batch:   0 Loss: 497.04968 Accuracy: 0.58055\n",
      "Epoch: 316 Batch:   0 Loss: 496.65643 Accuracy: 0.58260\n",
      "Epoch: 317 Batch:   0 Loss: 496.27490 Accuracy: 0.58415\n",
      "Epoch: 318 Batch:   0 Loss: 495.90485 Accuracy: 0.58724\n",
      "Epoch: 319 Batch:   0 Loss: 495.54599 Accuracy: 0.58981\n",
      "Test Loss: 498.60361 Accuracy: 0.51806\n",
      "Epoch: 320 Batch:   0 Loss: 495.19745 Accuracy: 0.59187\n",
      "Epoch: 321 Batch:   0 Loss: 494.85931 Accuracy: 0.59650\n",
      "Epoch: 322 Batch:   0 Loss: 494.53098 Accuracy: 0.60113\n",
      "Epoch: 323 Batch:   0 Loss: 494.21207 Accuracy: 0.60679\n",
      "Epoch: 324 Batch:   0 Loss: 493.90228 Accuracy: 0.60731\n",
      "Epoch: 325 Batch:   0 Loss: 493.60123 Accuracy: 0.61143\n",
      "Epoch: 326 Batch:   0 Loss: 493.30884 Accuracy: 0.61400\n",
      "Epoch: 327 Batch:   0 Loss: 493.02457 Accuracy: 0.61554\n",
      "Epoch: 328 Batch:   0 Loss: 492.74811 Accuracy: 0.61554\n",
      "Epoch: 329 Batch:   0 Loss: 492.47745 Accuracy: 0.61657\n",
      "Test Loss: 496.33469 Accuracy: 0.52553\n",
      "Epoch: 330 Batch:   0 Loss: 492.21170 Accuracy: 0.61863\n",
      "Epoch: 331 Batch:   0 Loss: 491.95676 Accuracy: 0.62172\n",
      "Epoch: 332 Batch:   0 Loss: 491.70636 Accuracy: 0.62584\n",
      "Epoch: 333 Batch:   0 Loss: 491.46399 Accuracy: 0.62841\n",
      "Epoch: 334 Batch:   0 Loss: 491.22803 Accuracy: 0.63201\n",
      "Epoch: 335 Batch:   0 Loss: 490.99796 Accuracy: 0.63664\n",
      "Epoch: 336 Batch:   0 Loss: 490.77365 Accuracy: 0.64179\n",
      "Epoch: 337 Batch:   0 Loss: 490.55481 Accuracy: 0.64282\n",
      "Epoch: 338 Batch:   0 Loss: 490.34131 Accuracy: 0.64488\n",
      "Epoch: 339 Batch:   0 Loss: 490.13281 Accuracy: 0.64900\n",
      "Test Loss: 494.71582 Accuracy: 0.53888\n",
      "Epoch: 340 Batch:   0 Loss: 489.92944 Accuracy: 0.64951\n",
      "Epoch: 341 Batch:   0 Loss: 489.72690 Accuracy: 0.65157\n",
      "Epoch: 342 Batch:   0 Loss: 489.53009 Accuracy: 0.65003\n",
      "Epoch: 343 Batch:   0 Loss: 489.33649 Accuracy: 0.65157\n",
      "Epoch: 344 Batch:   0 Loss: 489.14786 Accuracy: 0.65363\n",
      "Epoch: 345 Batch:   0 Loss: 488.96313 Accuracy: 0.65620\n",
      "Epoch: 346 Batch:   0 Loss: 488.78244 Accuracy: 0.65980\n",
      "Epoch: 347 Batch:   0 Loss: 488.60513 Accuracy: 0.66186\n",
      "Epoch: 348 Batch:   0 Loss: 488.43155 Accuracy: 0.66032\n",
      "Epoch: 349 Batch:   0 Loss: 488.26144 Accuracy: 0.66032\n",
      "Test Loss: 493.51999 Accuracy: 0.55092\n",
      "Epoch: 350 Batch:   0 Loss: 488.09427 Accuracy: 0.66547\n",
      "Epoch: 351 Batch:   0 Loss: 487.93045 Accuracy: 0.66650\n",
      "Epoch: 352 Batch:   0 Loss: 487.76959 Accuracy: 0.66804\n",
      "Epoch: 353 Batch:   0 Loss: 487.61029 Accuracy: 0.67113\n",
      "Epoch: 354 Batch:   0 Loss: 487.45422 Accuracy: 0.67216\n",
      "Epoch: 355 Batch:   0 Loss: 487.30078 Accuracy: 0.67627\n",
      "Epoch: 356 Batch:   0 Loss: 487.14981 Accuracy: 0.67422\n",
      "Epoch: 357 Batch:   0 Loss: 487.00137 Accuracy: 0.67524\n",
      "Epoch: 358 Batch:   0 Loss: 486.85532 Accuracy: 0.67524\n",
      "Epoch: 359 Batch:   0 Loss: 486.71115 Accuracy: 0.67730\n",
      "Test Loss: 492.61373 Accuracy: 0.56440\n",
      "Epoch: 360 Batch:   0 Loss: 486.56924 Accuracy: 0.67576\n",
      "Epoch: 361 Batch:   0 Loss: 486.42944 Accuracy: 0.67473\n",
      "Epoch: 362 Batch:   0 Loss: 486.29150 Accuracy: 0.67576\n",
      "Epoch: 363 Batch:   0 Loss: 486.15555 Accuracy: 0.67782\n",
      "Epoch: 364 Batch:   0 Loss: 486.02142 Accuracy: 0.67576\n",
      "Epoch: 365 Batch:   0 Loss: 485.88879 Accuracy: 0.67422\n",
      "Epoch: 366 Batch:   0 Loss: 485.75787 Accuracy: 0.67267\n",
      "Epoch: 367 Batch:   0 Loss: 485.62854 Accuracy: 0.67370\n",
      "Epoch: 368 Batch:   0 Loss: 485.50070 Accuracy: 0.67267\n",
      "Epoch: 369 Batch:   0 Loss: 485.37427 Accuracy: 0.67267\n",
      "Test Loss: 491.89517 Accuracy: 0.57081\n",
      "Epoch: 370 Batch:   0 Loss: 485.24927 Accuracy: 0.67473\n",
      "Epoch: 371 Batch:   0 Loss: 485.12564 Accuracy: 0.67422\n",
      "Epoch: 372 Batch:   0 Loss: 485.00302 Accuracy: 0.67319\n",
      "Epoch: 373 Batch:   0 Loss: 484.88177 Accuracy: 0.67267\n",
      "Epoch: 374 Batch:   0 Loss: 484.76160 Accuracy: 0.67010\n",
      "Epoch: 375 Batch:   0 Loss: 484.64255 Accuracy: 0.67113\n",
      "Epoch: 376 Batch:   0 Loss: 484.52454 Accuracy: 0.66907\n",
      "Epoch: 377 Batch:   0 Loss: 484.40677 Accuracy: 0.66958\n",
      "Epoch: 378 Batch:   0 Loss: 484.29031 Accuracy: 0.67061\n",
      "Epoch: 379 Batch:   0 Loss: 484.17477 Accuracy: 0.67010\n",
      "Test Loss: 491.29635 Accuracy: 0.57526\n",
      "Epoch: 380 Batch:   0 Loss: 484.06027 Accuracy: 0.67113\n",
      "Epoch: 381 Batch:   0 Loss: 483.94650 Accuracy: 0.67010\n",
      "Epoch: 382 Batch:   0 Loss: 483.83075 Accuracy: 0.67010\n",
      "Epoch: 383 Batch:   0 Loss: 483.71817 Accuracy: 0.67010\n",
      "Epoch: 384 Batch:   0 Loss: 483.60614 Accuracy: 0.67164\n",
      "Epoch: 385 Batch:   0 Loss: 483.49481 Accuracy: 0.67216\n",
      "Epoch: 386 Batch:   0 Loss: 483.38412 Accuracy: 0.67216\n",
      "Epoch: 387 Batch:   0 Loss: 483.27432 Accuracy: 0.67216\n",
      "Epoch: 388 Batch:   0 Loss: 483.16470 Accuracy: 0.67164\n",
      "Epoch: 389 Batch:   0 Loss: 483.05597 Accuracy: 0.67164\n",
      "Test Loss: 490.77531 Accuracy: 0.58101\n",
      "Epoch: 390 Batch:   0 Loss: 482.94757 Accuracy: 0.67113\n",
      "Epoch: 391 Batch:   0 Loss: 482.83981 Accuracy: 0.66855\n",
      "Epoch: 392 Batch:   0 Loss: 482.73260 Accuracy: 0.66958\n",
      "Epoch: 393 Batch:   0 Loss: 482.62573 Accuracy: 0.66907\n",
      "Epoch: 394 Batch:   0 Loss: 482.51938 Accuracy: 0.66855\n",
      "Epoch: 395 Batch:   0 Loss: 482.41342 Accuracy: 0.66958\n",
      "Epoch: 396 Batch:   0 Loss: 482.30786 Accuracy: 0.66855\n",
      "Epoch: 397 Batch:   0 Loss: 482.20264 Accuracy: 0.66650\n",
      "Epoch: 398 Batch:   0 Loss: 482.09775 Accuracy: 0.66701\n",
      "Epoch: 399 Batch:   0 Loss: 481.99359 Accuracy: 0.66804\n",
      "Test Loss: 490.30418 Accuracy: 0.58298\n",
      "Epoch: 400 Batch:   0 Loss: 481.88934 Accuracy: 0.66804\n",
      "Epoch: 401 Batch:   0 Loss: 481.78552 Accuracy: 0.66855\n",
      "Epoch: 402 Batch:   0 Loss: 481.68195 Accuracy: 0.66855\n",
      "Epoch: 403 Batch:   0 Loss: 481.57874 Accuracy: 0.66855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 404 Batch:   0 Loss: 481.47562 Accuracy: 0.66855\n",
      "Epoch: 405 Batch:   0 Loss: 481.37314 Accuracy: 0.66804\n",
      "Epoch: 406 Batch:   0 Loss: 481.27078 Accuracy: 0.66907\n",
      "Epoch: 407 Batch:   0 Loss: 481.16849 Accuracy: 0.66855\n",
      "Epoch: 408 Batch:   0 Loss: 481.06644 Accuracy: 0.66855\n",
      "Epoch: 409 Batch:   0 Loss: 480.96301 Accuracy: 0.66958\n",
      "Test Loss: 489.86151 Accuracy: 0.58233\n",
      "Epoch: 410 Batch:   0 Loss: 480.86218 Accuracy: 0.66907\n",
      "Epoch: 411 Batch:   0 Loss: 480.76151 Accuracy: 0.66958\n",
      "Epoch: 412 Batch:   0 Loss: 480.66104 Accuracy: 0.67113\n",
      "Epoch: 413 Batch:   0 Loss: 480.56070 Accuracy: 0.67061\n",
      "Epoch: 414 Batch:   0 Loss: 480.46063 Accuracy: 0.67164\n",
      "Epoch: 415 Batch:   0 Loss: 480.36047 Accuracy: 0.67113\n",
      "Epoch: 416 Batch:   0 Loss: 480.26077 Accuracy: 0.67010\n",
      "Epoch: 417 Batch:   0 Loss: 480.16089 Accuracy: 0.67010\n",
      "Epoch: 418 Batch:   0 Loss: 480.06131 Accuracy: 0.67061\n",
      "Epoch: 419 Batch:   0 Loss: 479.95935 Accuracy: 0.67061\n",
      "Test Loss: 489.43744 Accuracy: 0.58390\n",
      "Epoch: 420 Batch:   0 Loss: 479.86017 Accuracy: 0.67061\n",
      "Epoch: 421 Batch:   0 Loss: 479.76114 Accuracy: 0.67061\n",
      "Epoch: 422 Batch:   0 Loss: 479.66202 Accuracy: 0.67061\n",
      "Epoch: 423 Batch:   0 Loss: 479.56320 Accuracy: 0.67061\n",
      "Epoch: 424 Batch:   0 Loss: 479.46436 Accuracy: 0.67061\n",
      "Epoch: 425 Batch:   0 Loss: 479.36569 Accuracy: 0.67061\n",
      "Epoch: 426 Batch:   0 Loss: 479.26617 Accuracy: 0.67061\n",
      "Epoch: 427 Batch:   0 Loss: 479.16830 Accuracy: 0.67010\n",
      "Epoch: 428 Batch:   0 Loss: 479.07022 Accuracy: 0.67010\n",
      "Epoch: 429 Batch:   0 Loss: 478.97241 Accuracy: 0.66958\n",
      "Test Loss: 489.01752 Accuracy: 0.58429\n",
      "Epoch: 430 Batch:   0 Loss: 478.87469 Accuracy: 0.66958\n",
      "Epoch: 431 Batch:   0 Loss: 478.77686 Accuracy: 0.66958\n",
      "Epoch: 432 Batch:   0 Loss: 478.67914 Accuracy: 0.66958\n",
      "Epoch: 433 Batch:   0 Loss: 478.58130 Accuracy: 0.66958\n",
      "Epoch: 434 Batch:   0 Loss: 478.48386 Accuracy: 0.66958\n",
      "Epoch: 435 Batch:   0 Loss: 478.38620 Accuracy: 0.66958\n",
      "Epoch: 436 Batch:   0 Loss: 478.28860 Accuracy: 0.66907\n",
      "Epoch: 437 Batch:   0 Loss: 478.19101 Accuracy: 0.66855\n",
      "Epoch: 438 Batch:   0 Loss: 478.09357 Accuracy: 0.66907\n",
      "Epoch: 439 Batch:   0 Loss: 477.99600 Accuracy: 0.66855\n",
      "Test Loss: 488.60593 Accuracy: 0.58468\n",
      "Epoch: 440 Batch:   0 Loss: 477.89578 Accuracy: 0.66804\n",
      "Epoch: 441 Batch:   0 Loss: 477.79703 Accuracy: 0.66804\n",
      "Epoch: 442 Batch:   0 Loss: 477.69833 Accuracy: 0.66855\n",
      "Epoch: 443 Batch:   0 Loss: 477.59946 Accuracy: 0.66804\n",
      "Epoch: 444 Batch:   0 Loss: 477.50061 Accuracy: 0.66804\n",
      "Epoch: 445 Batch:   0 Loss: 477.40186 Accuracy: 0.66804\n",
      "Epoch: 446 Batch:   0 Loss: 477.30304 Accuracy: 0.66804\n",
      "Epoch: 447 Batch:   0 Loss: 477.20422 Accuracy: 0.66855\n",
      "Epoch: 448 Batch:   0 Loss: 477.10559 Accuracy: 0.66855\n",
      "Epoch: 449 Batch:   0 Loss: 477.00677 Accuracy: 0.66855\n",
      "Test Loss: 488.19259 Accuracy: 0.58520\n",
      "Epoch: 450 Batch:   0 Loss: 476.90787 Accuracy: 0.66855\n",
      "Epoch: 451 Batch:   0 Loss: 476.80908 Accuracy: 0.66907\n",
      "Epoch: 452 Batch:   0 Loss: 476.71024 Accuracy: 0.66907\n",
      "Epoch: 453 Batch:   0 Loss: 476.61142 Accuracy: 0.66907\n",
      "Epoch: 454 Batch:   0 Loss: 476.51254 Accuracy: 0.66958\n",
      "Epoch: 455 Batch:   0 Loss: 476.41364 Accuracy: 0.66958\n",
      "Epoch: 456 Batch:   0 Loss: 476.31488 Accuracy: 0.66958\n",
      "Epoch: 457 Batch:   0 Loss: 476.21594 Accuracy: 0.66958\n",
      "Epoch: 458 Batch:   0 Loss: 476.11707 Accuracy: 0.66958\n",
      "Epoch: 459 Batch:   0 Loss: 476.01807 Accuracy: 0.66958\n",
      "Test Loss: 487.77889 Accuracy: 0.58507\n",
      "Epoch: 460 Batch:   0 Loss: 475.91904 Accuracy: 0.67010\n",
      "Epoch: 461 Batch:   0 Loss: 475.81772 Accuracy: 0.67010\n",
      "Epoch: 462 Batch:   0 Loss: 475.71832 Accuracy: 0.67010\n",
      "Epoch: 463 Batch:   0 Loss: 475.61908 Accuracy: 0.67010\n",
      "Epoch: 464 Batch:   0 Loss: 475.51965 Accuracy: 0.67010\n",
      "Epoch: 465 Batch:   0 Loss: 475.42032 Accuracy: 0.67010\n",
      "Epoch: 466 Batch:   0 Loss: 475.32095 Accuracy: 0.67010\n",
      "Epoch: 467 Batch:   0 Loss: 475.22144 Accuracy: 0.67010\n",
      "Epoch: 468 Batch:   0 Loss: 475.12195 Accuracy: 0.67061\n",
      "Epoch: 469 Batch:   0 Loss: 475.02240 Accuracy: 0.67061\n",
      "Test Loss: 487.36572 Accuracy: 0.58455\n",
      "Epoch: 470 Batch:   0 Loss: 474.92285 Accuracy: 0.67061\n",
      "Epoch: 471 Batch:   0 Loss: 474.82315 Accuracy: 0.67061\n",
      "Epoch: 472 Batch:   0 Loss: 474.72095 Accuracy: 0.67061\n",
      "Epoch: 473 Batch:   0 Loss: 474.62131 Accuracy: 0.67061\n",
      "Epoch: 474 Batch:   0 Loss: 474.52127 Accuracy: 0.67061\n",
      "Epoch: 475 Batch:   0 Loss: 474.42160 Accuracy: 0.67061\n",
      "Epoch: 476 Batch:   0 Loss: 474.32159 Accuracy: 0.67010\n",
      "Epoch: 477 Batch:   0 Loss: 474.22162 Accuracy: 0.66958\n",
      "Epoch: 478 Batch:   0 Loss: 474.12170 Accuracy: 0.66958\n",
      "Epoch: 479 Batch:   0 Loss: 474.02161 Accuracy: 0.66907\n",
      "Test Loss: 486.94414 Accuracy: 0.58442\n",
      "Epoch: 480 Batch:   0 Loss: 473.92166 Accuracy: 0.66907\n",
      "Epoch: 481 Batch:   0 Loss: 473.82144 Accuracy: 0.66907\n",
      "Epoch: 482 Batch:   0 Loss: 473.72134 Accuracy: 0.66907\n",
      "Epoch: 483 Batch:   0 Loss: 473.62119 Accuracy: 0.66907\n",
      "Epoch: 484 Batch:   0 Loss: 473.52078 Accuracy: 0.66907\n",
      "Epoch: 485 Batch:   0 Loss: 473.42044 Accuracy: 0.66907\n",
      "Epoch: 486 Batch:   0 Loss: 473.32019 Accuracy: 0.66907\n",
      "Epoch: 487 Batch:   0 Loss: 473.21976 Accuracy: 0.66907\n",
      "Epoch: 488 Batch:   0 Loss: 473.11920 Accuracy: 0.66907\n",
      "Epoch: 489 Batch:   0 Loss: 473.01678 Accuracy: 0.66907\n",
      "Test Loss: 486.51868 Accuracy: 0.58377\n",
      "Epoch: 490 Batch:   0 Loss: 472.91541 Accuracy: 0.66907\n",
      "Epoch: 491 Batch:   0 Loss: 472.81375 Accuracy: 0.66907\n",
      "Epoch: 492 Batch:   0 Loss: 472.71228 Accuracy: 0.66907\n",
      "Epoch: 493 Batch:   0 Loss: 472.61078 Accuracy: 0.66855\n",
      "Epoch: 494 Batch:   0 Loss: 472.50906 Accuracy: 0.66855\n",
      "Epoch: 495 Batch:   0 Loss: 472.40735 Accuracy: 0.66855\n",
      "Epoch: 496 Batch:   0 Loss: 472.30548 Accuracy: 0.66855\n",
      "Epoch: 497 Batch:   0 Loss: 472.20367 Accuracy: 0.66855\n",
      "Epoch: 498 Batch:   0 Loss: 472.10178 Accuracy: 0.66907\n",
      "Epoch: 499 Batch:   0 Loss: 471.99988 Accuracy: 0.66907\n",
      "Test Loss: 486.08755 Accuracy: 0.58298\n",
      "Epoch: 500 Batch:   0 Loss: 471.89777 Accuracy: 0.66907\n",
      "Epoch: 501 Batch:   0 Loss: 471.79562 Accuracy: 0.66907\n",
      "Epoch: 502 Batch:   0 Loss: 471.69214 Accuracy: 0.66907\n",
      "Epoch: 503 Batch:   0 Loss: 471.58871 Accuracy: 0.66958\n",
      "Epoch: 504 Batch:   0 Loss: 471.48535 Accuracy: 0.66958\n",
      "Epoch: 505 Batch:   0 Loss: 471.38190 Accuracy: 0.66958\n",
      "Epoch: 506 Batch:   0 Loss: 471.27832 Accuracy: 0.66958\n",
      "Epoch: 507 Batch:   0 Loss: 471.17462 Accuracy: 0.66958\n",
      "Epoch: 508 Batch:   0 Loss: 471.07098 Accuracy: 0.66958\n",
      "Epoch: 509 Batch:   0 Loss: 470.96729 Accuracy: 0.66958\n",
      "Test Loss: 485.64956 Accuracy: 0.58285\n",
      "Epoch: 510 Batch:   0 Loss: 470.86346 Accuracy: 0.66958\n",
      "Epoch: 511 Batch:   0 Loss: 470.75903 Accuracy: 0.66958\n",
      "Epoch: 512 Batch:   0 Loss: 470.65506 Accuracy: 0.66958\n",
      "Epoch: 513 Batch:   0 Loss: 470.55109 Accuracy: 0.66958\n",
      "Epoch: 514 Batch:   0 Loss: 470.44690 Accuracy: 0.66958\n",
      "Epoch: 515 Batch:   0 Loss: 470.34262 Accuracy: 0.66958\n",
      "Epoch: 516 Batch:   0 Loss: 470.23831 Accuracy: 0.66958\n",
      "Epoch: 517 Batch:   0 Loss: 470.13400 Accuracy: 0.66907\n",
      "Epoch: 518 Batch:   0 Loss: 470.02948 Accuracy: 0.66907\n",
      "Epoch: 519 Batch:   0 Loss: 469.92459 Accuracy: 0.66907\n",
      "Test Loss: 485.20638 Accuracy: 0.58246\n",
      "Epoch: 520 Batch:   0 Loss: 469.81995 Accuracy: 0.66907\n",
      "Epoch: 521 Batch:   0 Loss: 469.71542 Accuracy: 0.66907\n",
      "Epoch: 522 Batch:   0 Loss: 469.61078 Accuracy: 0.66907\n",
      "Epoch: 523 Batch:   0 Loss: 469.50589 Accuracy: 0.66907\n",
      "Epoch: 524 Batch:   0 Loss: 469.40115 Accuracy: 0.66907\n",
      "Epoch: 525 Batch:   0 Loss: 469.29617 Accuracy: 0.66855\n",
      "Epoch: 526 Batch:   0 Loss: 469.19122 Accuracy: 0.66855\n",
      "Epoch: 527 Batch:   0 Loss: 469.08609 Accuracy: 0.66855\n",
      "Epoch: 528 Batch:   0 Loss: 468.98093 Accuracy: 0.66907\n",
      "Epoch: 529 Batch:   0 Loss: 468.87561 Accuracy: 0.66907\n",
      "Test Loss: 484.75961 Accuracy: 0.58181\n",
      "Epoch: 530 Batch:   0 Loss: 468.77023 Accuracy: 0.66907\n",
      "Epoch: 531 Batch:   0 Loss: 468.66492 Accuracy: 0.66907\n",
      "Epoch: 532 Batch:   0 Loss: 468.55850 Accuracy: 0.66907\n",
      "Epoch: 533 Batch:   0 Loss: 468.45242 Accuracy: 0.66907\n",
      "Epoch: 534 Batch:   0 Loss: 468.34631 Accuracy: 0.66907\n",
      "Epoch: 535 Batch:   0 Loss: 468.23935 Accuracy: 0.66907\n",
      "Epoch: 536 Batch:   0 Loss: 468.13242 Accuracy: 0.66907\n",
      "Epoch: 537 Batch:   0 Loss: 468.02554 Accuracy: 0.66907\n",
      "Epoch: 538 Batch:   0 Loss: 467.91824 Accuracy: 0.66855\n",
      "Epoch: 539 Batch:   0 Loss: 467.81113 Accuracy: 0.66855\n",
      "Test Loss: 484.29819 Accuracy: 0.58181\n",
      "Epoch: 540 Batch:   0 Loss: 467.70389 Accuracy: 0.66855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 541 Batch:   0 Loss: 467.59650 Accuracy: 0.66855\n",
      "Epoch: 542 Batch:   0 Loss: 467.48901 Accuracy: 0.66855\n",
      "Epoch: 543 Batch:   0 Loss: 467.38144 Accuracy: 0.66855\n",
      "Epoch: 544 Batch:   0 Loss: 467.27380 Accuracy: 0.66855\n",
      "Epoch: 545 Batch:   0 Loss: 467.16605 Accuracy: 0.66855\n",
      "Epoch: 546 Batch:   0 Loss: 467.05640 Accuracy: 0.66907\n",
      "Epoch: 547 Batch:   0 Loss: 466.94827 Accuracy: 0.66907\n",
      "Epoch: 548 Batch:   0 Loss: 466.84006 Accuracy: 0.66907\n",
      "Epoch: 549 Batch:   0 Loss: 466.73169 Accuracy: 0.66907\n",
      "Test Loss: 483.82401 Accuracy: 0.58180\n",
      "Epoch: 550 Batch:   0 Loss: 466.62329 Accuracy: 0.66907\n",
      "Epoch: 551 Batch:   0 Loss: 466.51483 Accuracy: 0.66907\n",
      "Epoch: 552 Batch:   0 Loss: 466.40616 Accuracy: 0.66907\n",
      "Epoch: 553 Batch:   0 Loss: 466.29758 Accuracy: 0.66907\n",
      "Epoch: 554 Batch:   0 Loss: 466.18875 Accuracy: 0.66907\n",
      "Epoch: 555 Batch:   0 Loss: 466.07990 Accuracy: 0.66907\n",
      "Epoch: 556 Batch:   0 Loss: 465.97086 Accuracy: 0.66907\n",
      "Epoch: 557 Batch:   0 Loss: 465.85962 Accuracy: 0.66907\n",
      "Epoch: 558 Batch:   0 Loss: 465.74982 Accuracy: 0.66907\n",
      "Epoch: 559 Batch:   0 Loss: 465.63995 Accuracy: 0.66907\n",
      "Test Loss: 483.34075 Accuracy: 0.58194\n",
      "Epoch: 560 Batch:   0 Loss: 465.52991 Accuracy: 0.66855\n",
      "Epoch: 561 Batch:   0 Loss: 465.41989 Accuracy: 0.66855\n",
      "Epoch: 562 Batch:   0 Loss: 465.30954 Accuracy: 0.66804\n",
      "Epoch: 563 Batch:   0 Loss: 465.19919 Accuracy: 0.66804\n",
      "Epoch: 564 Batch:   0 Loss: 465.08881 Accuracy: 0.66752\n",
      "Epoch: 565 Batch:   0 Loss: 464.97824 Accuracy: 0.66752\n",
      "Epoch: 566 Batch:   0 Loss: 464.86752 Accuracy: 0.66752\n",
      "Epoch: 567 Batch:   0 Loss: 464.75677 Accuracy: 0.66752\n",
      "Epoch: 568 Batch:   0 Loss: 464.64566 Accuracy: 0.66752\n",
      "Epoch: 569 Batch:   0 Loss: 464.53482 Accuracy: 0.66752\n",
      "Test Loss: 482.85177 Accuracy: 0.58154\n",
      "Epoch: 570 Batch:   0 Loss: 464.42368 Accuracy: 0.66752\n",
      "Epoch: 571 Batch:   0 Loss: 464.31241 Accuracy: 0.66752\n",
      "Epoch: 572 Batch:   0 Loss: 464.20096 Accuracy: 0.66804\n",
      "Epoch: 573 Batch:   0 Loss: 464.08841 Accuracy: 0.66855\n",
      "Epoch: 574 Batch:   0 Loss: 463.97794 Accuracy: 0.66855\n",
      "Epoch: 575 Batch:   0 Loss: 463.86725 Accuracy: 0.66855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-05 15:54:29,116] Trial 0 finished with value: 0.3663150752341066 and parameters: {'REG_W': 4.20001642467615e-06, 'REG_B': 0.002838626418152933, 'REG_Z': 2.0542461864837463e-05, 'SPAR_W': 0.8176662878132267, 'SPAR_B': 0.6441024812808256, 'SPAR_Z': 0.8082769449948486, 'loss': 'l2', 'LEARNING_RATE': 0.00021258085967771273, 'NUM_EPOCHS': 576, 'alpha': 0.11907989359869131}. Best is trial 0 with value: 0.3663150752341066.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 Batch:   0 Loss: 10902.04980 Accuracy: 0.49768\n",
      "Epoch:   1 Batch:   0 Loss: 10324.69238 Accuracy: 0.49768\n",
      "Epoch:   2 Batch:   0 Loss: 9766.38184 Accuracy: 0.49768\n",
      "Epoch:   3 Batch:   0 Loss: 9232.92188 Accuracy: 0.49768\n",
      "Epoch:   4 Batch:   0 Loss: 8725.19629 Accuracy: 0.49768\n",
      "Epoch:   5 Batch:   0 Loss: 8243.52930 Accuracy: 0.49768\n",
      "Epoch:   6 Batch:   0 Loss: 7787.74902 Accuracy: 0.49768\n",
      "Epoch:   7 Batch:   0 Loss: 7357.27783 Accuracy: 0.49768\n",
      "Epoch:   8 Batch:   0 Loss: 6951.28418 Accuracy: 0.49768\n",
      "Epoch:   9 Batch:   0 Loss: 6568.71973 Accuracy: 0.49768\n",
      "Test Loss: 5961.26831 Accuracy: 0.49987\n",
      "Epoch:  10 Batch:   0 Loss: 6208.46729 Accuracy: 0.49768\n",
      "Epoch:  11 Batch:   0 Loss: 5869.32812 Accuracy: 0.49768\n",
      "Epoch:  12 Batch:   0 Loss: 5550.10400 Accuracy: 0.49768\n",
      "Epoch:  13 Batch:   0 Loss: 5249.61816 Accuracy: 0.49768\n",
      "Epoch:  14 Batch:   0 Loss: 4966.71826 Accuracy: 0.49768\n",
      "Epoch:  15 Batch:   0 Loss: 4700.31396 Accuracy: 0.49768\n",
      "Epoch:  16 Batch:   0 Loss: 4449.35840 Accuracy: 0.49768\n",
      "Epoch:  17 Batch:   0 Loss: 4212.86670 Accuracy: 0.49768\n",
      "Epoch:  18 Batch:   0 Loss: 3989.92139 Accuracy: 0.49768\n",
      "Epoch:  19 Batch:   0 Loss: 3779.66504 Accuracy: 0.49768\n",
      "Test Loss: 3439.10281 Accuracy: 0.49987\n",
      "Epoch:  20 Batch:   0 Loss: 3581.29834 Accuracy: 0.49768\n",
      "Epoch:  21 Batch:   0 Loss: 3394.08301 Accuracy: 0.49768\n",
      "Epoch:  22 Batch:   0 Loss: 3217.34058 Accuracy: 0.49768\n",
      "Epoch:  23 Batch:   0 Loss: 3050.44775 Accuracy: 0.49768\n",
      "Epoch:  24 Batch:   0 Loss: 2892.82227 Accuracy: 0.49768\n",
      "Epoch:  25 Batch:   0 Loss: 2743.93823 Accuracy: 0.49768\n",
      "Epoch:  26 Batch:   0 Loss: 2603.31055 Accuracy: 0.49768\n",
      "Epoch:  27 Batch:   0 Loss: 2470.48315 Accuracy: 0.49768\n",
      "Epoch:  28 Batch:   0 Loss: 2345.04102 Accuracy: 0.49768\n",
      "Epoch:  29 Batch:   0 Loss: 2226.59839 Accuracy: 0.49768\n",
      "Test Loss: 2035.66576 Accuracy: 0.49987\n",
      "Epoch:  30 Batch:   0 Loss: 2114.79346 Accuracy: 0.49768\n",
      "Epoch:  31 Batch:   0 Loss: 2009.29041 Accuracy: 0.49768\n",
      "Epoch:  32 Batch:   0 Loss: 1909.77625 Accuracy: 0.49768\n",
      "Epoch:  33 Batch:   0 Loss: 1815.95288 Accuracy: 0.49768\n",
      "Epoch:  34 Batch:   0 Loss: 1727.53845 Accuracy: 0.49768\n",
      "Epoch:  35 Batch:   0 Loss: 1644.26672 Accuracy: 0.49768\n",
      "Epoch:  36 Batch:   0 Loss: 1565.88464 Accuracy: 0.49768\n",
      "Epoch:  37 Batch:   0 Loss: 1492.15161 Accuracy: 0.49768\n",
      "Epoch:  38 Batch:   0 Loss: 1422.83789 Accuracy: 0.49768\n",
      "Epoch:  39 Batch:   0 Loss: 1357.72058 Accuracy: 0.49768\n",
      "Test Loss: 1254.17227 Accuracy: 0.49987\n",
      "Epoch:  40 Batch:   0 Loss: 1296.59094 Accuracy: 0.49768\n",
      "Epoch:  41 Batch:   0 Loss: 1239.24463 Accuracy: 0.49768\n",
      "Epoch:  42 Batch:   0 Loss: 1185.48755 Accuracy: 0.49768\n",
      "Epoch:  43 Batch:   0 Loss: 1135.13318 Accuracy: 0.49768\n",
      "Epoch:  44 Batch:   0 Loss: 1088.00269 Accuracy: 0.49768\n",
      "Epoch:  45 Batch:   0 Loss: 1043.92297 Accuracy: 0.49768\n",
      "Epoch:  46 Batch:   0 Loss: 1002.73065 Accuracy: 0.49768\n",
      "Epoch:  47 Batch:   0 Loss: 964.26538 Accuracy: 0.49768\n",
      "Epoch:  48 Batch:   0 Loss: 928.37695 Accuracy: 0.49768\n",
      "Epoch:  49 Batch:   0 Loss: 894.91913 Accuracy: 0.49768\n",
      "Test Loss: 841.31544 Accuracy: 0.49987\n",
      "Epoch:  50 Batch:   0 Loss: 863.75372 Accuracy: 0.49768\n",
      "Epoch:  51 Batch:   0 Loss: 834.74786 Accuracy: 0.49768\n",
      "Epoch:  52 Batch:   0 Loss: 807.77325 Accuracy: 0.49768\n",
      "Epoch:  53 Batch:   0 Loss: 782.71002 Accuracy: 0.49768\n",
      "Epoch:  54 Batch:   0 Loss: 759.44080 Accuracy: 0.49768\n",
      "Epoch:  55 Batch:   0 Loss: 737.85651 Accuracy: 0.49768\n",
      "Epoch:  56 Batch:   0 Loss: 717.85187 Accuracy: 0.49768\n",
      "Epoch:  57 Batch:   0 Loss: 699.32776 Accuracy: 0.49768\n",
      "Epoch:  58 Batch:   0 Loss: 682.18774 Accuracy: 0.49768\n",
      "Epoch:  59 Batch:   0 Loss: 666.34320 Accuracy: 0.49768\n",
      "Test Loss: 639.34304 Accuracy: 0.49987\n",
      "Epoch:  60 Batch:   0 Loss: 651.70850 Accuracy: 0.49768\n",
      "Epoch:  61 Batch:   0 Loss: 638.20245 Accuracy: 0.49768\n",
      "Epoch:  62 Batch:   0 Loss: 625.74878 Accuracy: 0.49768\n",
      "Epoch:  63 Batch:   0 Loss: 614.27563 Accuracy: 0.49768\n",
      "Epoch:  64 Batch:   0 Loss: 603.71411 Accuracy: 0.49768\n",
      "Epoch:  65 Batch:   0 Loss: 594.00098 Accuracy: 0.49768\n",
      "Epoch:  66 Batch:   0 Loss: 585.07495 Accuracy: 0.49768\n",
      "Epoch:  67 Batch:   0 Loss: 576.87903 Accuracy: 0.49768\n",
      "Epoch:  68 Batch:   0 Loss: 569.36035 Accuracy: 0.49768\n",
      "Epoch:  69 Batch:   0 Loss: 562.46796 Accuracy: 0.49768\n",
      "Test Loss: 548.51449 Accuracy: 0.49987\n",
      "Epoch:  70 Batch:   0 Loss: 556.15503 Accuracy: 0.49768\n",
      "Epoch:  71 Batch:   0 Loss: 550.37744 Accuracy: 0.49768\n",
      "Epoch:  72 Batch:   0 Loss: 545.09369 Accuracy: 0.49717\n",
      "Epoch:  73 Batch:   0 Loss: 540.26599 Accuracy: 0.49717\n",
      "Epoch:  74 Batch:   0 Loss: 535.85742 Accuracy: 0.49717\n",
      "Epoch:  75 Batch:   0 Loss: 531.83490 Accuracy: 0.49717\n",
      "Epoch:  76 Batch:   0 Loss: 528.16754 Accuracy: 0.49717\n",
      "Epoch:  77 Batch:   0 Loss: 524.82587 Accuracy: 0.49717\n",
      "Epoch:  78 Batch:   0 Loss: 521.78302 Accuracy: 0.49717\n",
      "Epoch:  79 Batch:   0 Loss: 519.01404 Accuracy: 0.49717\n",
      "Test Loss: 510.97650 Accuracy: 0.49987\n",
      "Epoch:  80 Batch:   0 Loss: 516.49603 Accuracy: 0.49665\n",
      "Epoch:  81 Batch:   0 Loss: 514.20728 Accuracy: 0.49665\n",
      "Epoch:  82 Batch:   0 Loss: 512.12805 Accuracy: 0.49665\n",
      "Epoch:  83 Batch:   0 Loss: 510.23987 Accuracy: 0.49665\n",
      "Epoch:  84 Batch:   0 Loss: 508.52609 Accuracy: 0.49665\n",
      "Epoch:  85 Batch:   0 Loss: 506.97101 Accuracy: 0.49665\n",
      "Epoch:  86 Batch:   0 Loss: 505.56039 Accuracy: 0.49665\n",
      "Epoch:  87 Batch:   0 Loss: 504.28091 Accuracy: 0.49665\n",
      "Epoch:  88 Batch:   0 Loss: 503.12073 Accuracy: 0.49665\n",
      "Epoch:  89 Batch:   0 Loss: 502.06863 Accuracy: 0.49563\n",
      "Test Loss: 496.56868 Accuracy: 0.49791\n",
      "Epoch:  90 Batch:   0 Loss: 501.11453 Accuracy: 0.49563\n",
      "Epoch:  91 Batch:   0 Loss: 500.24905 Accuracy: 0.49614\n",
      "Epoch:  92 Batch:   0 Loss: 499.46396 Accuracy: 0.49717\n",
      "Epoch:  93 Batch:   0 Loss: 498.75089 Accuracy: 0.49717\n",
      "Epoch:  94 Batch:   0 Loss: 498.10352 Accuracy: 0.49871\n",
      "Epoch:  95 Batch:   0 Loss: 497.51520 Accuracy: 0.49974\n",
      "Epoch:  96 Batch:   0 Loss: 496.97998 Accuracy: 0.50077\n",
      "Epoch:  97 Batch:   0 Loss: 496.49261 Accuracy: 0.50129\n",
      "Epoch:  98 Batch:   0 Loss: 496.04810 Accuracy: 0.50026\n",
      "Epoch:  99 Batch:   0 Loss: 495.64236 Accuracy: 0.50437\n",
      "Test Loss: 491.23498 Accuracy: 0.48162\n",
      "Epoch: 100 Batch:   0 Loss: 495.27133 Accuracy: 0.49820\n",
      "Epoch: 101 Batch:   0 Loss: 494.93109 Accuracy: 0.50386\n",
      "Epoch: 102 Batch:   0 Loss: 494.61896 Accuracy: 0.52084\n",
      "Epoch: 103 Batch:   0 Loss: 494.33157 Accuracy: 0.54195\n",
      "Epoch: 104 Batch:   0 Loss: 494.06650 Accuracy: 0.56047\n",
      "Epoch: 105 Batch:   0 Loss: 493.82141 Accuracy: 0.57488\n",
      "Epoch: 106 Batch:   0 Loss: 493.59399 Accuracy: 0.60165\n",
      "Epoch: 107 Batch:   0 Loss: 493.38229 Accuracy: 0.61915\n",
      "Epoch: 108 Batch:   0 Loss: 493.18475 Accuracy: 0.64488\n",
      "Epoch: 109 Batch:   0 Loss: 492.99991 Accuracy: 0.65672\n",
      "Test Loss: 489.12389 Accuracy: 0.59160\n",
      "Epoch: 110 Batch:   0 Loss: 492.82626 Accuracy: 0.66701\n",
      "Epoch: 111 Batch:   0 Loss: 492.66241 Accuracy: 0.67679\n",
      "Epoch: 112 Batch:   0 Loss: 492.50745 Accuracy: 0.68863\n",
      "Epoch: 113 Batch:   0 Loss: 492.36014 Accuracy: 0.69223\n",
      "Epoch: 114 Batch:   0 Loss: 492.21979 Accuracy: 0.69429\n",
      "Epoch: 115 Batch:   0 Loss: 492.08563 Accuracy: 0.69480\n",
      "Epoch: 116 Batch:   0 Loss: 491.95697 Accuracy: 0.69583\n",
      "Epoch: 117 Batch:   0 Loss: 491.83316 Accuracy: 0.69583\n",
      "Epoch: 118 Batch:   0 Loss: 491.71341 Accuracy: 0.69738\n",
      "Epoch: 119 Batch:   0 Loss: 491.59756 Accuracy: 0.69892\n",
      "Test Loss: 488.05402 Accuracy: 0.63947\n",
      "Epoch: 120 Batch:   0 Loss: 491.48483 Accuracy: 0.69789\n",
      "Epoch: 121 Batch:   0 Loss: 491.37512 Accuracy: 0.69738\n",
      "Epoch: 122 Batch:   0 Loss: 491.26797 Accuracy: 0.69377\n",
      "Epoch: 123 Batch:   0 Loss: 491.16302 Accuracy: 0.69274\n",
      "Epoch: 124 Batch:   0 Loss: 491.06003 Accuracy: 0.69223\n",
      "Epoch: 125 Batch:   0 Loss: 490.95880 Accuracy: 0.68966\n",
      "Epoch: 126 Batch:   0 Loss: 490.85910 Accuracy: 0.68966\n",
      "Epoch: 127 Batch:   0 Loss: 490.76065 Accuracy: 0.68863\n",
      "Epoch: 128 Batch:   0 Loss: 490.66348 Accuracy: 0.68863\n",
      "Epoch: 129 Batch:   0 Loss: 490.56732 Accuracy: 0.68863\n",
      "Test Loss: 487.29360 Accuracy: 0.64327\n",
      "Epoch: 130 Batch:   0 Loss: 490.47214 Accuracy: 0.68708\n",
      "Epoch: 131 Batch:   0 Loss: 490.37750 Accuracy: 0.68708\n",
      "Epoch: 132 Batch:   0 Loss: 490.28360 Accuracy: 0.68605\n",
      "Epoch: 133 Batch:   0 Loss: 490.19040 Accuracy: 0.68554\n",
      "Epoch: 134 Batch:   0 Loss: 490.09753 Accuracy: 0.68554\n",
      "Epoch: 135 Batch:   0 Loss: 490.00513 Accuracy: 0.68502\n",
      "Epoch: 136 Batch:   0 Loss: 489.91318 Accuracy: 0.68399\n",
      "Epoch: 137 Batch:   0 Loss: 489.82156 Accuracy: 0.68348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 138 Batch:   0 Loss: 489.73010 Accuracy: 0.68296\n",
      "Epoch: 139 Batch:   0 Loss: 489.63882 Accuracy: 0.68348\n",
      "Test Loss: 486.62061 Accuracy: 0.64537\n",
      "Epoch: 140 Batch:   0 Loss: 489.54791 Accuracy: 0.68399\n",
      "Epoch: 141 Batch:   0 Loss: 489.45700 Accuracy: 0.68348\n",
      "Epoch: 142 Batch:   0 Loss: 489.36633 Accuracy: 0.68399\n",
      "Epoch: 143 Batch:   0 Loss: 489.27560 Accuracy: 0.68451\n",
      "Epoch: 144 Batch:   0 Loss: 489.18503 Accuracy: 0.68554\n",
      "Epoch: 145 Batch:   0 Loss: 489.09460 Accuracy: 0.68605\n",
      "Epoch: 146 Batch:   0 Loss: 489.00403 Accuracy: 0.68605\n",
      "Epoch: 147 Batch:   0 Loss: 488.91354 Accuracy: 0.68657\n",
      "Epoch: 148 Batch:   0 Loss: 488.82300 Accuracy: 0.68657\n",
      "Epoch: 149 Batch:   0 Loss: 488.73270 Accuracy: 0.68657\n",
      "Test Loss: 485.97265 Accuracy: 0.64445\n",
      "Epoch: 150 Batch:   0 Loss: 488.64221 Accuracy: 0.68708\n",
      "Epoch: 151 Batch:   0 Loss: 488.55157 Accuracy: 0.68708\n",
      "Epoch: 152 Batch:   0 Loss: 488.46109 Accuracy: 0.68708\n",
      "Epoch: 153 Batch:   0 Loss: 488.37051 Accuracy: 0.68708\n",
      "Epoch: 154 Batch:   0 Loss: 488.27985 Accuracy: 0.68708\n",
      "Epoch: 155 Batch:   0 Loss: 488.18903 Accuracy: 0.68708\n",
      "Epoch: 156 Batch:   0 Loss: 488.09821 Accuracy: 0.68811\n",
      "Epoch: 157 Batch:   0 Loss: 488.00739 Accuracy: 0.68811\n",
      "Epoch: 158 Batch:   0 Loss: 487.91632 Accuracy: 0.68863\n",
      "Epoch: 159 Batch:   0 Loss: 487.82535 Accuracy: 0.68914\n",
      "Test Loss: 485.33326 Accuracy: 0.64523\n",
      "Epoch: 160 Batch:   0 Loss: 487.73419 Accuracy: 0.68863\n",
      "Epoch: 161 Batch:   0 Loss: 487.64297 Accuracy: 0.69017\n",
      "Epoch: 162 Batch:   0 Loss: 487.55167 Accuracy: 0.69017\n",
      "Epoch: 163 Batch:   0 Loss: 487.46024 Accuracy: 0.69068\n",
      "Epoch: 164 Batch:   0 Loss: 487.36874 Accuracy: 0.69068\n",
      "Epoch: 165 Batch:   0 Loss: 487.27710 Accuracy: 0.69068\n",
      "Epoch: 166 Batch:   0 Loss: 487.18539 Accuracy: 0.69068\n",
      "Epoch: 167 Batch:   0 Loss: 487.09354 Accuracy: 0.69068\n",
      "Epoch: 168 Batch:   0 Loss: 487.00146 Accuracy: 0.69068\n",
      "Epoch: 169 Batch:   0 Loss: 486.90933 Accuracy: 0.69068\n",
      "Test Loss: 484.69779 Accuracy: 0.64628\n",
      "Epoch: 170 Batch:   0 Loss: 486.81723 Accuracy: 0.69120\n",
      "Epoch: 171 Batch:   0 Loss: 486.72470 Accuracy: 0.69120\n",
      "Epoch: 172 Batch:   0 Loss: 486.63245 Accuracy: 0.69120\n",
      "Epoch: 173 Batch:   0 Loss: 486.53973 Accuracy: 0.69068\n",
      "Epoch: 174 Batch:   0 Loss: 486.44705 Accuracy: 0.69068\n",
      "Epoch: 175 Batch:   0 Loss: 486.35425 Accuracy: 0.69017\n",
      "Epoch: 176 Batch:   0 Loss: 486.26120 Accuracy: 0.68966\n",
      "Epoch: 177 Batch:   0 Loss: 486.16815 Accuracy: 0.68914\n",
      "Epoch: 178 Batch:   0 Loss: 486.07486 Accuracy: 0.68863\n",
      "Epoch: 179 Batch:   0 Loss: 485.98148 Accuracy: 0.68966\n",
      "Test Loss: 484.06487 Accuracy: 0.64653\n",
      "Epoch: 180 Batch:   0 Loss: 485.88800 Accuracy: 0.68914\n",
      "Epoch: 181 Batch:   0 Loss: 485.79428 Accuracy: 0.68914\n",
      "Epoch: 182 Batch:   0 Loss: 485.70053 Accuracy: 0.68914\n",
      "Epoch: 183 Batch:   0 Loss: 485.60666 Accuracy: 0.68811\n",
      "Epoch: 184 Batch:   0 Loss: 485.51248 Accuracy: 0.68863\n",
      "Epoch: 185 Batch:   0 Loss: 485.41833 Accuracy: 0.68966\n",
      "Epoch: 186 Batch:   0 Loss: 485.32394 Accuracy: 0.69017\n",
      "Epoch: 187 Batch:   0 Loss: 485.22925 Accuracy: 0.68966\n",
      "Epoch: 188 Batch:   0 Loss: 485.13464 Accuracy: 0.69017\n",
      "Epoch: 189 Batch:   0 Loss: 485.03986 Accuracy: 0.69017\n",
      "Test Loss: 483.43381 Accuracy: 0.64797\n",
      "Epoch: 190 Batch:   0 Loss: 484.94479 Accuracy: 0.69068\n",
      "Epoch: 191 Batch:   0 Loss: 484.84967 Accuracy: 0.69120\n",
      "Epoch: 192 Batch:   0 Loss: 484.75446 Accuracy: 0.69120\n",
      "Epoch: 193 Batch:   0 Loss: 484.65897 Accuracy: 0.69171\n",
      "Epoch: 194 Batch:   0 Loss: 484.56329 Accuracy: 0.69171\n",
      "Epoch: 195 Batch:   0 Loss: 484.46750 Accuracy: 0.69171\n",
      "Epoch: 196 Batch:   0 Loss: 484.37152 Accuracy: 0.69120\n",
      "Epoch: 197 Batch:   0 Loss: 484.27554 Accuracy: 0.69120\n",
      "Epoch: 198 Batch:   0 Loss: 484.17917 Accuracy: 0.69068\n",
      "Epoch: 199 Batch:   0 Loss: 484.08273 Accuracy: 0.69068\n",
      "Test Loss: 482.80407 Accuracy: 0.64757\n",
      "Epoch: 200 Batch:   0 Loss: 483.98615 Accuracy: 0.69120\n",
      "Epoch: 201 Batch:   0 Loss: 483.88934 Accuracy: 0.69068\n",
      "Epoch: 202 Batch:   0 Loss: 483.79245 Accuracy: 0.69120\n",
      "Epoch: 203 Batch:   0 Loss: 483.69537 Accuracy: 0.69120\n",
      "Epoch: 204 Batch:   0 Loss: 483.59796 Accuracy: 0.69120\n",
      "Epoch: 205 Batch:   0 Loss: 483.50055 Accuracy: 0.69223\n",
      "Epoch: 206 Batch:   0 Loss: 483.40286 Accuracy: 0.69171\n",
      "Epoch: 207 Batch:   0 Loss: 483.30521 Accuracy: 0.69171\n",
      "Epoch: 208 Batch:   0 Loss: 483.20700 Accuracy: 0.69274\n",
      "Epoch: 209 Batch:   0 Loss: 483.10889 Accuracy: 0.69326\n",
      "Test Loss: 482.17495 Accuracy: 0.64849\n",
      "Epoch: 210 Batch:   0 Loss: 483.01056 Accuracy: 0.69326\n",
      "Epoch: 211 Batch:   0 Loss: 482.91211 Accuracy: 0.69326\n",
      "Epoch: 212 Batch:   0 Loss: 482.81339 Accuracy: 0.69326\n",
      "Epoch: 213 Batch:   0 Loss: 482.71442 Accuracy: 0.69326\n",
      "Epoch: 214 Batch:   0 Loss: 482.61526 Accuracy: 0.69377\n",
      "Epoch: 215 Batch:   0 Loss: 482.51605 Accuracy: 0.69377\n",
      "Epoch: 216 Batch:   0 Loss: 482.41653 Accuracy: 0.69532\n",
      "Epoch: 217 Batch:   0 Loss: 482.31686 Accuracy: 0.69686\n",
      "Epoch: 218 Batch:   0 Loss: 482.21707 Accuracy: 0.69686\n",
      "Epoch: 219 Batch:   0 Loss: 482.11703 Accuracy: 0.69738\n",
      "Test Loss: 481.54568 Accuracy: 0.64783\n",
      "Epoch: 220 Batch:   0 Loss: 482.01672 Accuracy: 0.69789\n",
      "Epoch: 221 Batch:   0 Loss: 481.91626 Accuracy: 0.69840\n",
      "Epoch: 222 Batch:   0 Loss: 481.81564 Accuracy: 0.69892\n",
      "Epoch: 223 Batch:   0 Loss: 481.71475 Accuracy: 0.69943\n",
      "Epoch: 224 Batch:   0 Loss: 481.61377 Accuracy: 0.69995\n",
      "Epoch: 225 Batch:   0 Loss: 481.51251 Accuracy: 0.69995\n",
      "Epoch: 226 Batch:   0 Loss: 481.41104 Accuracy: 0.69995\n",
      "Epoch: 227 Batch:   0 Loss: 481.30835 Accuracy: 0.69943\n",
      "Epoch: 228 Batch:   0 Loss: 481.20602 Accuracy: 0.69995\n",
      "Epoch: 229 Batch:   0 Loss: 481.10367 Accuracy: 0.69943\n",
      "Test Loss: 480.91590 Accuracy: 0.64809\n",
      "Epoch: 230 Batch:   0 Loss: 481.00092 Accuracy: 0.69943\n",
      "Epoch: 231 Batch:   0 Loss: 480.89789 Accuracy: 0.69995\n",
      "Epoch: 232 Batch:   0 Loss: 480.79468 Accuracy: 0.70098\n",
      "Epoch: 233 Batch:   0 Loss: 480.69128 Accuracy: 0.70098\n",
      "Epoch: 234 Batch:   0 Loss: 480.58768 Accuracy: 0.70046\n",
      "Epoch: 235 Batch:   0 Loss: 480.48389 Accuracy: 0.70046\n",
      "Epoch: 236 Batch:   0 Loss: 480.37961 Accuracy: 0.70046\n",
      "Epoch: 237 Batch:   0 Loss: 480.27530 Accuracy: 0.69995\n",
      "Epoch: 238 Batch:   0 Loss: 480.17078 Accuracy: 0.70098\n",
      "Epoch: 239 Batch:   0 Loss: 480.06604 Accuracy: 0.70149\n",
      "Test Loss: 480.28554 Accuracy: 0.64835\n",
      "Epoch: 240 Batch:   0 Loss: 479.96094 Accuracy: 0.70149\n",
      "Epoch: 241 Batch:   0 Loss: 479.85574 Accuracy: 0.70201\n",
      "Epoch: 242 Batch:   0 Loss: 479.75021 Accuracy: 0.70252\n",
      "Epoch: 243 Batch:   0 Loss: 479.64444 Accuracy: 0.70252\n",
      "Epoch: 244 Batch:   0 Loss: 479.53860 Accuracy: 0.70304\n",
      "Epoch: 245 Batch:   0 Loss: 479.43237 Accuracy: 0.70304\n",
      "Epoch: 246 Batch:   0 Loss: 479.32584 Accuracy: 0.70304\n",
      "Epoch: 247 Batch:   0 Loss: 479.21909 Accuracy: 0.70304\n",
      "Epoch: 248 Batch:   0 Loss: 479.11224 Accuracy: 0.70252\n",
      "Epoch: 249 Batch:   0 Loss: 479.00504 Accuracy: 0.70304\n",
      "Test Loss: 479.65118 Accuracy: 0.64756\n",
      "Epoch: 250 Batch:   0 Loss: 478.89761 Accuracy: 0.70304\n",
      "Epoch: 251 Batch:   0 Loss: 478.78986 Accuracy: 0.70304\n",
      "Epoch: 252 Batch:   0 Loss: 478.68173 Accuracy: 0.70355\n",
      "Epoch: 253 Batch:   0 Loss: 478.57349 Accuracy: 0.70355\n",
      "Epoch: 254 Batch:   0 Loss: 478.46506 Accuracy: 0.70355\n",
      "Epoch: 255 Batch:   0 Loss: 478.35635 Accuracy: 0.70355\n",
      "Epoch: 256 Batch:   0 Loss: 478.24725 Accuracy: 0.70355\n",
      "Epoch: 257 Batch:   0 Loss: 478.13794 Accuracy: 0.70355\n",
      "Epoch: 258 Batch:   0 Loss: 478.02832 Accuracy: 0.70407\n",
      "Epoch: 259 Batch:   0 Loss: 477.91843 Accuracy: 0.70407\n",
      "Test Loss: 479.01111 Accuracy: 0.64704\n",
      "Epoch: 260 Batch:   0 Loss: 477.80835 Accuracy: 0.70407\n",
      "Epoch: 261 Batch:   0 Loss: 477.69312 Accuracy: 0.70252\n",
      "Epoch: 262 Batch:   0 Loss: 477.58490 Accuracy: 0.70304\n",
      "Epoch: 263 Batch:   0 Loss: 477.47495 Accuracy: 0.70304\n",
      "Epoch: 264 Batch:   0 Loss: 477.36618 Accuracy: 0.70355\n",
      "Epoch: 265 Batch:   0 Loss: 477.25708 Accuracy: 0.70355\n",
      "Epoch: 266 Batch:   0 Loss: 477.14771 Accuracy: 0.70355\n",
      "Epoch: 267 Batch:   0 Loss: 477.03812 Accuracy: 0.70355\n",
      "Epoch: 268 Batch:   0 Loss: 476.92700 Accuracy: 0.70355\n",
      "Epoch: 269 Batch:   0 Loss: 476.81693 Accuracy: 0.70407\n",
      "Test Loss: 478.38108 Accuracy: 0.64691\n",
      "Epoch: 270 Batch:   0 Loss: 476.70670 Accuracy: 0.70407\n",
      "Epoch: 271 Batch:   0 Loss: 476.59616 Accuracy: 0.70407\n",
      "Epoch: 272 Batch:   0 Loss: 476.48529 Accuracy: 0.70458\n",
      "Epoch: 273 Batch:   0 Loss: 476.37390 Accuracy: 0.70458\n",
      "Epoch: 274 Batch:   0 Loss: 476.26181 Accuracy: 0.70510\n",
      "Epoch: 275 Batch:   0 Loss: 476.14990 Accuracy: 0.70561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 276 Batch:   0 Loss: 476.03763 Accuracy: 0.70407\n",
      "Epoch: 277 Batch:   0 Loss: 475.92520 Accuracy: 0.70407\n",
      "Epoch: 278 Batch:   0 Loss: 475.81223 Accuracy: 0.70407\n",
      "Epoch: 279 Batch:   0 Loss: 475.69913 Accuracy: 0.70355\n",
      "Test Loss: 477.75079 Accuracy: 0.64573\n",
      "Epoch: 280 Batch:   0 Loss: 475.58548 Accuracy: 0.70355\n",
      "Epoch: 281 Batch:   0 Loss: 475.47165 Accuracy: 0.70355\n",
      "Epoch: 282 Batch:   0 Loss: 475.35739 Accuracy: 0.70304\n",
      "Epoch: 283 Batch:   0 Loss: 475.24271 Accuracy: 0.70355\n",
      "Epoch: 284 Batch:   0 Loss: 475.12784 Accuracy: 0.70355\n",
      "Epoch: 285 Batch:   0 Loss: 475.01260 Accuracy: 0.70355\n",
      "Epoch: 286 Batch:   0 Loss: 474.89691 Accuracy: 0.70355\n",
      "Epoch: 287 Batch:   0 Loss: 474.78076 Accuracy: 0.70407\n",
      "Epoch: 288 Batch:   0 Loss: 474.66446 Accuracy: 0.70407\n",
      "Epoch: 289 Batch:   0 Loss: 474.54776 Accuracy: 0.70407\n",
      "Test Loss: 477.10758 Accuracy: 0.64521\n",
      "Epoch: 290 Batch:   0 Loss: 474.43054 Accuracy: 0.70407\n",
      "Epoch: 291 Batch:   0 Loss: 474.31317 Accuracy: 0.70458\n",
      "Epoch: 292 Batch:   0 Loss: 474.19531 Accuracy: 0.70458\n",
      "Epoch: 293 Batch:   0 Loss: 474.07709 Accuracy: 0.70458\n",
      "Epoch: 294 Batch:   0 Loss: 473.95847 Accuracy: 0.70458\n",
      "Epoch: 295 Batch:   0 Loss: 473.83951 Accuracy: 0.70407\n",
      "Epoch: 296 Batch:   0 Loss: 473.72003 Accuracy: 0.70407\n",
      "Epoch: 297 Batch:   0 Loss: 473.60022 Accuracy: 0.70458\n",
      "Epoch: 298 Batch:   0 Loss: 473.47839 Accuracy: 0.70510\n",
      "Epoch: 299 Batch:   0 Loss: 473.35870 Accuracy: 0.70561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-05 15:55:14,817] Trial 1 finished with value: 0.4822216572117298 and parameters: {'REG_W': 2.150499128114348e-06, 'REG_B': 0.0013123991795708923, 'REG_Z': 2.7678514122803996e-05, 'SPAR_W': 0.9022371274865826, 'SPAR_B': 0.5539345534584593, 'SPAR_Z': 0.9993522107302426, 'loss': 'l2', 'LEARNING_RATE': 0.00031975053586635394, 'NUM_EPOCHS': 300, 'alpha': 0.31386241034032014}. Best is trial 1 with value: 0.4822216572117298.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 476.44933 Accuracy: 0.64455\n",
      "Epoch:   0 Batch:   0 Loss: 2.93354 Accuracy: 0.49768\n",
      "Epoch:   1 Batch:   0 Loss: 2.47058 Accuracy: 0.49768\n",
      "Epoch:   2 Batch:   0 Loss: 2.38001 Accuracy: 0.49768\n",
      "Epoch:   3 Batch:   0 Loss: 2.29064 Accuracy: 0.49768\n",
      "Epoch:   4 Batch:   0 Loss: 2.20256 Accuracy: 0.49768\n",
      "Epoch:   5 Batch:   0 Loss: 2.11593 Accuracy: 0.49768\n",
      "Epoch:   6 Batch:   0 Loss: 2.03092 Accuracy: 0.49768\n",
      "Epoch:   7 Batch:   0 Loss: 1.94775 Accuracy: 0.49768\n",
      "Epoch:   8 Batch:   0 Loss: 1.86662 Accuracy: 0.49768\n",
      "Epoch:   9 Batch:   0 Loss: 1.78777 Accuracy: 0.49768\n",
      "Test Loss: 1.72032 Accuracy: 0.49987\n",
      "Epoch:  10 Batch:   0 Loss: 1.71142 Accuracy: 0.49768\n",
      "Epoch:  11 Batch:   0 Loss: 1.63784 Accuracy: 0.49768\n",
      "Epoch:  12 Batch:   0 Loss: 1.56728 Accuracy: 0.49768\n",
      "Epoch:  13 Batch:   0 Loss: 1.50000 Accuracy: 0.49768\n",
      "Epoch:  14 Batch:   0 Loss: 1.43628 Accuracy: 0.49768\n",
      "Epoch:  15 Batch:   0 Loss: 1.37635 Accuracy: 0.49768\n",
      "Epoch:  16 Batch:   0 Loss: 1.32043 Accuracy: 0.49768\n",
      "Epoch:  17 Batch:   0 Loss: 1.26872 Accuracy: 0.49768\n",
      "Epoch:  18 Batch:   0 Loss: 1.22134 Accuracy: 0.49768\n",
      "Epoch:  19 Batch:   0 Loss: 1.17837 Accuracy: 0.49768\n",
      "Test Loss: 1.15254 Accuracy: 0.49987\n",
      "Epoch:  20 Batch:   0 Loss: 1.13980 Accuracy: 0.49768\n",
      "Epoch:  21 Batch:   0 Loss: 1.10556 Accuracy: 0.49768\n",
      "Epoch:  22 Batch:   0 Loss: 1.07548 Accuracy: 0.49768\n",
      "Epoch:  23 Batch:   0 Loss: 1.04935 Accuracy: 0.49768\n",
      "Epoch:  24 Batch:   0 Loss: 1.02687 Accuracy: 0.49768\n",
      "Epoch:  25 Batch:   0 Loss: 1.00773 Accuracy: 0.49768\n",
      "Epoch:  26 Batch:   0 Loss: 0.99155 Accuracy: 0.49768\n",
      "Epoch:  27 Batch:   0 Loss: 0.97799 Accuracy: 0.49768\n",
      "Epoch:  28 Batch:   0 Loss: 0.96666 Accuracy: 0.49768\n",
      "Epoch:  29 Batch:   0 Loss: 0.95722 Accuracy: 0.49768\n",
      "Test Loss: 0.95550 Accuracy: 0.49987\n",
      "Epoch:  30 Batch:   0 Loss: 0.94937 Accuracy: 0.49768\n",
      "Epoch:  31 Batch:   0 Loss: 0.94281 Accuracy: 0.49768\n",
      "Epoch:  32 Batch:   0 Loss: 0.93730 Accuracy: 0.49768\n",
      "Epoch:  33 Batch:   0 Loss: 0.93262 Accuracy: 0.49768\n",
      "Epoch:  34 Batch:   0 Loss: 0.92860 Accuracy: 0.49768\n",
      "Epoch:  35 Batch:   0 Loss: 0.92510 Accuracy: 0.49768\n",
      "Epoch:  36 Batch:   0 Loss: 0.92199 Accuracy: 0.49768\n",
      "Epoch:  37 Batch:   0 Loss: 0.91920 Accuracy: 0.49768\n",
      "Epoch:  38 Batch:   0 Loss: 0.91663 Accuracy: 0.49768\n",
      "Epoch:  39 Batch:   0 Loss: 0.91425 Accuracy: 0.49768\n",
      "Test Loss: 0.91382 Accuracy: 0.49987\n",
      "Epoch:  40 Batch:   0 Loss: 0.91200 Accuracy: 0.49768\n",
      "Epoch:  41 Batch:   0 Loss: 0.90985 Accuracy: 0.49768\n",
      "Epoch:  42 Batch:   0 Loss: 0.90778 Accuracy: 0.49665\n",
      "Epoch:  43 Batch:   0 Loss: 0.90577 Accuracy: 0.49665\n",
      "Epoch:  44 Batch:   0 Loss: 0.90381 Accuracy: 0.49614\n",
      "Epoch:  45 Batch:   0 Loss: 0.90188 Accuracy: 0.49614\n",
      "Epoch:  46 Batch:   0 Loss: 0.89999 Accuracy: 0.49614\n",
      "Epoch:  47 Batch:   0 Loss: 0.89812 Accuracy: 0.49563\n",
      "Epoch:  48 Batch:   0 Loss: 0.89628 Accuracy: 0.49511\n",
      "Epoch:  49 Batch:   0 Loss: 0.89445 Accuracy: 0.49305\n",
      "Test Loss: 0.89352 Accuracy: 0.49125\n",
      "Epoch:  50 Batch:   0 Loss: 0.89264 Accuracy: 0.49202\n",
      "Epoch:  51 Batch:   0 Loss: 0.89085 Accuracy: 0.49357\n",
      "Epoch:  52 Batch:   0 Loss: 0.88907 Accuracy: 0.49820\n",
      "Epoch:  53 Batch:   0 Loss: 0.88731 Accuracy: 0.49974\n",
      "Epoch:  54 Batch:   0 Loss: 0.88556 Accuracy: 0.49923\n",
      "Epoch:  55 Batch:   0 Loss: 0.88383 Accuracy: 0.50437\n",
      "Epoch:  56 Batch:   0 Loss: 0.88211 Accuracy: 0.50489\n",
      "Epoch:  57 Batch:   0 Loss: 0.88040 Accuracy: 0.50592\n",
      "Epoch:  58 Batch:   0 Loss: 0.87871 Accuracy: 0.50592\n",
      "Epoch:  59 Batch:   0 Loss: 0.87702 Accuracy: 0.50746\n",
      "Test Loss: 0.87612 Accuracy: 0.46931\n",
      "Epoch:  60 Batch:   0 Loss: 0.87535 Accuracy: 0.50746\n",
      "Epoch:  61 Batch:   0 Loss: 0.87370 Accuracy: 0.50798\n",
      "Epoch:  62 Batch:   0 Loss: 0.87205 Accuracy: 0.50901\n",
      "Epoch:  63 Batch:   0 Loss: 0.87042 Accuracy: 0.50952\n",
      "Epoch:  64 Batch:   0 Loss: 0.86880 Accuracy: 0.50952\n",
      "Epoch:  65 Batch:   0 Loss: 0.86720 Accuracy: 0.51004\n",
      "Epoch:  66 Batch:   0 Loss: 0.86560 Accuracy: 0.51055\n",
      "Epoch:  67 Batch:   0 Loss: 0.86402 Accuracy: 0.51055\n",
      "Epoch:  68 Batch:   0 Loss: 0.86245 Accuracy: 0.51055\n",
      "Epoch:  69 Batch:   0 Loss: 0.86089 Accuracy: 0.51055\n",
      "Test Loss: 0.86012 Accuracy: 0.46579\n",
      "Epoch:  70 Batch:   0 Loss: 0.85934 Accuracy: 0.51107\n",
      "Epoch:  71 Batch:   0 Loss: 0.85781 Accuracy: 0.51209\n",
      "Epoch:  72 Batch:   0 Loss: 0.85628 Accuracy: 0.51209\n",
      "Epoch:  73 Batch:   0 Loss: 0.85477 Accuracy: 0.51312\n",
      "Epoch:  74 Batch:   0 Loss: 0.85327 Accuracy: 0.51415\n",
      "Epoch:  75 Batch:   0 Loss: 0.85178 Accuracy: 0.51518\n",
      "Epoch:  76 Batch:   0 Loss: 0.85031 Accuracy: 0.51518\n",
      "Epoch:  77 Batch:   0 Loss: 0.84884 Accuracy: 0.51570\n",
      "Epoch:  78 Batch:   0 Loss: 0.84739 Accuracy: 0.51570\n",
      "Epoch:  79 Batch:   0 Loss: 0.84594 Accuracy: 0.51621\n",
      "Test Loss: 0.84530 Accuracy: 0.46553\n",
      "Epoch:  80 Batch:   0 Loss: 0.84451 Accuracy: 0.51621\n",
      "Epoch:  81 Batch:   0 Loss: 0.84309 Accuracy: 0.51724\n",
      "Epoch:  82 Batch:   0 Loss: 0.84168 Accuracy: 0.51776\n",
      "Epoch:  83 Batch:   0 Loss: 0.84028 Accuracy: 0.51879\n",
      "Epoch:  84 Batch:   0 Loss: 0.83890 Accuracy: 0.51827\n",
      "Epoch:  85 Batch:   0 Loss: 0.83752 Accuracy: 0.51879\n",
      "Epoch:  86 Batch:   0 Loss: 0.83615 Accuracy: 0.51879\n",
      "Epoch:  87 Batch:   0 Loss: 0.83480 Accuracy: 0.51930\n",
      "Epoch:  88 Batch:   0 Loss: 0.83345 Accuracy: 0.52084\n",
      "Epoch:  89 Batch:   0 Loss: 0.83212 Accuracy: 0.52136\n",
      "Test Loss: 0.83160 Accuracy: 0.46305\n",
      "Epoch:  90 Batch:   0 Loss: 0.83080 Accuracy: 0.52187\n",
      "Epoch:  91 Batch:   0 Loss: 0.82949 Accuracy: 0.52187\n",
      "Epoch:  92 Batch:   0 Loss: 0.82818 Accuracy: 0.52239\n",
      "Epoch:  93 Batch:   0 Loss: 0.82689 Accuracy: 0.52239\n",
      "Epoch:  94 Batch:   0 Loss: 0.82561 Accuracy: 0.52239\n",
      "Epoch:  95 Batch:   0 Loss: 0.82434 Accuracy: 0.52290\n",
      "Epoch:  96 Batch:   0 Loss: 0.82308 Accuracy: 0.52239\n",
      "Epoch:  97 Batch:   0 Loss: 0.82183 Accuracy: 0.52239\n",
      "Epoch:  98 Batch:   0 Loss: 0.82059 Accuracy: 0.52290\n",
      "Epoch:  99 Batch:   0 Loss: 0.81936 Accuracy: 0.52393\n",
      "Test Loss: 0.81895 Accuracy: 0.46175\n",
      "Epoch: 100 Batch:   0 Loss: 0.81814 Accuracy: 0.52393\n",
      "Epoch: 101 Batch:   0 Loss: 0.81693 Accuracy: 0.52342\n",
      "Epoch: 102 Batch:   0 Loss: 0.81573 Accuracy: 0.52290\n",
      "Epoch: 103 Batch:   0 Loss: 0.81454 Accuracy: 0.52342\n",
      "Epoch: 104 Batch:   0 Loss: 0.81336 Accuracy: 0.52187\n",
      "Epoch: 105 Batch:   0 Loss: 0.81219 Accuracy: 0.52187\n",
      "Epoch: 106 Batch:   0 Loss: 0.81102 Accuracy: 0.52239\n",
      "Epoch: 107 Batch:   0 Loss: 0.80987 Accuracy: 0.52239\n",
      "Epoch: 108 Batch:   0 Loss: 0.80873 Accuracy: 0.52290\n",
      "Epoch: 109 Batch:   0 Loss: 0.80760 Accuracy: 0.52290\n",
      "Test Loss: 0.80730 Accuracy: 0.46214\n",
      "Epoch: 110 Batch:   0 Loss: 0.80647 Accuracy: 0.52393\n",
      "Epoch: 111 Batch:   0 Loss: 0.80536 Accuracy: 0.52496\n",
      "Epoch: 112 Batch:   0 Loss: 0.80426 Accuracy: 0.52548\n",
      "Epoch: 113 Batch:   0 Loss: 0.80316 Accuracy: 0.52702\n",
      "Epoch: 114 Batch:   0 Loss: 0.80207 Accuracy: 0.52856\n",
      "Epoch: 115 Batch:   0 Loss: 0.80100 Accuracy: 0.52856\n",
      "Epoch: 116 Batch:   0 Loss: 0.79993 Accuracy: 0.52959\n",
      "Epoch: 117 Batch:   0 Loss: 0.79887 Accuracy: 0.52959\n",
      "Epoch: 118 Batch:   0 Loss: 0.79782 Accuracy: 0.52959\n",
      "Epoch: 119 Batch:   0 Loss: 0.79678 Accuracy: 0.52805\n",
      "Test Loss: 0.79658 Accuracy: 0.45588\n",
      "Epoch: 120 Batch:   0 Loss: 0.79574 Accuracy: 0.52856\n",
      "Epoch: 121 Batch:   0 Loss: 0.79472 Accuracy: 0.53062\n",
      "Epoch: 122 Batch:   0 Loss: 0.79371 Accuracy: 0.53165\n",
      "Epoch: 123 Batch:   0 Loss: 0.79270 Accuracy: 0.53165\n",
      "Epoch: 124 Batch:   0 Loss: 0.79170 Accuracy: 0.53165\n",
      "Epoch: 125 Batch:   0 Loss: 0.79071 Accuracy: 0.53165\n",
      "Epoch: 126 Batch:   0 Loss: 0.78973 Accuracy: 0.53217\n",
      "Epoch: 127 Batch:   0 Loss: 0.78876 Accuracy: 0.53217\n",
      "Epoch: 128 Batch:   0 Loss: 0.78780 Accuracy: 0.53423\n",
      "Epoch: 129 Batch:   0 Loss: 0.78684 Accuracy: 0.53525\n",
      "Test Loss: 0.78673 Accuracy: 0.45484\n",
      "Epoch: 130 Batch:   0 Loss: 0.78589 Accuracy: 0.53628\n",
      "Epoch: 131 Batch:   0 Loss: 0.78495 Accuracy: 0.53577\n",
      "Epoch: 132 Batch:   0 Loss: 0.78402 Accuracy: 0.53937\n",
      "Epoch: 133 Batch:   0 Loss: 0.78310 Accuracy: 0.53834\n",
      "Epoch: 134 Batch:   0 Loss: 0.78218 Accuracy: 0.53886\n",
      "Epoch: 135 Batch:   0 Loss: 0.78128 Accuracy: 0.54297\n",
      "Epoch: 136 Batch:   0 Loss: 0.78038 Accuracy: 0.54297\n",
      "Epoch: 137 Batch:   0 Loss: 0.77949 Accuracy: 0.54143\n",
      "Epoch: 138 Batch:   0 Loss: 0.77860 Accuracy: 0.54297\n",
      "Epoch: 139 Batch:   0 Loss: 0.77773 Accuracy: 0.54452\n",
      "Test Loss: 0.77771 Accuracy: 0.45721\n",
      "Epoch: 140 Batch:   0 Loss: 0.77686 Accuracy: 0.54349\n",
      "Epoch: 141 Batch:   0 Loss: 0.77600 Accuracy: 0.54709\n",
      "Epoch: 142 Batch:   0 Loss: 0.77515 Accuracy: 0.55069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 143 Batch:   0 Loss: 0.77431 Accuracy: 0.55275\n",
      "Epoch: 144 Batch:   0 Loss: 0.77347 Accuracy: 0.55584\n",
      "Epoch: 145 Batch:   0 Loss: 0.77264 Accuracy: 0.55790\n",
      "Epoch: 146 Batch:   0 Loss: 0.77182 Accuracy: 0.55739\n",
      "Epoch: 147 Batch:   0 Loss: 0.77100 Accuracy: 0.55944\n",
      "Epoch: 148 Batch:   0 Loss: 0.77019 Accuracy: 0.56047\n",
      "Epoch: 149 Batch:   0 Loss: 0.76939 Accuracy: 0.56150\n",
      "Test Loss: 0.76945 Accuracy: 0.47146\n",
      "Epoch: 150 Batch:   0 Loss: 0.76860 Accuracy: 0.55996\n",
      "Epoch: 151 Batch:   0 Loss: 0.76782 Accuracy: 0.56047\n",
      "Epoch: 152 Batch:   0 Loss: 0.76704 Accuracy: 0.56047\n",
      "Epoch: 153 Batch:   0 Loss: 0.76626 Accuracy: 0.55790\n",
      "Epoch: 154 Batch:   0 Loss: 0.76550 Accuracy: 0.56047\n",
      "Epoch: 155 Batch:   0 Loss: 0.76474 Accuracy: 0.56253\n",
      "Epoch: 156 Batch:   0 Loss: 0.76399 Accuracy: 0.56202\n",
      "Epoch: 157 Batch:   0 Loss: 0.76325 Accuracy: 0.56253\n",
      "Epoch: 158 Batch:   0 Loss: 0.76251 Accuracy: 0.56356\n",
      "Epoch: 159 Batch:   0 Loss: 0.76178 Accuracy: 0.56819\n",
      "Test Loss: 0.76191 Accuracy: 0.48807\n",
      "Epoch: 160 Batch:   0 Loss: 0.76106 Accuracy: 0.56871\n",
      "Epoch: 161 Batch:   0 Loss: 0.76034 Accuracy: 0.57025\n",
      "Epoch: 162 Batch:   0 Loss: 0.75963 Accuracy: 0.56871\n",
      "Epoch: 163 Batch:   0 Loss: 0.75892 Accuracy: 0.57180\n",
      "Epoch: 164 Batch:   0 Loss: 0.75823 Accuracy: 0.57180\n",
      "Epoch: 165 Batch:   0 Loss: 0.75754 Accuracy: 0.57231\n",
      "Epoch: 166 Batch:   0 Loss: 0.75685 Accuracy: 0.57437\n",
      "Epoch: 167 Batch:   0 Loss: 0.75617 Accuracy: 0.57591\n",
      "Epoch: 168 Batch:   0 Loss: 0.75550 Accuracy: 0.57694\n",
      "Epoch: 169 Batch:   0 Loss: 0.75484 Accuracy: 0.57746\n",
      "Test Loss: 0.75503 Accuracy: 0.51788\n",
      "Epoch: 170 Batch:   0 Loss: 0.75418 Accuracy: 0.57900\n",
      "Epoch: 171 Batch:   0 Loss: 0.75353 Accuracy: 0.57694\n",
      "Epoch: 172 Batch:   0 Loss: 0.75288 Accuracy: 0.58003\n",
      "Epoch: 173 Batch:   0 Loss: 0.75224 Accuracy: 0.58569\n",
      "Epoch: 174 Batch:   0 Loss: 0.75160 Accuracy: 0.58724\n",
      "Epoch: 175 Batch:   0 Loss: 0.75098 Accuracy: 0.59032\n",
      "Epoch: 176 Batch:   0 Loss: 0.75035 Accuracy: 0.59238\n",
      "Epoch: 177 Batch:   0 Loss: 0.74974 Accuracy: 0.59444\n",
      "Epoch: 178 Batch:   0 Loss: 0.74913 Accuracy: 0.59804\n",
      "Epoch: 179 Batch:   0 Loss: 0.74852 Accuracy: 0.60113\n",
      "Test Loss: 0.74878 Accuracy: 0.54873\n",
      "Epoch: 180 Batch:   0 Loss: 0.74792 Accuracy: 0.60576\n",
      "Epoch: 181 Batch:   0 Loss: 0.74733 Accuracy: 0.60628\n",
      "Epoch: 182 Batch:   0 Loss: 0.74674 Accuracy: 0.61194\n",
      "Epoch: 183 Batch:   0 Loss: 0.74616 Accuracy: 0.61657\n",
      "Epoch: 184 Batch:   0 Loss: 0.74558 Accuracy: 0.62120\n",
      "Epoch: 185 Batch:   0 Loss: 0.74501 Accuracy: 0.62378\n",
      "Epoch: 186 Batch:   0 Loss: 0.74444 Accuracy: 0.62995\n",
      "Epoch: 187 Batch:   0 Loss: 0.74388 Accuracy: 0.63510\n",
      "Epoch: 188 Batch:   0 Loss: 0.74333 Accuracy: 0.64591\n",
      "Epoch: 189 Batch:   0 Loss: 0.74278 Accuracy: 0.65260\n",
      "Test Loss: 0.74309 Accuracy: 0.57762\n",
      "Epoch: 190 Batch:   0 Loss: 0.74224 Accuracy: 0.65517\n",
      "Epoch: 191 Batch:   0 Loss: 0.74170 Accuracy: 0.65826\n",
      "Epoch: 192 Batch:   0 Loss: 0.74117 Accuracy: 0.66289\n",
      "Epoch: 193 Batch:   0 Loss: 0.74064 Accuracy: 0.66701\n",
      "Epoch: 194 Batch:   0 Loss: 0.74011 Accuracy: 0.67061\n",
      "Epoch: 195 Batch:   0 Loss: 0.73960 Accuracy: 0.67319\n",
      "Epoch: 196 Batch:   0 Loss: 0.73908 Accuracy: 0.67576\n",
      "Epoch: 197 Batch:   0 Loss: 0.73858 Accuracy: 0.67782\n",
      "Epoch: 198 Batch:   0 Loss: 0.73807 Accuracy: 0.68194\n",
      "Epoch: 199 Batch:   0 Loss: 0.73758 Accuracy: 0.68194\n",
      "Test Loss: 0.73794 Accuracy: 0.60208\n",
      "Epoch: 200 Batch:   0 Loss: 0.73708 Accuracy: 0.68348\n",
      "Epoch: 201 Batch:   0 Loss: 0.73659 Accuracy: 0.68502\n",
      "Epoch: 202 Batch:   0 Loss: 0.73611 Accuracy: 0.68863\n",
      "Epoch: 203 Batch:   0 Loss: 0.73563 Accuracy: 0.69120\n",
      "Epoch: 204 Batch:   0 Loss: 0.73516 Accuracy: 0.69429\n",
      "Epoch: 205 Batch:   0 Loss: 0.73469 Accuracy: 0.69686\n",
      "Epoch: 206 Batch:   0 Loss: 0.73423 Accuracy: 0.69892\n",
      "Epoch: 207 Batch:   0 Loss: 0.73377 Accuracy: 0.69892\n",
      "Epoch: 208 Batch:   0 Loss: 0.73331 Accuracy: 0.69738\n",
      "Epoch: 209 Batch:   0 Loss: 0.73286 Accuracy: 0.69995\n",
      "Test Loss: 0.73327 Accuracy: 0.60980\n",
      "Epoch: 210 Batch:   0 Loss: 0.73242 Accuracy: 0.69995\n",
      "Epoch: 211 Batch:   0 Loss: 0.73197 Accuracy: 0.70149\n",
      "Epoch: 212 Batch:   0 Loss: 0.73154 Accuracy: 0.70252\n",
      "Epoch: 213 Batch:   0 Loss: 0.73111 Accuracy: 0.70304\n",
      "Epoch: 214 Batch:   0 Loss: 0.73068 Accuracy: 0.70098\n",
      "Epoch: 215 Batch:   0 Loss: 0.73025 Accuracy: 0.69892\n",
      "Epoch: 216 Batch:   0 Loss: 0.72983 Accuracy: 0.69738\n",
      "Epoch: 217 Batch:   0 Loss: 0.72942 Accuracy: 0.69583\n",
      "Epoch: 218 Batch:   0 Loss: 0.72901 Accuracy: 0.69429\n",
      "Epoch: 219 Batch:   0 Loss: 0.72860 Accuracy: 0.68914\n",
      "Test Loss: 0.72905 Accuracy: 0.62446\n",
      "Epoch: 220 Batch:   0 Loss: 0.72819 Accuracy: 0.68811\n",
      "Epoch: 221 Batch:   0 Loss: 0.72779 Accuracy: 0.68657\n",
      "Epoch: 222 Batch:   0 Loss: 0.72739 Accuracy: 0.68348\n",
      "Epoch: 223 Batch:   0 Loss: 0.72700 Accuracy: 0.68348\n",
      "Epoch: 224 Batch:   0 Loss: 0.72661 Accuracy: 0.67936\n",
      "Epoch: 225 Batch:   0 Loss: 0.72622 Accuracy: 0.67679\n",
      "Epoch: 226 Batch:   0 Loss: 0.72584 Accuracy: 0.67473\n",
      "Epoch: 227 Batch:   0 Loss: 0.72547 Accuracy: 0.67216\n",
      "Epoch: 228 Batch:   0 Loss: 0.72509 Accuracy: 0.66958\n",
      "Epoch: 229 Batch:   0 Loss: 0.72472 Accuracy: 0.66547\n",
      "Test Loss: 0.72523 Accuracy: 0.62199\n",
      "Epoch: 230 Batch:   0 Loss: 0.72436 Accuracy: 0.66289\n",
      "Epoch: 231 Batch:   0 Loss: 0.72400 Accuracy: 0.65775\n",
      "Epoch: 232 Batch:   0 Loss: 0.72364 Accuracy: 0.65311\n",
      "Epoch: 233 Batch:   0 Loss: 0.72329 Accuracy: 0.64848\n",
      "Epoch: 234 Batch:   0 Loss: 0.72294 Accuracy: 0.64436\n",
      "Epoch: 235 Batch:   0 Loss: 0.72259 Accuracy: 0.64076\n",
      "Epoch: 236 Batch:   0 Loss: 0.72225 Accuracy: 0.63459\n",
      "Epoch: 237 Batch:   0 Loss: 0.72191 Accuracy: 0.62892\n",
      "Epoch: 238 Batch:   0 Loss: 0.72158 Accuracy: 0.62378\n",
      "Epoch: 239 Batch:   0 Loss: 0.72124 Accuracy: 0.62069\n",
      "Test Loss: 0.72178 Accuracy: 0.61115\n",
      "Epoch: 240 Batch:   0 Loss: 0.72092 Accuracy: 0.61348\n",
      "Epoch: 241 Batch:   0 Loss: 0.72059 Accuracy: 0.60731\n",
      "Epoch: 242 Batch:   0 Loss: 0.72027 Accuracy: 0.60113\n",
      "Epoch: 243 Batch:   0 Loss: 0.71995 Accuracy: 0.59701\n",
      "Epoch: 244 Batch:   0 Loss: 0.71964 Accuracy: 0.59032\n",
      "Epoch: 245 Batch:   0 Loss: 0.71932 Accuracy: 0.58569\n",
      "Epoch: 246 Batch:   0 Loss: 0.71901 Accuracy: 0.58106\n",
      "Epoch: 247 Batch:   0 Loss: 0.71871 Accuracy: 0.57746\n",
      "Epoch: 248 Batch:   0 Loss: 0.71841 Accuracy: 0.57283\n",
      "Epoch: 249 Batch:   0 Loss: 0.71811 Accuracy: 0.56768\n",
      "Test Loss: 0.71867 Accuracy: 0.59300\n",
      "Epoch: 250 Batch:   0 Loss: 0.71781 Accuracy: 0.56047\n",
      "Epoch: 251 Batch:   0 Loss: 0.71751 Accuracy: 0.55481\n",
      "Epoch: 252 Batch:   0 Loss: 0.71722 Accuracy: 0.55121\n",
      "Epoch: 253 Batch:   0 Loss: 0.71694 Accuracy: 0.54606\n",
      "Epoch: 254 Batch:   0 Loss: 0.71665 Accuracy: 0.53937\n",
      "Epoch: 255 Batch:   0 Loss: 0.71637 Accuracy: 0.53268\n",
      "Epoch: 256 Batch:   0 Loss: 0.71609 Accuracy: 0.53217\n",
      "Epoch: 257 Batch:   0 Loss: 0.71581 Accuracy: 0.52908\n",
      "Epoch: 258 Batch:   0 Loss: 0.71554 Accuracy: 0.52908\n",
      "Epoch: 259 Batch:   0 Loss: 0.71527 Accuracy: 0.52496\n",
      "Test Loss: 0.71586 Accuracy: 0.57145\n",
      "Epoch: 260 Batch:   0 Loss: 0.71500 Accuracy: 0.52290\n",
      "Epoch: 261 Batch:   0 Loss: 0.71474 Accuracy: 0.52084\n",
      "Epoch: 262 Batch:   0 Loss: 0.71446 Accuracy: 0.51981\n",
      "Epoch: 263 Batch:   0 Loss: 0.71420 Accuracy: 0.51827\n",
      "Epoch: 264 Batch:   0 Loss: 0.71393 Accuracy: 0.51724\n",
      "Epoch: 265 Batch:   0 Loss: 0.71367 Accuracy: 0.51621\n",
      "Epoch: 266 Batch:   0 Loss: 0.71341 Accuracy: 0.51518\n",
      "Epoch: 267 Batch:   0 Loss: 0.71316 Accuracy: 0.51518\n",
      "Epoch: 268 Batch:   0 Loss: 0.71291 Accuracy: 0.51518\n",
      "Epoch: 269 Batch:   0 Loss: 0.71266 Accuracy: 0.51415\n",
      "Test Loss: 0.71329 Accuracy: 0.54833\n",
      "Epoch: 270 Batch:   0 Loss: 0.71241 Accuracy: 0.51415\n",
      "Epoch: 271 Batch:   0 Loss: 0.71216 Accuracy: 0.51415\n",
      "Epoch: 272 Batch:   0 Loss: 0.71192 Accuracy: 0.51415\n",
      "Epoch: 273 Batch:   0 Loss: 0.71168 Accuracy: 0.51312\n",
      "Epoch: 274 Batch:   0 Loss: 0.71144 Accuracy: 0.51261\n",
      "Epoch: 275 Batch:   0 Loss: 0.71121 Accuracy: 0.51261\n",
      "Epoch: 276 Batch:   0 Loss: 0.71098 Accuracy: 0.51261\n",
      "Epoch: 277 Batch:   0 Loss: 0.71074 Accuracy: 0.51209\n",
      "Epoch: 278 Batch:   0 Loss: 0.71051 Accuracy: 0.51209\n",
      "Epoch: 279 Batch:   0 Loss: 0.71029 Accuracy: 0.51209\n",
      "Test Loss: 0.71097 Accuracy: 0.52443\n",
      "Epoch: 280 Batch:   0 Loss: 0.71006 Accuracy: 0.51158\n",
      "Epoch: 281 Batch:   0 Loss: 0.70984 Accuracy: 0.51158\n",
      "Epoch: 282 Batch:   0 Loss: 0.70962 Accuracy: 0.51004\n",
      "Epoch: 283 Batch:   0 Loss: 0.70941 Accuracy: 0.50952\n",
      "Epoch: 284 Batch:   0 Loss: 0.70919 Accuracy: 0.50849\n",
      "Epoch: 285 Batch:   0 Loss: 0.70898 Accuracy: 0.50798\n",
      "Epoch: 286 Batch:   0 Loss: 0.70877 Accuracy: 0.50592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 287 Batch:   0 Loss: 0.70854 Accuracy: 0.50592\n",
      "Epoch: 288 Batch:   0 Loss: 0.70832 Accuracy: 0.50489\n",
      "Epoch: 289 Batch:   0 Loss: 0.70811 Accuracy: 0.50489\n",
      "Test Loss: 0.70883 Accuracy: 0.50444\n",
      "Epoch: 290 Batch:   0 Loss: 0.70790 Accuracy: 0.50489\n",
      "Epoch: 291 Batch:   0 Loss: 0.70769 Accuracy: 0.50489\n",
      "Epoch: 292 Batch:   0 Loss: 0.70748 Accuracy: 0.50489\n",
      "Epoch: 293 Batch:   0 Loss: 0.70728 Accuracy: 0.50437\n",
      "Epoch: 294 Batch:   0 Loss: 0.70707 Accuracy: 0.50386\n",
      "Epoch: 295 Batch:   0 Loss: 0.70687 Accuracy: 0.50386\n",
      "Epoch: 296 Batch:   0 Loss: 0.70667 Accuracy: 0.50386\n",
      "Epoch: 297 Batch:   0 Loss: 0.70647 Accuracy: 0.50386\n",
      "Epoch: 298 Batch:   0 Loss: 0.70627 Accuracy: 0.50386\n",
      "Epoch: 299 Batch:   0 Loss: 0.70608 Accuracy: 0.50335\n",
      "Test Loss: 0.70687 Accuracy: 0.50065\n",
      "Epoch: 300 Batch:   0 Loss: 0.70588 Accuracy: 0.50335\n",
      "Epoch: 301 Batch:   0 Loss: 0.70569 Accuracy: 0.50335\n",
      "Epoch: 302 Batch:   0 Loss: 0.70550 Accuracy: 0.50335\n",
      "Epoch: 303 Batch:   0 Loss: 0.70532 Accuracy: 0.50335\n",
      "Epoch: 304 Batch:   0 Loss: 0.70513 Accuracy: 0.50335\n",
      "Epoch: 305 Batch:   0 Loss: 0.70495 Accuracy: 0.50335\n",
      "Epoch: 306 Batch:   0 Loss: 0.70476 Accuracy: 0.50335\n",
      "Epoch: 307 Batch:   0 Loss: 0.70458 Accuracy: 0.50335\n",
      "Epoch: 308 Batch:   0 Loss: 0.70440 Accuracy: 0.50335\n",
      "Epoch: 309 Batch:   0 Loss: 0.70422 Accuracy: 0.50335\n",
      "Test Loss: 0.70508 Accuracy: 0.50026\n",
      "Epoch: 310 Batch:   0 Loss: 0.70404 Accuracy: 0.50335\n",
      "Epoch: 311 Batch:   0 Loss: 0.70386 Accuracy: 0.50335\n",
      "Epoch: 312 Batch:   0 Loss: 0.70369 Accuracy: 0.50335\n",
      "Epoch: 313 Batch:   0 Loss: 0.70351 Accuracy: 0.50335\n",
      "Epoch: 314 Batch:   0 Loss: 0.70334 Accuracy: 0.50335\n",
      "Epoch: 315 Batch:   0 Loss: 0.70316 Accuracy: 0.50335\n",
      "Epoch: 316 Batch:   0 Loss: 0.70299 Accuracy: 0.50335\n",
      "Epoch: 317 Batch:   0 Loss: 0.70282 Accuracy: 0.50335\n",
      "Epoch: 318 Batch:   0 Loss: 0.70265 Accuracy: 0.50335\n",
      "Epoch: 319 Batch:   0 Loss: 0.70248 Accuracy: 0.50335\n",
      "Test Loss: 0.70339 Accuracy: 0.50026\n",
      "Epoch: 320 Batch:   0 Loss: 0.70231 Accuracy: 0.50335\n",
      "Epoch: 321 Batch:   0 Loss: 0.70214 Accuracy: 0.50335\n",
      "Epoch: 322 Batch:   0 Loss: 0.70197 Accuracy: 0.50335\n",
      "Epoch: 323 Batch:   0 Loss: 0.70180 Accuracy: 0.50335\n",
      "Epoch: 324 Batch:   0 Loss: 0.70163 Accuracy: 0.50335\n",
      "Epoch: 325 Batch:   0 Loss: 0.70146 Accuracy: 0.50335\n",
      "Epoch: 326 Batch:   0 Loss: 0.70130 Accuracy: 0.50335\n",
      "Epoch: 327 Batch:   0 Loss: 0.70113 Accuracy: 0.50335\n",
      "Epoch: 328 Batch:   0 Loss: 0.70096 Accuracy: 0.50335\n",
      "Epoch: 329 Batch:   0 Loss: 0.70079 Accuracy: 0.50335\n",
      "Test Loss: 0.70176 Accuracy: 0.50013\n",
      "Epoch: 330 Batch:   0 Loss: 0.70062 Accuracy: 0.50335\n",
      "Epoch: 331 Batch:   0 Loss: 0.70046 Accuracy: 0.50335\n",
      "Epoch: 332 Batch:   0 Loss: 0.70029 Accuracy: 0.50335\n",
      "Epoch: 333 Batch:   0 Loss: 0.70012 Accuracy: 0.50335\n",
      "Epoch: 334 Batch:   0 Loss: 0.69995 Accuracy: 0.50335\n",
      "Epoch: 335 Batch:   0 Loss: 0.69978 Accuracy: 0.50335\n",
      "Epoch: 336 Batch:   0 Loss: 0.69961 Accuracy: 0.50335\n",
      "Epoch: 337 Batch:   0 Loss: 0.69944 Accuracy: 0.50335\n",
      "Epoch: 338 Batch:   0 Loss: 0.69926 Accuracy: 0.50335\n",
      "Epoch: 339 Batch:   0 Loss: 0.69909 Accuracy: 0.50335\n",
      "Test Loss: 0.70014 Accuracy: 0.50013\n",
      "Epoch: 340 Batch:   0 Loss: 0.69891 Accuracy: 0.50335\n",
      "Epoch: 341 Batch:   0 Loss: 0.69873 Accuracy: 0.50335\n",
      "Epoch: 342 Batch:   0 Loss: 0.69855 Accuracy: 0.50335\n",
      "Epoch: 343 Batch:   0 Loss: 0.69837 Accuracy: 0.50335\n",
      "Epoch: 344 Batch:   0 Loss: 0.69819 Accuracy: 0.50335\n",
      "Epoch: 345 Batch:   0 Loss: 0.69801 Accuracy: 0.50283\n",
      "Epoch: 346 Batch:   0 Loss: 0.69782 Accuracy: 0.50283\n",
      "Epoch: 347 Batch:   0 Loss: 0.69764 Accuracy: 0.50283\n",
      "Epoch: 348 Batch:   0 Loss: 0.69746 Accuracy: 0.50283\n",
      "Epoch: 349 Batch:   0 Loss: 0.69727 Accuracy: 0.50283\n",
      "Test Loss: 0.69851 Accuracy: 0.50013\n",
      "Epoch: 350 Batch:   0 Loss: 0.69708 Accuracy: 0.50283\n",
      "Epoch: 351 Batch:   0 Loss: 0.69690 Accuracy: 0.50283\n",
      "Epoch: 352 Batch:   0 Loss: 0.69671 Accuracy: 0.50283\n",
      "Epoch: 353 Batch:   0 Loss: 0.69651 Accuracy: 0.50283\n",
      "Epoch: 354 Batch:   0 Loss: 0.69632 Accuracy: 0.50283\n",
      "Epoch: 355 Batch:   0 Loss: 0.69612 Accuracy: 0.50283\n",
      "Epoch: 356 Batch:   0 Loss: 0.69592 Accuracy: 0.50283\n",
      "Epoch: 357 Batch:   0 Loss: 0.69572 Accuracy: 0.50283\n",
      "Epoch: 358 Batch:   0 Loss: 0.69552 Accuracy: 0.50283\n",
      "Epoch: 359 Batch:   0 Loss: 0.69531 Accuracy: 0.50232\n",
      "Test Loss: 0.69678 Accuracy: 0.50013\n",
      "Epoch: 360 Batch:   0 Loss: 0.69510 Accuracy: 0.50232\n",
      "Epoch: 361 Batch:   0 Loss: 0.69488 Accuracy: 0.50232\n",
      "Epoch: 362 Batch:   0 Loss: 0.69467 Accuracy: 0.50232\n",
      "Epoch: 363 Batch:   0 Loss: 0.69445 Accuracy: 0.50232\n",
      "Epoch: 364 Batch:   0 Loss: 0.69423 Accuracy: 0.50232\n",
      "Epoch: 365 Batch:   0 Loss: 0.69400 Accuracy: 0.50232\n",
      "Epoch: 366 Batch:   0 Loss: 0.69377 Accuracy: 0.50232\n",
      "Epoch: 367 Batch:   0 Loss: 0.69353 Accuracy: 0.50232\n",
      "Epoch: 368 Batch:   0 Loss: 0.69329 Accuracy: 0.50232\n",
      "Epoch: 369 Batch:   0 Loss: 0.69305 Accuracy: 0.50232\n",
      "Test Loss: 0.69489 Accuracy: 0.50013\n",
      "Epoch: 370 Batch:   0 Loss: 0.69281 Accuracy: 0.50232\n",
      "Epoch: 371 Batch:   0 Loss: 0.69256 Accuracy: 0.50232\n",
      "Epoch: 372 Batch:   0 Loss: 0.69230 Accuracy: 0.50232\n",
      "Epoch: 373 Batch:   0 Loss: 0.69204 Accuracy: 0.50232\n",
      "Epoch: 374 Batch:   0 Loss: 0.69177 Accuracy: 0.50232\n",
      "Epoch: 375 Batch:   0 Loss: 0.69150 Accuracy: 0.50232\n",
      "Epoch: 376 Batch:   0 Loss: 0.69123 Accuracy: 0.50232\n",
      "Epoch: 377 Batch:   0 Loss: 0.69095 Accuracy: 0.50232\n",
      "Epoch: 378 Batch:   0 Loss: 0.69067 Accuracy: 0.50232\n",
      "Epoch: 379 Batch:   0 Loss: 0.69038 Accuracy: 0.50232\n",
      "Test Loss: 0.69281 Accuracy: 0.50013\n",
      "Epoch: 380 Batch:   0 Loss: 0.69008 Accuracy: 0.50232\n",
      "Epoch: 381 Batch:   0 Loss: 0.68978 Accuracy: 0.50232\n",
      "Epoch: 382 Batch:   0 Loss: 0.68948 Accuracy: 0.50232\n",
      "Epoch: 383 Batch:   0 Loss: 0.68917 Accuracy: 0.50232\n",
      "Epoch: 384 Batch:   0 Loss: 0.68885 Accuracy: 0.50232\n",
      "Epoch: 385 Batch:   0 Loss: 0.68853 Accuracy: 0.50232\n",
      "Epoch: 386 Batch:   0 Loss: 0.68821 Accuracy: 0.50232\n",
      "Epoch: 387 Batch:   0 Loss: 0.68787 Accuracy: 0.50232\n",
      "Epoch: 388 Batch:   0 Loss: 0.68754 Accuracy: 0.50232\n",
      "Epoch: 389 Batch:   0 Loss: 0.68719 Accuracy: 0.50232\n",
      "Test Loss: 0.69057 Accuracy: 0.50013\n",
      "Epoch: 390 Batch:   0 Loss: 0.68684 Accuracy: 0.50232\n",
      "Epoch: 391 Batch:   0 Loss: 0.68649 Accuracy: 0.50232\n",
      "Epoch: 392 Batch:   0 Loss: 0.68613 Accuracy: 0.50232\n",
      "Epoch: 393 Batch:   0 Loss: 0.68576 Accuracy: 0.50232\n",
      "Epoch: 394 Batch:   0 Loss: 0.68539 Accuracy: 0.50232\n",
      "Epoch: 395 Batch:   0 Loss: 0.68501 Accuracy: 0.50232\n",
      "Epoch: 396 Batch:   0 Loss: 0.68462 Accuracy: 0.50232\n",
      "Epoch: 397 Batch:   0 Loss: 0.68423 Accuracy: 0.50232\n",
      "Epoch: 398 Batch:   0 Loss: 0.68383 Accuracy: 0.50232\n",
      "Epoch: 399 Batch:   0 Loss: 0.68343 Accuracy: 0.50232\n",
      "Test Loss: 0.68817 Accuracy: 0.50013\n",
      "Epoch: 400 Batch:   0 Loss: 0.68302 Accuracy: 0.50232\n",
      "Epoch: 401 Batch:   0 Loss: 0.68260 Accuracy: 0.50232\n",
      "Epoch: 402 Batch:   0 Loss: 0.68217 Accuracy: 0.50232\n",
      "Epoch: 403 Batch:   0 Loss: 0.68174 Accuracy: 0.50232\n",
      "Epoch: 404 Batch:   0 Loss: 0.68131 Accuracy: 0.50232\n",
      "Epoch: 405 Batch:   0 Loss: 0.68087 Accuracy: 0.50232\n",
      "Epoch: 406 Batch:   0 Loss: 0.68042 Accuracy: 0.50232\n",
      "Epoch: 407 Batch:   0 Loss: 0.67997 Accuracy: 0.50232\n",
      "Epoch: 408 Batch:   0 Loss: 0.67951 Accuracy: 0.50232\n",
      "Epoch: 409 Batch:   0 Loss: 0.67904 Accuracy: 0.50232\n",
      "Test Loss: 0.68568 Accuracy: 0.50013\n",
      "Epoch: 410 Batch:   0 Loss: 0.67856 Accuracy: 0.50232\n",
      "Epoch: 411 Batch:   0 Loss: 0.67808 Accuracy: 0.50232\n",
      "Epoch: 412 Batch:   0 Loss: 0.67759 Accuracy: 0.50232\n",
      "Epoch: 413 Batch:   0 Loss: 0.67710 Accuracy: 0.50232\n",
      "Epoch: 414 Batch:   0 Loss: 0.67659 Accuracy: 0.50232\n",
      "Epoch: 415 Batch:   0 Loss: 0.67609 Accuracy: 0.50232\n",
      "Epoch: 416 Batch:   0 Loss: 0.67557 Accuracy: 0.50232\n",
      "Epoch: 417 Batch:   0 Loss: 0.67505 Accuracy: 0.50232\n",
      "Epoch: 418 Batch:   0 Loss: 0.67452 Accuracy: 0.50232\n",
      "Epoch: 419 Batch:   0 Loss: 0.67399 Accuracy: 0.50232\n",
      "Test Loss: 0.68322 Accuracy: 0.50013\n",
      "Epoch: 420 Batch:   0 Loss: 0.67345 Accuracy: 0.50232\n",
      "Epoch: 421 Batch:   0 Loss: 0.67290 Accuracy: 0.50232\n",
      "Epoch: 422 Batch:   0 Loss: 0.67234 Accuracy: 0.50232\n",
      "Epoch: 423 Batch:   0 Loss: 0.67178 Accuracy: 0.50232\n",
      "Epoch: 424 Batch:   0 Loss: 0.67121 Accuracy: 0.50232\n",
      "Epoch: 425 Batch:   0 Loss: 0.67064 Accuracy: 0.50232\n",
      "Epoch: 426 Batch:   0 Loss: 0.67006 Accuracy: 0.50232\n",
      "Epoch: 427 Batch:   0 Loss: 0.66947 Accuracy: 0.50232\n",
      "Epoch: 428 Batch:   0 Loss: 0.66888 Accuracy: 0.50232\n",
      "Epoch: 429 Batch:   0 Loss: 0.66828 Accuracy: 0.50232\n",
      "Test Loss: 0.68093 Accuracy: 0.50013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 430 Batch:   0 Loss: 0.66768 Accuracy: 0.50232\n",
      "Epoch: 431 Batch:   0 Loss: 0.66707 Accuracy: 0.50232\n",
      "Epoch: 432 Batch:   0 Loss: 0.66645 Accuracy: 0.50232\n",
      "Epoch: 433 Batch:   0 Loss: 0.66583 Accuracy: 0.50232\n",
      "Epoch: 434 Batch:   0 Loss: 0.66520 Accuracy: 0.50232\n",
      "Epoch: 435 Batch:   0 Loss: 0.66457 Accuracy: 0.50232\n",
      "Epoch: 436 Batch:   0 Loss: 0.66393 Accuracy: 0.50232\n",
      "Epoch: 437 Batch:   0 Loss: 0.66329 Accuracy: 0.50232\n",
      "Epoch: 438 Batch:   0 Loss: 0.66264 Accuracy: 0.50232\n",
      "Epoch: 439 Batch:   0 Loss: 0.66199 Accuracy: 0.50232\n",
      "Test Loss: 0.67892 Accuracy: 0.50013\n",
      "Epoch: 440 Batch:   0 Loss: 0.66132 Accuracy: 0.50232\n",
      "Epoch: 441 Batch:   0 Loss: 0.66066 Accuracy: 0.50232\n",
      "Epoch: 442 Batch:   0 Loss: 0.65999 Accuracy: 0.50232\n",
      "Epoch: 443 Batch:   0 Loss: 0.65931 Accuracy: 0.50232\n",
      "Epoch: 444 Batch:   0 Loss: 0.65863 Accuracy: 0.50232\n",
      "Epoch: 445 Batch:   0 Loss: 0.65795 Accuracy: 0.50232\n",
      "Epoch: 446 Batch:   0 Loss: 0.65726 Accuracy: 0.50232\n",
      "Epoch: 447 Batch:   0 Loss: 0.65656 Accuracy: 0.50232\n",
      "Epoch: 448 Batch:   0 Loss: 0.65586 Accuracy: 0.50232\n",
      "Epoch: 449 Batch:   0 Loss: 0.65515 Accuracy: 0.50232\n",
      "Test Loss: 0.67729 Accuracy: 0.50013\n",
      "Epoch: 450 Batch:   0 Loss: 0.65444 Accuracy: 0.50232\n",
      "Epoch: 451 Batch:   0 Loss: 0.65373 Accuracy: 0.50232\n",
      "Epoch: 452 Batch:   0 Loss: 0.65301 Accuracy: 0.50232\n",
      "Epoch: 453 Batch:   0 Loss: 0.65229 Accuracy: 0.50232\n",
      "Epoch: 454 Batch:   0 Loss: 0.65157 Accuracy: 0.50232\n",
      "Epoch: 455 Batch:   0 Loss: 0.65084 Accuracy: 0.50232\n",
      "Epoch: 456 Batch:   0 Loss: 0.65011 Accuracy: 0.50232\n",
      "Epoch: 457 Batch:   0 Loss: 0.64937 Accuracy: 0.50232\n",
      "Epoch: 458 Batch:   0 Loss: 0.64863 Accuracy: 0.50232\n",
      "Epoch: 459 Batch:   0 Loss: 0.64790 Accuracy: 0.50232\n",
      "Test Loss: 0.67614 Accuracy: 0.50013\n",
      "Epoch: 460 Batch:   0 Loss: 0.64715 Accuracy: 0.50232\n",
      "Epoch: 461 Batch:   0 Loss: 0.64641 Accuracy: 0.50232\n",
      "Epoch: 462 Batch:   0 Loss: 0.64567 Accuracy: 0.50232\n",
      "Epoch: 463 Batch:   0 Loss: 0.64492 Accuracy: 0.50232\n",
      "Epoch: 464 Batch:   0 Loss: 0.64417 Accuracy: 0.50232\n",
      "Epoch: 465 Batch:   0 Loss: 0.64342 Accuracy: 0.50232\n",
      "Epoch: 466 Batch:   0 Loss: 0.64267 Accuracy: 0.50232\n",
      "Epoch: 467 Batch:   0 Loss: 0.64192 Accuracy: 0.50232\n",
      "Epoch: 468 Batch:   0 Loss: 0.64118 Accuracy: 0.50232\n",
      "Epoch: 469 Batch:   0 Loss: 0.64043 Accuracy: 0.50232\n",
      "Test Loss: 0.67545 Accuracy: 0.50013\n",
      "Epoch: 470 Batch:   0 Loss: 0.63968 Accuracy: 0.50232\n",
      "Epoch: 471 Batch:   0 Loss: 0.63893 Accuracy: 0.50232\n",
      "Epoch: 472 Batch:   0 Loss: 0.63819 Accuracy: 0.50232\n",
      "Epoch: 473 Batch:   0 Loss: 0.63745 Accuracy: 0.50232\n",
      "Epoch: 474 Batch:   0 Loss: 0.63671 Accuracy: 0.50232\n",
      "Epoch: 475 Batch:   0 Loss: 0.63597 Accuracy: 0.50232\n",
      "Epoch: 476 Batch:   0 Loss: 0.63523 Accuracy: 0.50232\n",
      "Epoch: 477 Batch:   0 Loss: 0.63449 Accuracy: 0.50232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nadit\\anaconda3\\envs\\ProtoNN\\lib\\site-packages\\sklearn\\metrics\\classification.py:872: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "[I 2024-01-05 15:56:28,799] Trial 2 finished with value: 0.0 and parameters: {'REG_W': 4.585711316360162e-06, 'REG_B': 0.009570099408400576, 'REG_Z': 2.310643699106014e-05, 'SPAR_W': 0.9066577093571926, 'SPAR_B': 0.5722913519872839, 'SPAR_Z': 0.7526557798361891, 'loss': 'xentropy', 'LEARNING_RATE': 0.0005282606618032462, 'NUM_EPOCHS': 479, 'alpha': 0.73882098493583}. Best is trial 1 with value: 0.4822216572117298.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 478 Batch:   0 Loss: 0.63376 Accuracy: 0.50232\n"
     ]
    }
   ],
   "source": [
    "op_fun = partial(objective,x_train=x_train, x_test=x_test, y_train=y_train, y_test=y_test, x_val = x_val, y_val = y_val)\n",
    "study.optimize(op_fun,n_trials=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'REG_W': 2.150499128114348e-06,\n",
       " 'REG_B': 0.0013123991795708923,\n",
       " 'REG_Z': 2.7678514122803996e-05,\n",
       " 'SPAR_W': 0.9022371274865826,\n",
       " 'SPAR_B': 0.5539345534584593,\n",
       " 'SPAR_Z': 0.9993522107302426,\n",
       " 'loss': 'l2',\n",
       " 'LEARNING_RATE': 0.00031975053586635394,\n",
       " 'NUM_EPOCHS': 300,\n",
       " 'alpha': 0.31386241034032014}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECTION_DIM = 5 #d^\n",
    "NUM_PROTOTYPES = 40 #m\n",
    "REG_W = study.best_params['REG_W']\n",
    "REG_B = study.best_params['REG_B']\n",
    "REG_Z = study.best_params['REG_Z']\n",
    "SPAR_W = study.best_params['SPAR_W']\n",
    "SPAR_B = study.best_params['SPAR_B']\n",
    "SPAR_Z = study.best_params['SPAR_Z']\n",
    "loss = study.best_params['loss']\n",
    "LEARNING_RATE = study.best_params['LEARNING_RATE']\n",
    "NUM_EPOCHS = study.best_params['NUM_EPOCHS']\n",
    "BATCH_SIZE = 2048\n",
    "GAMMA = gamma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROJECTION_DIM = 5 #d^\n",
    "# NUM_PROTOTYPES = 40 #m\n",
    "# REG_W = hyper['REG_W']\n",
    "# REG_B = hyper['REG_B']\n",
    "# REG_Z = hyper['REG_Z']\n",
    "# SPAR_W = hyper['SPAR_W']\n",
    "# SPAR_B = hyper['SPAR_B']\n",
    "# SPAR_Z = hyper['SPAR_Z']\n",
    "# LEARNING_RATE = hyper['LEARNING_RATE']\n",
    "# NUM_EPOCHS = 600\n",
    "# BATCH_SIZE = 32\n",
    "# GAMMA = gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BJwO4MXatk9G"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:07:22.641991Z",
     "start_time": "2018-08-15T13:06:10.309353Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 98358,
     "status": "ok",
     "timestamp": 1567154584840,
     "user": {
      "displayName": "Gokul Hari",
      "photoUrl": "",
      "userId": "16159457985484250305"
     },
     "user_tz": -330
    },
    "id": "MrKAP5_RQJ2b",
    "outputId": "2fb982af-47ae-4867-c5e5-b2ecc2b9dfc4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 Batch:   0 Loss: 2451.24023 Accuracy: 0.50926\n",
      "Epoch:   1 Batch:   0 Loss: 2074.21533 Accuracy: 0.50926\n",
      "Epoch:   2 Batch:   0 Loss: 1743.72595 Accuracy: 0.50926\n",
      "Epoch:   3 Batch:   0 Loss: 1460.87170 Accuracy: 0.50926\n",
      "Epoch:   4 Batch:   0 Loss: 1223.36365 Accuracy: 0.50926\n",
      "Epoch:   5 Batch:   0 Loss: 1026.71033 Accuracy: 0.50926\n",
      "Epoch:   6 Batch:   0 Loss: 865.61444 Accuracy: 0.50926\n",
      "Epoch:   7 Batch:   0 Loss: 734.78174 Accuracy: 0.50926\n",
      "Epoch:   8 Batch:   0 Loss: 629.30042 Accuracy: 0.50926\n",
      "Epoch:   9 Batch:   0 Loss: 544.80389 Accuracy: 0.50926\n",
      "Test Loss: 477.86724 Accuracy: 0.50013\n",
      "Epoch:  10 Batch:   0 Loss: 477.50461 Accuracy: 0.50926\n",
      "Epoch:  11 Batch:   0 Loss: 424.18887 Accuracy: 0.50926\n",
      "Epoch:  12 Batch:   0 Loss: 382.17645 Accuracy: 0.50926\n",
      "Epoch:  13 Batch:   0 Loss: 349.25397 Accuracy: 0.50926\n",
      "Epoch:  14 Batch:   0 Loss: 323.61194 Accuracy: 0.50926\n",
      "Epoch:  15 Batch:   0 Loss: 303.77664 Accuracy: 0.50926\n",
      "Epoch:  16 Batch:   0 Loss: 288.55142 Accuracy: 0.50926\n",
      "Epoch:  17 Batch:   0 Loss: 276.96402 Accuracy: 0.50926\n",
      "Epoch:  18 Batch:   0 Loss: 268.22723 Accuracy: 0.50926\n",
      "Epoch:  19 Batch:   0 Loss: 261.70477 Accuracy: 0.50926\n",
      "Test Loss: 254.83389 Accuracy: 0.50026\n",
      "Epoch:  20 Batch:   0 Loss: 256.88544 Accuracy: 0.50926\n",
      "Epoch:  21 Batch:   0 Loss: 253.36122 Accuracy: 0.50926\n",
      "Epoch:  22 Batch:   0 Loss: 250.80997 Accuracy: 0.50926\n",
      "Epoch:  23 Batch:   0 Loss: 248.97993 Accuracy: 0.50926\n",
      "Epoch:  24 Batch:   0 Loss: 247.67715 Accuracy: 0.50926\n",
      "Epoch:  25 Batch:   0 Loss: 246.75414 Accuracy: 0.50926\n",
      "Epoch:  26 Batch:   0 Loss: 246.10049 Accuracy: 0.50926\n",
      "Epoch:  27 Batch:   0 Loss: 245.63466 Accuracy: 0.50926\n",
      "Epoch:  28 Batch:   0 Loss: 245.29752 Accuracy: 0.50926\n",
      "Epoch:  29 Batch:   0 Loss: 245.04672 Accuracy: 0.50926\n",
      "Test Loss: 241.86007 Accuracy: 0.50784\n",
      "Epoch:  30 Batch:   0 Loss: 244.85275 Accuracy: 0.51132\n",
      "Epoch:  31 Batch:   0 Loss: 244.69505 Accuracy: 0.51235\n",
      "Epoch:  32 Batch:   0 Loss: 244.55977 Accuracy: 0.51440\n",
      "Epoch:  33 Batch:   0 Loss: 244.43784 Accuracy: 0.51543\n",
      "Epoch:  34 Batch:   0 Loss: 244.32318 Accuracy: 0.51955\n",
      "Epoch:  35 Batch:   0 Loss: 244.21208 Accuracy: 0.52366\n",
      "Epoch:  36 Batch:   0 Loss: 244.10242 Accuracy: 0.53601\n",
      "Epoch:  37 Batch:   0 Loss: 243.99269 Accuracy: 0.55144\n",
      "Epoch:  38 Batch:   0 Loss: 243.88226 Accuracy: 0.56379\n",
      "Epoch:  39 Batch:   0 Loss: 243.77092 Accuracy: 0.57716\n",
      "Test Loss: 241.01891 Accuracy: 0.57145\n",
      "Epoch:  40 Batch:   0 Loss: 243.65836 Accuracy: 0.58230\n",
      "Epoch:  41 Batch:   0 Loss: 243.54468 Accuracy: 0.59259\n",
      "Epoch:  42 Batch:   0 Loss: 243.42978 Accuracy: 0.60082\n",
      "Epoch:  43 Batch:   0 Loss: 243.31389 Accuracy: 0.61008\n",
      "Epoch:  44 Batch:   0 Loss: 243.19688 Accuracy: 0.60905\n",
      "Epoch:  45 Batch:   0 Loss: 243.07899 Accuracy: 0.61934\n",
      "Epoch:  46 Batch:   0 Loss: 242.96014 Accuracy: 0.62449\n",
      "Epoch:  47 Batch:   0 Loss: 242.84044 Accuracy: 0.62963\n",
      "Epoch:  48 Batch:   0 Loss: 242.71977 Accuracy: 0.63683\n",
      "Epoch:  49 Batch:   0 Loss: 242.59827 Accuracy: 0.63889\n",
      "Test Loss: 240.42747 Accuracy: 0.60410\n",
      "Epoch:  50 Batch:   0 Loss: 242.47580 Accuracy: 0.64095\n",
      "Epoch:  51 Batch:   0 Loss: 242.35240 Accuracy: 0.63992\n",
      "Epoch:  52 Batch:   0 Loss: 242.22801 Accuracy: 0.64506\n",
      "Epoch:  53 Batch:   0 Loss: 242.10268 Accuracy: 0.64609\n",
      "Epoch:  54 Batch:   0 Loss: 241.97623 Accuracy: 0.64712\n",
      "Epoch:  55 Batch:   0 Loss: 241.84869 Accuracy: 0.64815\n",
      "Epoch:  56 Batch:   0 Loss: 241.72000 Accuracy: 0.65021\n",
      "Epoch:  57 Batch:   0 Loss: 241.59016 Accuracy: 0.65021\n",
      "Epoch:  58 Batch:   0 Loss: 241.45905 Accuracy: 0.65226\n",
      "Epoch:  59 Batch:   0 Loss: 241.32668 Accuracy: 0.65123\n",
      "Test Loss: 239.79664 Accuracy: 0.62395\n",
      "Epoch:  60 Batch:   0 Loss: 241.19287 Accuracy: 0.65123\n",
      "Epoch:  61 Batch:   0 Loss: 241.05779 Accuracy: 0.65226\n",
      "Epoch:  62 Batch:   0 Loss: 240.92125 Accuracy: 0.65535\n",
      "Epoch:  63 Batch:   0 Loss: 240.78320 Accuracy: 0.65535\n",
      "Epoch:  64 Batch:   0 Loss: 240.64362 Accuracy: 0.65844\n",
      "Epoch:  65 Batch:   0 Loss: 240.50259 Accuracy: 0.66152\n",
      "Epoch:  66 Batch:   0 Loss: 240.35980 Accuracy: 0.66461\n",
      "Epoch:  67 Batch:   0 Loss: 240.21542 Accuracy: 0.66770\n",
      "Epoch:  68 Batch:   0 Loss: 240.06934 Accuracy: 0.66872\n",
      "Epoch:  69 Batch:   0 Loss: 239.92149 Accuracy: 0.66975\n",
      "Test Loss: 239.09454 Accuracy: 0.64184\n",
      "Epoch:  70 Batch:   0 Loss: 239.77193 Accuracy: 0.67490\n",
      "Epoch:  71 Batch:   0 Loss: 239.62048 Accuracy: 0.67798\n",
      "Epoch:  72 Batch:   0 Loss: 239.46716 Accuracy: 0.67901\n",
      "Epoch:  73 Batch:   0 Loss: 239.31201 Accuracy: 0.68519\n",
      "Epoch:  74 Batch:   0 Loss: 239.15491 Accuracy: 0.68930\n",
      "Epoch:  75 Batch:   0 Loss: 238.99586 Accuracy: 0.69342\n",
      "Epoch:  76 Batch:   0 Loss: 238.83447 Accuracy: 0.69547\n",
      "Epoch:  77 Batch:   0 Loss: 238.67085 Accuracy: 0.69547\n",
      "Epoch:  78 Batch:   0 Loss: 238.50552 Accuracy: 0.69650\n",
      "Epoch:  79 Batch:   0 Loss: 238.33807 Accuracy: 0.69753\n",
      "Test Loss: 238.28964 Accuracy: 0.64563\n",
      "Epoch:  80 Batch:   0 Loss: 238.16858 Accuracy: 0.69650\n",
      "Epoch:  81 Batch:   0 Loss: 237.99696 Accuracy: 0.69650\n",
      "Epoch:  82 Batch:   0 Loss: 237.82315 Accuracy: 0.69753\n",
      "Epoch:  83 Batch:   0 Loss: 237.64723 Accuracy: 0.69650\n",
      "Epoch:  84 Batch:   0 Loss: 237.46902 Accuracy: 0.69650\n",
      "Epoch:  85 Batch:   0 Loss: 237.28862 Accuracy: 0.69753\n",
      "Epoch:  86 Batch:   0 Loss: 237.10603 Accuracy: 0.69856\n",
      "Epoch:  87 Batch:   0 Loss: 236.92114 Accuracy: 0.69856\n",
      "Epoch:  88 Batch:   0 Loss: 236.73349 Accuracy: 0.69959\n",
      "Epoch:  89 Batch:   0 Loss: 236.54323 Accuracy: 0.69959\n",
      "Test Loss: 237.35043 Accuracy: 0.64981\n",
      "Epoch:  90 Batch:   0 Loss: 236.35088 Accuracy: 0.69856\n",
      "Epoch:  91 Batch:   0 Loss: 236.15648 Accuracy: 0.69856\n",
      "Epoch:  92 Batch:   0 Loss: 235.95978 Accuracy: 0.70062\n",
      "Epoch:  93 Batch:   0 Loss: 235.76068 Accuracy: 0.70062\n",
      "Epoch:  94 Batch:   0 Loss: 235.55933 Accuracy: 0.69959\n",
      "Epoch:  95 Batch:   0 Loss: 235.35526 Accuracy: 0.70062\n",
      "Epoch:  96 Batch:   0 Loss: 235.14906 Accuracy: 0.70062\n",
      "Epoch:  97 Batch:   0 Loss: 234.94044 Accuracy: 0.70165\n",
      "Epoch:  98 Batch:   0 Loss: 234.72945 Accuracy: 0.70267\n",
      "Epoch:  99 Batch:   0 Loss: 234.51611 Accuracy: 0.70370\n",
      "Test Loss: 236.24930 Accuracy: 0.65320\n",
      "Epoch: 100 Batch:   0 Loss: 234.30038 Accuracy: 0.70370\n",
      "Epoch: 101 Batch:   0 Loss: 234.08221 Accuracy: 0.70267\n",
      "Epoch: 102 Batch:   0 Loss: 233.86171 Accuracy: 0.70473\n",
      "Epoch: 103 Batch:   0 Loss: 233.63849 Accuracy: 0.70473\n",
      "Epoch: 104 Batch:   0 Loss: 233.41290 Accuracy: 0.70473\n",
      "Epoch: 105 Batch:   0 Loss: 233.18468 Accuracy: 0.70576\n",
      "Epoch: 106 Batch:   0 Loss: 232.95393 Accuracy: 0.70576\n",
      "Epoch: 107 Batch:   0 Loss: 232.72086 Accuracy: 0.70473\n",
      "Epoch: 108 Batch:   0 Loss: 232.48543 Accuracy: 0.70370\n",
      "Epoch: 109 Batch:   0 Loss: 232.24654 Accuracy: 0.70370\n",
      "Test Loss: 234.95958 Accuracy: 0.65646\n",
      "Epoch: 110 Batch:   0 Loss: 232.00616 Accuracy: 0.70473\n",
      "Epoch: 111 Batch:   0 Loss: 231.76331 Accuracy: 0.70473\n",
      "Epoch: 112 Batch:   0 Loss: 231.51703 Accuracy: 0.70576\n",
      "Epoch: 113 Batch:   0 Loss: 231.26953 Accuracy: 0.70988\n",
      "Epoch: 114 Batch:   0 Loss: 231.01962 Accuracy: 0.71091\n",
      "Epoch: 115 Batch:   0 Loss: 230.76724 Accuracy: 0.71091\n",
      "Epoch: 116 Batch:   0 Loss: 230.50534 Accuracy: 0.71193\n",
      "Epoch: 117 Batch:   0 Loss: 230.24002 Accuracy: 0.71296\n",
      "Epoch: 118 Batch:   0 Loss: 229.98125 Accuracy: 0.71399\n",
      "Epoch: 119 Batch:   0 Loss: 229.71716 Accuracy: 0.71502\n",
      "Test Loss: 233.44702 Accuracy: 0.65959\n",
      "Epoch: 120 Batch:   0 Loss: 229.45206 Accuracy: 0.71502\n",
      "Epoch: 121 Batch:   0 Loss: 229.18448 Accuracy: 0.71605\n",
      "Epoch: 122 Batch:   0 Loss: 228.91261 Accuracy: 0.71811\n",
      "Epoch: 123 Batch:   0 Loss: 228.64153 Accuracy: 0.71605\n",
      "Epoch: 124 Batch:   0 Loss: 228.36800 Accuracy: 0.71708\n",
      "Epoch: 125 Batch:   0 Loss: 228.09210 Accuracy: 0.71811\n",
      "Epoch: 126 Batch:   0 Loss: 227.81386 Accuracy: 0.71811\n",
      "Epoch: 127 Batch:   0 Loss: 227.53317 Accuracy: 0.71811\n",
      "Epoch: 128 Batch:   0 Loss: 227.24591 Accuracy: 0.72016\n",
      "Epoch: 129 Batch:   0 Loss: 226.96071 Accuracy: 0.72016\n",
      "Test Loss: 231.73658 Accuracy: 0.66312\n",
      "Epoch: 130 Batch:   0 Loss: 226.66881 Accuracy: 0.72119\n",
      "Epoch: 131 Batch:   0 Loss: 226.37904 Accuracy: 0.72531\n",
      "Epoch: 132 Batch:   0 Loss: 226.08717 Accuracy: 0.72531\n",
      "Epoch: 133 Batch:   0 Loss: 225.79291 Accuracy: 0.72428\n",
      "Epoch: 134 Batch:   0 Loss: 225.49626 Accuracy: 0.72428\n",
      "Epoch: 135 Batch:   0 Loss: 225.19286 Accuracy: 0.72531\n",
      "Epoch: 136 Batch:   0 Loss: 224.87514 Accuracy: 0.72531\n",
      "Epoch: 137 Batch:   0 Loss: 224.56514 Accuracy: 0.72428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 138 Batch:   0 Loss: 224.24927 Accuracy: 0.72428\n",
      "Epoch: 139 Batch:   0 Loss: 223.93344 Accuracy: 0.72325\n",
      "Test Loss: 229.80793 Accuracy: 0.66756\n",
      "Epoch: 140 Batch:   0 Loss: 223.60954 Accuracy: 0.72325\n",
      "Epoch: 141 Batch:   0 Loss: 223.27963 Accuracy: 0.72428\n",
      "Epoch: 142 Batch:   0 Loss: 222.95563 Accuracy: 0.72634\n",
      "Epoch: 143 Batch:   0 Loss: 222.62927 Accuracy: 0.72634\n",
      "Epoch: 144 Batch:   0 Loss: 222.30005 Accuracy: 0.72531\n",
      "Epoch: 145 Batch:   0 Loss: 221.96027 Accuracy: 0.72737\n",
      "Epoch: 146 Batch:   0 Loss: 221.62448 Accuracy: 0.72942\n",
      "Epoch: 147 Batch:   0 Loss: 221.28580 Accuracy: 0.73148\n",
      "Epoch: 148 Batch:   0 Loss: 220.94569 Accuracy: 0.73251\n",
      "Epoch: 149 Batch:   0 Loss: 220.60274 Accuracy: 0.73457\n",
      "Test Loss: 227.66409 Accuracy: 0.67383\n",
      "Epoch: 150 Batch:   0 Loss: 220.25867 Accuracy: 0.73971\n",
      "Epoch: 151 Batch:   0 Loss: 219.91237 Accuracy: 0.74280\n",
      "Epoch: 152 Batch:   0 Loss: 219.56287 Accuracy: 0.74383\n",
      "Epoch: 153 Batch:   0 Loss: 219.21159 Accuracy: 0.74691\n",
      "Epoch: 154 Batch:   0 Loss: 218.85799 Accuracy: 0.74794\n",
      "Epoch: 155 Batch:   0 Loss: 218.50266 Accuracy: 0.75103\n",
      "Epoch: 156 Batch:   0 Loss: 218.14355 Accuracy: 0.75412\n",
      "Epoch: 157 Batch:   0 Loss: 217.78329 Accuracy: 0.75309\n",
      "Epoch: 158 Batch:   0 Loss: 217.41376 Accuracy: 0.75309\n",
      "Epoch: 159 Batch:   0 Loss: 217.04688 Accuracy: 0.75412\n",
      "Test Loss: 225.34139 Accuracy: 0.68349\n",
      "Epoch: 160 Batch:   0 Loss: 216.67274 Accuracy: 0.75514\n",
      "Epoch: 161 Batch:   0 Loss: 216.30266 Accuracy: 0.75617\n",
      "Epoch: 162 Batch:   0 Loss: 215.93001 Accuracy: 0.75926\n",
      "Epoch: 163 Batch:   0 Loss: 215.55588 Accuracy: 0.76029\n",
      "Epoch: 164 Batch:   0 Loss: 215.18080 Accuracy: 0.76029\n",
      "Epoch: 165 Batch:   0 Loss: 214.80443 Accuracy: 0.76235\n",
      "Epoch: 166 Batch:   0 Loss: 214.42670 Accuracy: 0.76337\n",
      "Epoch: 167 Batch:   0 Loss: 214.04579 Accuracy: 0.76337\n",
      "Epoch: 168 Batch:   0 Loss: 213.66115 Accuracy: 0.76440\n",
      "Epoch: 169 Batch:   0 Loss: 213.28006 Accuracy: 0.76646\n",
      "Test Loss: 222.84187 Accuracy: 0.69734\n",
      "Epoch: 170 Batch:   0 Loss: 212.89557 Accuracy: 0.76646\n",
      "Epoch: 171 Batch:   0 Loss: 212.51164 Accuracy: 0.76852\n",
      "Epoch: 172 Batch:   0 Loss: 212.12247 Accuracy: 0.76955\n",
      "Epoch: 173 Batch:   0 Loss: 211.73560 Accuracy: 0.77469\n",
      "Epoch: 174 Batch:   0 Loss: 211.34767 Accuracy: 0.77469\n",
      "Epoch: 175 Batch:   0 Loss: 210.95863 Accuracy: 0.77881\n",
      "Epoch: 176 Batch:   0 Loss: 210.56837 Accuracy: 0.77881\n",
      "Epoch: 177 Batch:   0 Loss: 210.17758 Accuracy: 0.78189\n",
      "Epoch: 178 Batch:   0 Loss: 209.78137 Accuracy: 0.78395\n",
      "Epoch: 179 Batch:   0 Loss: 209.38451 Accuracy: 0.78498\n",
      "Test Loss: 220.22064 Accuracy: 0.71536\n",
      "Epoch: 180 Batch:   0 Loss: 208.98956 Accuracy: 0.78601\n",
      "Epoch: 181 Batch:   0 Loss: 208.59491 Accuracy: 0.78601\n",
      "Epoch: 182 Batch:   0 Loss: 208.19962 Accuracy: 0.78807\n",
      "Epoch: 183 Batch:   0 Loss: 207.80357 Accuracy: 0.78909\n",
      "Epoch: 184 Batch:   0 Loss: 207.40677 Accuracy: 0.78909\n",
      "Epoch: 185 Batch:   0 Loss: 207.00917 Accuracy: 0.79218\n",
      "Epoch: 186 Batch:   0 Loss: 206.61055 Accuracy: 0.79218\n",
      "Epoch: 187 Batch:   0 Loss: 206.21132 Accuracy: 0.79321\n",
      "Epoch: 188 Batch:   0 Loss: 205.81116 Accuracy: 0.79527\n",
      "Epoch: 189 Batch:   0 Loss: 205.40826 Accuracy: 0.79630\n",
      "Test Loss: 217.57046 Accuracy: 0.72620\n",
      "Epoch: 190 Batch:   0 Loss: 205.00554 Accuracy: 0.79835\n",
      "Epoch: 191 Batch:   0 Loss: 204.59918 Accuracy: 0.79835\n",
      "Epoch: 192 Batch:   0 Loss: 204.19571 Accuracy: 0.79938\n",
      "Epoch: 193 Batch:   0 Loss: 203.78662 Accuracy: 0.79938\n",
      "Epoch: 194 Batch:   0 Loss: 203.38058 Accuracy: 0.80247\n",
      "Epoch: 195 Batch:   0 Loss: 202.97429 Accuracy: 0.80453\n",
      "Epoch: 196 Batch:   0 Loss: 202.56346 Accuracy: 0.80658\n",
      "Epoch: 197 Batch:   0 Loss: 202.15619 Accuracy: 0.80658\n",
      "Epoch: 198 Batch:   0 Loss: 201.75110 Accuracy: 0.80864\n",
      "Epoch: 199 Batch:   0 Loss: 201.34213 Accuracy: 0.80864\n",
      "Test Loss: 214.94092 Accuracy: 0.73116\n",
      "Epoch: 200 Batch:   0 Loss: 200.93901 Accuracy: 0.80864\n",
      "Epoch: 201 Batch:   0 Loss: 200.53554 Accuracy: 0.80864\n",
      "Epoch: 202 Batch:   0 Loss: 200.13161 Accuracy: 0.80864\n",
      "Epoch: 203 Batch:   0 Loss: 199.72499 Accuracy: 0.81070\n",
      "Epoch: 204 Batch:   0 Loss: 199.31903 Accuracy: 0.81173\n",
      "Epoch: 205 Batch:   0 Loss: 198.91266 Accuracy: 0.81173\n",
      "Epoch: 206 Batch:   0 Loss: 198.50591 Accuracy: 0.81276\n",
      "Epoch: 207 Batch:   0 Loss: 198.09613 Accuracy: 0.81379\n",
      "Epoch: 208 Batch:   0 Loss: 197.68242 Accuracy: 0.81481\n",
      "Epoch: 209 Batch:   0 Loss: 197.26991 Accuracy: 0.81790\n",
      "Test Loss: 212.38710 Accuracy: 0.73834\n",
      "Epoch: 210 Batch:   0 Loss: 196.85759 Accuracy: 0.81790\n",
      "Epoch: 211 Batch:   0 Loss: 196.44548 Accuracy: 0.81893\n",
      "Epoch: 212 Batch:   0 Loss: 196.03323 Accuracy: 0.81893\n",
      "Epoch: 213 Batch:   0 Loss: 195.62108 Accuracy: 0.81893\n",
      "Epoch: 214 Batch:   0 Loss: 195.20836 Accuracy: 0.81996\n",
      "Epoch: 215 Batch:   0 Loss: 194.79474 Accuracy: 0.82202\n",
      "Epoch: 216 Batch:   0 Loss: 194.38062 Accuracy: 0.82202\n",
      "Epoch: 217 Batch:   0 Loss: 193.96883 Accuracy: 0.82305\n",
      "Epoch: 218 Batch:   0 Loss: 193.55696 Accuracy: 0.82510\n",
      "Epoch: 219 Batch:   0 Loss: 193.14268 Accuracy: 0.82613\n",
      "Test Loss: 209.92393 Accuracy: 0.74448\n",
      "Epoch: 220 Batch:   0 Loss: 192.72940 Accuracy: 0.82613\n",
      "Epoch: 221 Batch:   0 Loss: 192.31859 Accuracy: 0.82819\n",
      "Epoch: 222 Batch:   0 Loss: 191.90776 Accuracy: 0.82922\n",
      "Epoch: 223 Batch:   0 Loss: 191.48938 Accuracy: 0.82922\n",
      "Epoch: 224 Batch:   0 Loss: 191.07739 Accuracy: 0.83128\n",
      "Epoch: 225 Batch:   0 Loss: 190.66638 Accuracy: 0.83333\n",
      "Epoch: 226 Batch:   0 Loss: 190.25549 Accuracy: 0.83436\n",
      "Epoch: 227 Batch:   0 Loss: 189.84351 Accuracy: 0.83436\n",
      "Epoch: 228 Batch:   0 Loss: 189.43243 Accuracy: 0.83436\n",
      "Epoch: 229 Batch:   0 Loss: 189.02112 Accuracy: 0.83436\n",
      "Test Loss: 207.57233 Accuracy: 0.74591\n",
      "Epoch: 230 Batch:   0 Loss: 188.60959 Accuracy: 0.83539\n",
      "Epoch: 231 Batch:   0 Loss: 188.19824 Accuracy: 0.83745\n",
      "Epoch: 232 Batch:   0 Loss: 187.78703 Accuracy: 0.83745\n",
      "Epoch: 233 Batch:   0 Loss: 187.37589 Accuracy: 0.83745\n",
      "Epoch: 234 Batch:   0 Loss: 186.96519 Accuracy: 0.83848\n",
      "Epoch: 235 Batch:   0 Loss: 186.55470 Accuracy: 0.83745\n",
      "Epoch: 236 Batch:   0 Loss: 186.14420 Accuracy: 0.83745\n",
      "Epoch: 237 Batch:   0 Loss: 185.73384 Accuracy: 0.83951\n",
      "Epoch: 238 Batch:   0 Loss: 185.32471 Accuracy: 0.83951\n",
      "Epoch: 239 Batch:   0 Loss: 184.91475 Accuracy: 0.84156\n",
      "Test Loss: 205.33058 Accuracy: 0.74970\n",
      "Epoch: 240 Batch:   0 Loss: 184.50529 Accuracy: 0.84362\n",
      "Epoch: 241 Batch:   0 Loss: 184.09636 Accuracy: 0.84362\n",
      "Epoch: 242 Batch:   0 Loss: 183.68773 Accuracy: 0.84362\n",
      "Epoch: 243 Batch:   0 Loss: 183.27917 Accuracy: 0.84568\n",
      "Epoch: 244 Batch:   0 Loss: 182.87128 Accuracy: 0.84671\n",
      "Epoch: 245 Batch:   0 Loss: 182.46375 Accuracy: 0.84774\n",
      "Epoch: 246 Batch:   0 Loss: 182.05632 Accuracy: 0.84774\n",
      "Epoch: 247 Batch:   0 Loss: 181.64925 Accuracy: 0.84877\n",
      "Epoch: 248 Batch:   0 Loss: 181.24283 Accuracy: 0.84877\n",
      "Epoch: 249 Batch:   0 Loss: 180.83684 Accuracy: 0.84979\n",
      "Test Loss: 203.19934 Accuracy: 0.74931\n",
      "Epoch: 250 Batch:   0 Loss: 180.43071 Accuracy: 0.85185\n",
      "Epoch: 251 Batch:   0 Loss: 180.02524 Accuracy: 0.85288\n",
      "Epoch: 252 Batch:   0 Loss: 179.62021 Accuracy: 0.85288\n",
      "Epoch: 253 Batch:   0 Loss: 179.21565 Accuracy: 0.85288\n",
      "Epoch: 254 Batch:   0 Loss: 178.81148 Accuracy: 0.85288\n",
      "Epoch: 255 Batch:   0 Loss: 178.40779 Accuracy: 0.85391\n",
      "Epoch: 256 Batch:   0 Loss: 178.00427 Accuracy: 0.85391\n",
      "Epoch: 257 Batch:   0 Loss: 177.60179 Accuracy: 0.85494\n",
      "Epoch: 258 Batch:   0 Loss: 177.19966 Accuracy: 0.85597\n",
      "Epoch: 259 Batch:   0 Loss: 176.79797 Accuracy: 0.85597\n",
      "Test Loss: 201.19099 Accuracy: 0.75035\n",
      "Epoch: 260 Batch:   0 Loss: 176.39674 Accuracy: 0.85597\n",
      "Epoch: 261 Batch:   0 Loss: 175.99632 Accuracy: 0.85700\n",
      "Epoch: 262 Batch:   0 Loss: 175.59622 Accuracy: 0.85700\n",
      "Epoch: 263 Batch:   0 Loss: 175.19678 Accuracy: 0.85700\n",
      "Epoch: 264 Batch:   0 Loss: 174.79807 Accuracy: 0.85905\n",
      "Epoch: 265 Batch:   0 Loss: 174.40001 Accuracy: 0.85905\n",
      "Epoch: 266 Batch:   0 Loss: 174.00247 Accuracy: 0.85905\n",
      "Epoch: 267 Batch:   0 Loss: 173.60542 Accuracy: 0.85905\n",
      "Epoch: 268 Batch:   0 Loss: 173.20918 Accuracy: 0.85905\n",
      "Epoch: 269 Batch:   0 Loss: 172.81361 Accuracy: 0.86008\n",
      "Test Loss: 199.29580 Accuracy: 0.75100\n",
      "Epoch: 270 Batch:   0 Loss: 172.41867 Accuracy: 0.86008\n",
      "Epoch: 271 Batch:   0 Loss: 172.02440 Accuracy: 0.86008\n",
      "Epoch: 272 Batch:   0 Loss: 171.63066 Accuracy: 0.86008\n",
      "Epoch: 273 Batch:   0 Loss: 171.23781 Accuracy: 0.86008\n",
      "Epoch: 274 Batch:   0 Loss: 170.84543 Accuracy: 0.86111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 275 Batch:   0 Loss: 170.45360 Accuracy: 0.86111\n",
      "Epoch: 276 Batch:   0 Loss: 170.06297 Accuracy: 0.86111\n",
      "Epoch: 277 Batch:   0 Loss: 169.67278 Accuracy: 0.86214\n",
      "Epoch: 278 Batch:   0 Loss: 169.28302 Accuracy: 0.86214\n",
      "Epoch: 279 Batch:   0 Loss: 168.89392 Accuracy: 0.86214\n",
      "Test Loss: 197.50776 Accuracy: 0.75218\n",
      "Epoch: 280 Batch:   0 Loss: 168.50591 Accuracy: 0.86214\n",
      "Epoch: 281 Batch:   0 Loss: 168.11650 Accuracy: 0.86214\n",
      "Epoch: 282 Batch:   0 Loss: 167.72818 Accuracy: 0.86214\n",
      "Epoch: 283 Batch:   0 Loss: 167.34050 Accuracy: 0.86317\n",
      "Epoch: 284 Batch:   0 Loss: 166.95360 Accuracy: 0.86317\n",
      "Epoch: 285 Batch:   0 Loss: 166.56729 Accuracy: 0.86317\n",
      "Epoch: 286 Batch:   0 Loss: 166.18208 Accuracy: 0.86420\n",
      "Epoch: 287 Batch:   0 Loss: 165.79742 Accuracy: 0.86420\n",
      "Epoch: 288 Batch:   0 Loss: 165.41367 Accuracy: 0.86420\n",
      "Epoch: 289 Batch:   0 Loss: 165.03093 Accuracy: 0.86523\n",
      "Test Loss: 195.81694 Accuracy: 0.75440\n",
      "Epoch: 290 Batch:   0 Loss: 164.64886 Accuracy: 0.86523\n",
      "Epoch: 291 Batch:   0 Loss: 164.26761 Accuracy: 0.86523\n",
      "Epoch: 292 Batch:   0 Loss: 163.88692 Accuracy: 0.86523\n",
      "Epoch: 293 Batch:   0 Loss: 163.50716 Accuracy: 0.86626\n",
      "Epoch: 294 Batch:   0 Loss: 163.12811 Accuracy: 0.86626\n",
      "Epoch: 295 Batch:   0 Loss: 162.74986 Accuracy: 0.86626\n",
      "Epoch: 296 Batch:   0 Loss: 162.37238 Accuracy: 0.86626\n",
      "Epoch: 297 Batch:   0 Loss: 161.99561 Accuracy: 0.86626\n",
      "Epoch: 298 Batch:   0 Loss: 161.61969 Accuracy: 0.86728\n",
      "Epoch: 299 Batch:   0 Loss: 161.24457 Accuracy: 0.86626\n",
      "Test Loss: 194.20961 Accuracy: 0.75727\n",
      "Epoch: 300 Batch:   0 Loss: 160.87007 Accuracy: 0.86728\n",
      "Epoch: 301 Batch:   0 Loss: 160.49640 Accuracy: 0.86831\n",
      "Epoch: 302 Batch:   0 Loss: 160.12383 Accuracy: 0.86934\n",
      "Epoch: 303 Batch:   0 Loss: 159.75200 Accuracy: 0.86934\n",
      "Epoch: 304 Batch:   0 Loss: 159.38080 Accuracy: 0.87037\n",
      "Epoch: 305 Batch:   0 Loss: 159.01064 Accuracy: 0.87037\n",
      "Epoch: 306 Batch:   0 Loss: 158.64099 Accuracy: 0.87037\n",
      "Epoch: 307 Batch:   0 Loss: 158.27213 Accuracy: 0.87037\n",
      "Epoch: 308 Batch:   0 Loss: 157.90372 Accuracy: 0.87037\n",
      "Epoch: 309 Batch:   0 Loss: 157.53661 Accuracy: 0.87140\n",
      "Test Loss: 192.66733 Accuracy: 0.75832\n",
      "Epoch: 310 Batch:   0 Loss: 157.17035 Accuracy: 0.87140\n",
      "Epoch: 311 Batch:   0 Loss: 156.80464 Accuracy: 0.87140\n",
      "Epoch: 312 Batch:   0 Loss: 156.43997 Accuracy: 0.87140\n",
      "Epoch: 313 Batch:   0 Loss: 156.07613 Accuracy: 0.87140\n",
      "Epoch: 314 Batch:   0 Loss: 155.71310 Accuracy: 0.87243\n",
      "Epoch: 315 Batch:   0 Loss: 155.35065 Accuracy: 0.87243\n",
      "Epoch: 316 Batch:   0 Loss: 154.98914 Accuracy: 0.87243\n",
      "Epoch: 317 Batch:   0 Loss: 154.62851 Accuracy: 0.87243\n",
      "Epoch: 318 Batch:   0 Loss: 154.26862 Accuracy: 0.87243\n",
      "Epoch: 319 Batch:   0 Loss: 153.90948 Accuracy: 0.87243\n",
      "Test Loss: 191.18132 Accuracy: 0.75989\n",
      "Epoch: 320 Batch:   0 Loss: 153.55193 Accuracy: 0.87243\n",
      "Epoch: 321 Batch:   0 Loss: 153.19475 Accuracy: 0.87243\n",
      "Epoch: 322 Batch:   0 Loss: 152.83842 Accuracy: 0.87243\n",
      "Epoch: 323 Batch:   0 Loss: 152.48299 Accuracy: 0.87243\n",
      "Epoch: 324 Batch:   0 Loss: 152.12822 Accuracy: 0.87346\n",
      "Epoch: 325 Batch:   0 Loss: 151.77415 Accuracy: 0.87346\n",
      "Epoch: 326 Batch:   0 Loss: 151.42079 Accuracy: 0.87346\n",
      "Epoch: 327 Batch:   0 Loss: 151.06845 Accuracy: 0.87449\n",
      "Epoch: 328 Batch:   0 Loss: 150.71381 Accuracy: 0.87449\n",
      "Epoch: 329 Batch:   0 Loss: 150.35986 Accuracy: 0.87449\n",
      "Test Loss: 189.74272 Accuracy: 0.76028\n",
      "Epoch: 330 Batch:   0 Loss: 150.00681 Accuracy: 0.87449\n",
      "Epoch: 331 Batch:   0 Loss: 149.65453 Accuracy: 0.87449\n",
      "Epoch: 332 Batch:   0 Loss: 149.30305 Accuracy: 0.87449\n",
      "Epoch: 333 Batch:   0 Loss: 148.95236 Accuracy: 0.87449\n",
      "Epoch: 334 Batch:   0 Loss: 148.60257 Accuracy: 0.87551\n",
      "Epoch: 335 Batch:   0 Loss: 148.25336 Accuracy: 0.87551\n",
      "Epoch: 336 Batch:   0 Loss: 147.90515 Accuracy: 0.87551\n",
      "Epoch: 337 Batch:   0 Loss: 147.55777 Accuracy: 0.87551\n",
      "Epoch: 338 Batch:   0 Loss: 147.21097 Accuracy: 0.87551\n",
      "Epoch: 339 Batch:   0 Loss: 146.86507 Accuracy: 0.87654\n",
      "Test Loss: 188.34202 Accuracy: 0.75976\n",
      "Epoch: 340 Batch:   0 Loss: 146.51991 Accuracy: 0.87654\n",
      "Epoch: 341 Batch:   0 Loss: 146.17563 Accuracy: 0.87654\n",
      "Epoch: 342 Batch:   0 Loss: 145.83221 Accuracy: 0.87757\n",
      "Epoch: 343 Batch:   0 Loss: 145.48952 Accuracy: 0.87757\n",
      "Epoch: 344 Batch:   0 Loss: 145.14761 Accuracy: 0.87757\n",
      "Epoch: 345 Batch:   0 Loss: 144.80647 Accuracy: 0.87860\n",
      "Epoch: 346 Batch:   0 Loss: 144.46603 Accuracy: 0.87860\n",
      "Epoch: 347 Batch:   0 Loss: 144.12627 Accuracy: 0.88066\n",
      "Epoch: 348 Batch:   0 Loss: 143.78740 Accuracy: 0.88066\n",
      "Epoch: 349 Batch:   0 Loss: 143.44991 Accuracy: 0.88066\n",
      "Test Loss: 186.98205 Accuracy: 0.75989\n",
      "Epoch: 350 Batch:   0 Loss: 143.11273 Accuracy: 0.88066\n",
      "Epoch: 351 Batch:   0 Loss: 142.77628 Accuracy: 0.88169\n",
      "Epoch: 352 Batch:   0 Loss: 142.44052 Accuracy: 0.88272\n",
      "Epoch: 353 Batch:   0 Loss: 142.10567 Accuracy: 0.88477\n",
      "Epoch: 354 Batch:   0 Loss: 141.77156 Accuracy: 0.88477\n",
      "Epoch: 355 Batch:   0 Loss: 141.43826 Accuracy: 0.88477\n",
      "Epoch: 356 Batch:   0 Loss: 141.10570 Accuracy: 0.88477\n",
      "Epoch: 357 Batch:   0 Loss: 140.77380 Accuracy: 0.88477\n",
      "Epoch: 358 Batch:   0 Loss: 140.44289 Accuracy: 0.88477\n",
      "Epoch: 359 Batch:   0 Loss: 140.11275 Accuracy: 0.88374\n",
      "Test Loss: 185.65529 Accuracy: 0.76067\n",
      "Epoch: 360 Batch:   0 Loss: 139.78302 Accuracy: 0.88374\n",
      "Epoch: 361 Batch:   0 Loss: 139.45425 Accuracy: 0.88374\n",
      "Epoch: 362 Batch:   0 Loss: 139.12639 Accuracy: 0.88374\n",
      "Epoch: 363 Batch:   0 Loss: 138.79886 Accuracy: 0.88374\n",
      "Epoch: 364 Batch:   0 Loss: 138.47226 Accuracy: 0.88477\n",
      "Epoch: 365 Batch:   0 Loss: 138.14639 Accuracy: 0.88374\n",
      "Epoch: 366 Batch:   0 Loss: 137.82140 Accuracy: 0.88272\n",
      "Epoch: 367 Batch:   0 Loss: 137.49724 Accuracy: 0.88272\n",
      "Epoch: 368 Batch:   0 Loss: 137.17409 Accuracy: 0.88272\n",
      "Epoch: 369 Batch:   0 Loss: 136.85167 Accuracy: 0.88477\n",
      "Test Loss: 184.36983 Accuracy: 0.76198\n",
      "Epoch: 370 Batch:   0 Loss: 136.52985 Accuracy: 0.88477\n",
      "Epoch: 371 Batch:   0 Loss: 136.20863 Accuracy: 0.88477\n",
      "Epoch: 372 Batch:   0 Loss: 135.88821 Accuracy: 0.88477\n",
      "Epoch: 373 Batch:   0 Loss: 135.56857 Accuracy: 0.88477\n",
      "Epoch: 374 Batch:   0 Loss: 135.24960 Accuracy: 0.88477\n",
      "Epoch: 375 Batch:   0 Loss: 134.93134 Accuracy: 0.88477\n",
      "Epoch: 376 Batch:   0 Loss: 134.61398 Accuracy: 0.88477\n",
      "Epoch: 377 Batch:   0 Loss: 134.29736 Accuracy: 0.88477\n",
      "Epoch: 378 Batch:   0 Loss: 133.98151 Accuracy: 0.88477\n",
      "Epoch: 379 Batch:   0 Loss: 133.66646 Accuracy: 0.88477\n",
      "Test Loss: 183.11567 Accuracy: 0.76224\n",
      "Epoch: 380 Batch:   0 Loss: 133.35190 Accuracy: 0.88477\n",
      "Epoch: 381 Batch:   0 Loss: 133.03801 Accuracy: 0.88477\n",
      "Epoch: 382 Batch:   0 Loss: 132.72484 Accuracy: 0.88477\n",
      "Epoch: 383 Batch:   0 Loss: 132.41231 Accuracy: 0.88477\n",
      "Epoch: 384 Batch:   0 Loss: 132.10075 Accuracy: 0.88477\n",
      "Epoch: 385 Batch:   0 Loss: 131.78987 Accuracy: 0.88477\n",
      "Epoch: 386 Batch:   0 Loss: 131.47978 Accuracy: 0.88477\n",
      "Epoch: 387 Batch:   0 Loss: 131.17029 Accuracy: 0.88477\n",
      "Epoch: 388 Batch:   0 Loss: 130.86153 Accuracy: 0.88580\n",
      "Epoch: 389 Batch:   0 Loss: 130.55344 Accuracy: 0.88580\n",
      "Test Loss: 181.90274 Accuracy: 0.76224\n",
      "Epoch: 390 Batch:   0 Loss: 130.24614 Accuracy: 0.88580\n",
      "Epoch: 391 Batch:   0 Loss: 129.93961 Accuracy: 0.88580\n",
      "Epoch: 392 Batch:   0 Loss: 129.63368 Accuracy: 0.88580\n",
      "Epoch: 393 Batch:   0 Loss: 129.32866 Accuracy: 0.88580\n",
      "Epoch: 394 Batch:   0 Loss: 129.02432 Accuracy: 0.88580\n",
      "Epoch: 395 Batch:   0 Loss: 128.72050 Accuracy: 0.88580\n",
      "Epoch: 396 Batch:   0 Loss: 128.41766 Accuracy: 0.88580\n",
      "Epoch: 397 Batch:   0 Loss: 128.11555 Accuracy: 0.88580\n",
      "Epoch: 398 Batch:   0 Loss: 127.81418 Accuracy: 0.88580\n",
      "Epoch: 399 Batch:   0 Loss: 127.51363 Accuracy: 0.88580\n",
      "Test Loss: 180.72325 Accuracy: 0.76420\n",
      "Epoch: 400 Batch:   0 Loss: 127.21379 Accuracy: 0.88786\n",
      "Epoch: 401 Batch:   0 Loss: 126.91464 Accuracy: 0.88786\n",
      "Epoch: 402 Batch:   0 Loss: 126.61625 Accuracy: 0.88786\n",
      "Epoch: 403 Batch:   0 Loss: 126.31903 Accuracy: 0.88786\n",
      "Epoch: 404 Batch:   0 Loss: 126.02223 Accuracy: 0.88786\n",
      "Epoch: 405 Batch:   0 Loss: 125.72603 Accuracy: 0.88786\n",
      "Epoch: 406 Batch:   0 Loss: 125.43074 Accuracy: 0.88786\n",
      "Epoch: 407 Batch:   0 Loss: 125.13626 Accuracy: 0.88786\n",
      "Epoch: 408 Batch:   0 Loss: 124.84235 Accuracy: 0.88889\n",
      "Epoch: 409 Batch:   0 Loss: 124.54918 Accuracy: 0.88889\n",
      "Test Loss: 179.58144 Accuracy: 0.76328\n",
      "Epoch: 410 Batch:   0 Loss: 124.25688 Accuracy: 0.88889\n",
      "Epoch: 411 Batch:   0 Loss: 123.96533 Accuracy: 0.88889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 412 Batch:   0 Loss: 123.67459 Accuracy: 0.88889\n",
      "Epoch: 413 Batch:   0 Loss: 123.38446 Accuracy: 0.88889\n",
      "Epoch: 414 Batch:   0 Loss: 123.09527 Accuracy: 0.88889\n",
      "Epoch: 415 Batch:   0 Loss: 122.80695 Accuracy: 0.88889\n",
      "Epoch: 416 Batch:   0 Loss: 122.51919 Accuracy: 0.88889\n",
      "Epoch: 417 Batch:   0 Loss: 122.23209 Accuracy: 0.88889\n",
      "Epoch: 418 Batch:   0 Loss: 121.94585 Accuracy: 0.88889\n",
      "Epoch: 419 Batch:   0 Loss: 121.66039 Accuracy: 0.88889\n",
      "Test Loss: 178.46993 Accuracy: 0.76302\n",
      "Epoch: 420 Batch:   0 Loss: 121.37563 Accuracy: 0.88889\n",
      "Epoch: 421 Batch:   0 Loss: 121.09156 Accuracy: 0.88889\n",
      "Epoch: 422 Batch:   0 Loss: 120.80815 Accuracy: 0.88992\n",
      "Epoch: 423 Batch:   0 Loss: 120.52553 Accuracy: 0.88992\n",
      "Epoch: 424 Batch:   0 Loss: 120.24365 Accuracy: 0.89095\n",
      "Epoch: 425 Batch:   0 Loss: 119.96241 Accuracy: 0.89198\n",
      "Epoch: 426 Batch:   0 Loss: 119.68205 Accuracy: 0.89095\n",
      "Epoch: 427 Batch:   0 Loss: 119.40240 Accuracy: 0.89095\n",
      "Epoch: 428 Batch:   0 Loss: 119.12359 Accuracy: 0.89095\n",
      "Epoch: 429 Batch:   0 Loss: 118.84548 Accuracy: 0.89198\n",
      "Test Loss: 177.38703 Accuracy: 0.76354\n",
      "Epoch: 430 Batch:   0 Loss: 118.56814 Accuracy: 0.89300\n",
      "Epoch: 431 Batch:   0 Loss: 118.29155 Accuracy: 0.89300\n",
      "Epoch: 432 Batch:   0 Loss: 118.01575 Accuracy: 0.89506\n",
      "Epoch: 433 Batch:   0 Loss: 117.74068 Accuracy: 0.89506\n",
      "Epoch: 434 Batch:   0 Loss: 117.46639 Accuracy: 0.89506\n",
      "Epoch: 435 Batch:   0 Loss: 117.19287 Accuracy: 0.89506\n",
      "Epoch: 436 Batch:   0 Loss: 116.92003 Accuracy: 0.89506\n",
      "Epoch: 437 Batch:   0 Loss: 116.64801 Accuracy: 0.89506\n",
      "Epoch: 438 Batch:   0 Loss: 116.37681 Accuracy: 0.89506\n",
      "Epoch: 439 Batch:   0 Loss: 116.10641 Accuracy: 0.89506\n",
      "Test Loss: 176.34137 Accuracy: 0.76407\n",
      "Epoch: 440 Batch:   0 Loss: 115.83678 Accuracy: 0.89506\n",
      "Epoch: 441 Batch:   0 Loss: 115.56799 Accuracy: 0.89403\n",
      "Epoch: 442 Batch:   0 Loss: 115.29999 Accuracy: 0.89403\n",
      "Epoch: 443 Batch:   0 Loss: 115.03248 Accuracy: 0.89403\n",
      "Epoch: 444 Batch:   0 Loss: 114.76598 Accuracy: 0.89403\n",
      "Epoch: 445 Batch:   0 Loss: 114.50034 Accuracy: 0.89300\n",
      "Epoch: 446 Batch:   0 Loss: 114.23547 Accuracy: 0.89300\n",
      "Epoch: 447 Batch:   0 Loss: 113.97123 Accuracy: 0.89300\n",
      "Epoch: 448 Batch:   0 Loss: 113.70801 Accuracy: 0.89300\n",
      "Epoch: 449 Batch:   0 Loss: 113.44545 Accuracy: 0.89300\n",
      "Test Loss: 175.34518 Accuracy: 0.76407\n",
      "Epoch: 450 Batch:   0 Loss: 113.18371 Accuracy: 0.89300\n",
      "Epoch: 451 Batch:   0 Loss: 112.92275 Accuracy: 0.89300\n",
      "Epoch: 452 Batch:   0 Loss: 112.66256 Accuracy: 0.89300\n",
      "Epoch: 453 Batch:   0 Loss: 112.40322 Accuracy: 0.89300\n",
      "Epoch: 454 Batch:   0 Loss: 112.14463 Accuracy: 0.89300\n",
      "Epoch: 455 Batch:   0 Loss: 111.88686 Accuracy: 0.89095\n",
      "Epoch: 456 Batch:   0 Loss: 111.63020 Accuracy: 0.89095\n",
      "Epoch: 457 Batch:   0 Loss: 111.37411 Accuracy: 0.89095\n",
      "Epoch: 458 Batch:   0 Loss: 111.11861 Accuracy: 0.89095\n",
      "Epoch: 459 Batch:   0 Loss: 110.86404 Accuracy: 0.89095\n",
      "Test Loss: 174.38531 Accuracy: 0.76459\n",
      "Epoch: 460 Batch:   0 Loss: 110.61034 Accuracy: 0.89095\n",
      "Epoch: 461 Batch:   0 Loss: 110.35745 Accuracy: 0.89095\n",
      "Epoch: 462 Batch:   0 Loss: 110.10539 Accuracy: 0.89095\n",
      "Epoch: 463 Batch:   0 Loss: 109.85419 Accuracy: 0.89095\n",
      "Epoch: 464 Batch:   0 Loss: 109.60371 Accuracy: 0.89095\n",
      "Epoch: 465 Batch:   0 Loss: 109.35416 Accuracy: 0.89095\n",
      "Epoch: 466 Batch:   0 Loss: 109.10548 Accuracy: 0.89095\n",
      "Epoch: 467 Batch:   0 Loss: 108.85765 Accuracy: 0.88992\n",
      "Epoch: 468 Batch:   0 Loss: 108.61061 Accuracy: 0.88992\n",
      "Epoch: 469 Batch:   0 Loss: 108.36456 Accuracy: 0.88992\n",
      "Test Loss: 173.46904 Accuracy: 0.76446\n",
      "Epoch: 470 Batch:   0 Loss: 108.11936 Accuracy: 0.88992\n",
      "Epoch: 471 Batch:   0 Loss: 107.87507 Accuracy: 0.88992\n",
      "Epoch: 472 Batch:   0 Loss: 107.63152 Accuracy: 0.88992\n",
      "Epoch: 473 Batch:   0 Loss: 107.38891 Accuracy: 0.88992\n",
      "Epoch: 474 Batch:   0 Loss: 107.14720 Accuracy: 0.89095\n",
      "Epoch: 475 Batch:   0 Loss: 106.90632 Accuracy: 0.89095\n",
      "Epoch: 476 Batch:   0 Loss: 106.66634 Accuracy: 0.89095\n",
      "Epoch: 477 Batch:   0 Loss: 106.42724 Accuracy: 0.89095\n",
      "Epoch: 478 Batch:   0 Loss: 106.18887 Accuracy: 0.89095\n",
      "Epoch: 479 Batch:   0 Loss: 105.95143 Accuracy: 0.89095\n",
      "Test Loss: 172.60294 Accuracy: 0.76394\n",
      "Epoch: 480 Batch:   0 Loss: 105.71491 Accuracy: 0.89095\n",
      "Epoch: 481 Batch:   0 Loss: 105.47934 Accuracy: 0.89198\n",
      "Epoch: 482 Batch:   0 Loss: 105.24435 Accuracy: 0.89198\n",
      "Epoch: 483 Batch:   0 Loss: 105.01046 Accuracy: 0.89198\n",
      "Epoch: 484 Batch:   0 Loss: 104.77749 Accuracy: 0.89198\n",
      "Epoch: 485 Batch:   0 Loss: 104.54539 Accuracy: 0.89198\n",
      "Epoch: 486 Batch:   0 Loss: 104.31422 Accuracy: 0.89198\n",
      "Epoch: 487 Batch:   0 Loss: 104.08401 Accuracy: 0.89198\n",
      "Epoch: 488 Batch:   0 Loss: 103.85457 Accuracy: 0.89198\n",
      "Epoch: 489 Batch:   0 Loss: 103.62613 Accuracy: 0.89198\n",
      "Test Loss: 171.78836 Accuracy: 0.76498\n",
      "Epoch: 490 Batch:   0 Loss: 103.39851 Accuracy: 0.89300\n",
      "Epoch: 491 Batch:   0 Loss: 103.17184 Accuracy: 0.89300\n",
      "Epoch: 492 Batch:   0 Loss: 102.94610 Accuracy: 0.89300\n",
      "Epoch: 493 Batch:   0 Loss: 102.72131 Accuracy: 0.89300\n",
      "Epoch: 494 Batch:   0 Loss: 102.49763 Accuracy: 0.89300\n",
      "Epoch: 495 Batch:   0 Loss: 102.27464 Accuracy: 0.89300\n",
      "Epoch: 496 Batch:   0 Loss: 102.05272 Accuracy: 0.89300\n",
      "Epoch: 497 Batch:   0 Loss: 101.83168 Accuracy: 0.89300\n",
      "Epoch: 498 Batch:   0 Loss: 101.61163 Accuracy: 0.89300\n",
      "Epoch: 499 Batch:   0 Loss: 101.39247 Accuracy: 0.89300\n",
      "Test Loss: 171.02956 Accuracy: 0.76511\n",
      "Epoch: 500 Batch:   0 Loss: 101.17435 Accuracy: 0.89198\n",
      "Epoch: 501 Batch:   0 Loss: 100.95718 Accuracy: 0.89198\n",
      "Epoch: 502 Batch:   0 Loss: 100.74096 Accuracy: 0.89198\n",
      "Epoch: 503 Batch:   0 Loss: 100.52570 Accuracy: 0.89198\n",
      "Epoch: 504 Batch:   0 Loss: 100.31139 Accuracy: 0.89198\n",
      "Epoch: 505 Batch:   0 Loss: 100.09800 Accuracy: 0.89198\n",
      "Epoch: 506 Batch:   0 Loss: 99.88564 Accuracy: 0.89198\n",
      "Epoch: 507 Batch:   0 Loss: 99.67403 Accuracy: 0.89198\n",
      "Epoch: 508 Batch:   0 Loss: 99.46356 Accuracy: 0.89198\n",
      "Epoch: 509 Batch:   0 Loss: 99.25462 Accuracy: 0.89198\n",
      "Test Loss: 170.33022 Accuracy: 0.76511\n",
      "Epoch: 510 Batch:   0 Loss: 99.04613 Accuracy: 0.89198\n",
      "Epoch: 511 Batch:   0 Loss: 98.83862 Accuracy: 0.89198\n",
      "Epoch: 512 Batch:   0 Loss: 98.63214 Accuracy: 0.89198\n",
      "Epoch: 513 Batch:   0 Loss: 98.42693 Accuracy: 0.89198\n",
      "Epoch: 514 Batch:   0 Loss: 98.22247 Accuracy: 0.89300\n",
      "Epoch: 515 Batch:   0 Loss: 98.01897 Accuracy: 0.89300\n",
      "Epoch: 516 Batch:   0 Loss: 97.81657 Accuracy: 0.89300\n",
      "Epoch: 517 Batch:   0 Loss: 97.61497 Accuracy: 0.89300\n",
      "Epoch: 518 Batch:   0 Loss: 97.41446 Accuracy: 0.89300\n",
      "Epoch: 519 Batch:   0 Loss: 97.21507 Accuracy: 0.89300\n",
      "Test Loss: 169.69774 Accuracy: 0.76485\n",
      "Epoch: 520 Batch:   0 Loss: 97.01640 Accuracy: 0.89300\n",
      "Epoch: 521 Batch:   0 Loss: 96.81881 Accuracy: 0.89300\n",
      "Epoch: 522 Batch:   0 Loss: 96.62223 Accuracy: 0.89300\n",
      "Epoch: 523 Batch:   0 Loss: 96.41873 Accuracy: 0.89300\n",
      "Epoch: 524 Batch:   0 Loss: 96.23486 Accuracy: 0.89403\n",
      "Epoch: 525 Batch:   0 Loss: 96.03774 Accuracy: 0.89403\n",
      "Epoch: 526 Batch:   0 Loss: 95.84538 Accuracy: 0.89506\n",
      "Epoch: 527 Batch:   0 Loss: 95.65445 Accuracy: 0.89506\n",
      "Epoch: 528 Batch:   0 Loss: 95.46329 Accuracy: 0.89506\n",
      "Epoch: 529 Batch:   0 Loss: 95.27410 Accuracy: 0.89506\n",
      "Test Loss: 169.12874 Accuracy: 0.76472\n",
      "Epoch: 530 Batch:   0 Loss: 95.08583 Accuracy: 0.89403\n",
      "Epoch: 531 Batch:   0 Loss: 94.89872 Accuracy: 0.89403\n",
      "Epoch: 532 Batch:   0 Loss: 94.71260 Accuracy: 0.89403\n",
      "Epoch: 533 Batch:   0 Loss: 94.52750 Accuracy: 0.89403\n",
      "Epoch: 534 Batch:   0 Loss: 94.34334 Accuracy: 0.89403\n",
      "Epoch: 535 Batch:   0 Loss: 94.16019 Accuracy: 0.89403\n",
      "Epoch: 536 Batch:   0 Loss: 93.97812 Accuracy: 0.89403\n",
      "Epoch: 537 Batch:   0 Loss: 93.79695 Accuracy: 0.89403\n",
      "Epoch: 538 Batch:   0 Loss: 93.61689 Accuracy: 0.89403\n",
      "Epoch: 539 Batch:   0 Loss: 93.43779 Accuracy: 0.89403\n",
      "Test Loss: 168.62061 Accuracy: 0.76524\n",
      "Epoch: 540 Batch:   0 Loss: 93.25990 Accuracy: 0.89300\n",
      "Epoch: 541 Batch:   0 Loss: 93.08301 Accuracy: 0.89300\n",
      "Epoch: 542 Batch:   0 Loss: 92.90713 Accuracy: 0.89300\n",
      "Epoch: 543 Batch:   0 Loss: 92.73228 Accuracy: 0.89300\n",
      "Epoch: 544 Batch:   0 Loss: 92.55846 Accuracy: 0.89300\n",
      "Epoch: 545 Batch:   0 Loss: 92.38562 Accuracy: 0.89403\n",
      "Epoch: 546 Batch:   0 Loss: 92.21391 Accuracy: 0.89403\n",
      "Epoch: 547 Batch:   0 Loss: 92.04321 Accuracy: 0.89403\n",
      "Epoch: 548 Batch:   0 Loss: 91.87356 Accuracy: 0.89403\n",
      "Epoch: 549 Batch:   0 Loss: 91.70470 Accuracy: 0.89403\n",
      "Test Loss: 168.18333 Accuracy: 0.76563\n",
      "Epoch: 550 Batch:   0 Loss: 91.53718 Accuracy: 0.89403\n",
      "Epoch: 551 Batch:   0 Loss: 91.37059 Accuracy: 0.89403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 552 Batch:   0 Loss: 91.20496 Accuracy: 0.89403\n",
      "Epoch: 553 Batch:   0 Loss: 91.04049 Accuracy: 0.89403\n",
      "Epoch: 554 Batch:   0 Loss: 90.87702 Accuracy: 0.89403\n",
      "Epoch: 555 Batch:   0 Loss: 90.71461 Accuracy: 0.89403\n",
      "Epoch: 556 Batch:   0 Loss: 90.55315 Accuracy: 0.89403\n",
      "Epoch: 557 Batch:   0 Loss: 90.39311 Accuracy: 0.89403\n",
      "Epoch: 558 Batch:   0 Loss: 90.23405 Accuracy: 0.89403\n",
      "Epoch: 559 Batch:   0 Loss: 90.07581 Accuracy: 0.89403\n",
      "Test Loss: 167.80643 Accuracy: 0.76694\n",
      "Epoch: 560 Batch:   0 Loss: 89.91855 Accuracy: 0.89403\n",
      "Epoch: 561 Batch:   0 Loss: 89.76235 Accuracy: 0.89403\n",
      "Epoch: 562 Batch:   0 Loss: 89.60722 Accuracy: 0.89403\n",
      "Epoch: 563 Batch:   0 Loss: 89.45299 Accuracy: 0.89403\n",
      "Epoch: 564 Batch:   0 Loss: 89.29990 Accuracy: 0.89403\n",
      "Epoch: 565 Batch:   0 Loss: 89.14786 Accuracy: 0.89403\n",
      "Epoch: 566 Batch:   0 Loss: 88.99692 Accuracy: 0.89403\n",
      "Epoch: 567 Batch:   0 Loss: 88.84698 Accuracy: 0.89403\n",
      "Epoch: 568 Batch:   0 Loss: 88.69809 Accuracy: 0.89403\n",
      "Epoch: 569 Batch:   0 Loss: 88.55025 Accuracy: 0.89506\n",
      "Test Loss: 167.50573 Accuracy: 0.76786\n",
      "Epoch: 570 Batch:   0 Loss: 88.40346 Accuracy: 0.89506\n",
      "Epoch: 571 Batch:   0 Loss: 88.25764 Accuracy: 0.89506\n",
      "Epoch: 572 Batch:   0 Loss: 88.11289 Accuracy: 0.89506\n",
      "Epoch: 573 Batch:   0 Loss: 87.96916 Accuracy: 0.89506\n",
      "Epoch: 574 Batch:   0 Loss: 87.82645 Accuracy: 0.89506\n",
      "Epoch: 575 Batch:   0 Loss: 87.68477 Accuracy: 0.89506\n",
      "Epoch: 576 Batch:   0 Loss: 87.54406 Accuracy: 0.89506\n",
      "Epoch: 577 Batch:   0 Loss: 87.40445 Accuracy: 0.89506\n",
      "Epoch: 578 Batch:   0 Loss: 87.26579 Accuracy: 0.89609\n",
      "Epoch: 579 Batch:   0 Loss: 87.12817 Accuracy: 0.89609\n",
      "Test Loss: 167.27712 Accuracy: 0.76812\n",
      "Epoch: 580 Batch:   0 Loss: 86.99167 Accuracy: 0.89609\n",
      "Epoch: 581 Batch:   0 Loss: 86.85611 Accuracy: 0.89609\n",
      "Epoch: 582 Batch:   0 Loss: 86.72147 Accuracy: 0.89609\n",
      "Epoch: 583 Batch:   0 Loss: 86.58790 Accuracy: 0.89609\n",
      "Epoch: 584 Batch:   0 Loss: 86.45531 Accuracy: 0.89609\n",
      "Epoch: 585 Batch:   0 Loss: 86.32378 Accuracy: 0.89609\n",
      "Epoch: 586 Batch:   0 Loss: 86.19321 Accuracy: 0.89609\n",
      "Epoch: 587 Batch:   0 Loss: 86.06371 Accuracy: 0.89609\n",
      "Epoch: 588 Batch:   0 Loss: 85.93515 Accuracy: 0.89609\n",
      "Epoch: 589 Batch:   0 Loss: 85.80761 Accuracy: 0.89609\n",
      "Test Loss: 167.11146 Accuracy: 0.76825\n",
      "Epoch: 590 Batch:   0 Loss: 85.68111 Accuracy: 0.89609\n",
      "Epoch: 591 Batch:   0 Loss: 85.55569 Accuracy: 0.89609\n",
      "Epoch: 592 Batch:   0 Loss: 85.42511 Accuracy: 0.89609\n",
      "Epoch: 593 Batch:   0 Loss: 85.29590 Accuracy: 0.89609\n",
      "Epoch: 594 Batch:   0 Loss: 85.17622 Accuracy: 0.89609\n",
      "Epoch: 595 Batch:   0 Loss: 85.05317 Accuracy: 0.89609\n",
      "Epoch: 596 Batch:   0 Loss: 84.93324 Accuracy: 0.89609\n",
      "Epoch: 597 Batch:   0 Loss: 84.81329 Accuracy: 0.89609\n",
      "Epoch: 598 Batch:   0 Loss: 84.69479 Accuracy: 0.89609\n",
      "Epoch: 599 Batch:   0 Loss: 84.58721 Accuracy: 0.89609\n",
      "Test Loss: 167.03391 Accuracy: 0.76825\n",
      "Epoch: 600 Batch:   0 Loss: 84.45306 Accuracy: 0.89609\n",
      "Epoch: 601 Batch:   0 Loss: 84.34361 Accuracy: 0.89609\n",
      "Epoch: 602 Batch:   0 Loss: 84.22761 Accuracy: 0.89609\n",
      "Epoch: 603 Batch:   0 Loss: 84.11331 Accuracy: 0.89609\n",
      "Epoch: 604 Batch:   0 Loss: 84.00072 Accuracy: 0.89506\n",
      "Epoch: 605 Batch:   0 Loss: 83.88835 Accuracy: 0.89506\n",
      "Epoch: 606 Batch:   0 Loss: 83.77753 Accuracy: 0.89506\n",
      "Epoch: 607 Batch:   0 Loss: 83.66712 Accuracy: 0.89506\n",
      "Epoch: 608 Batch:   0 Loss: 83.55750 Accuracy: 0.89506\n",
      "Epoch: 609 Batch:   0 Loss: 83.44898 Accuracy: 0.89506\n",
      "Test Loss: 166.94846 Accuracy: 0.76812\n",
      "Epoch: 610 Batch:   0 Loss: 83.34126 Accuracy: 0.89506\n",
      "Epoch: 611 Batch:   0 Loss: 83.23460 Accuracy: 0.89506\n",
      "Epoch: 612 Batch:   0 Loss: 83.13875 Accuracy: 0.89506\n",
      "Epoch: 613 Batch:   0 Loss: 83.02004 Accuracy: 0.89506\n",
      "Epoch: 614 Batch:   0 Loss: 82.92163 Accuracy: 0.89506\n",
      "Epoch: 615 Batch:   0 Loss: 82.81636 Accuracy: 0.89506\n",
      "Epoch: 616 Batch:   0 Loss: 82.71500 Accuracy: 0.89506\n",
      "Epoch: 617 Batch:   0 Loss: 82.61335 Accuracy: 0.89506\n",
      "Epoch: 618 Batch:   0 Loss: 82.51321 Accuracy: 0.89506\n",
      "Epoch: 619 Batch:   0 Loss: 82.41362 Accuracy: 0.89506\n",
      "Test Loss: 166.92355 Accuracy: 0.76838\n",
      "Epoch: 620 Batch:   0 Loss: 82.31517 Accuracy: 0.89506\n",
      "Epoch: 621 Batch:   0 Loss: 82.21738 Accuracy: 0.89506\n",
      "Epoch: 622 Batch:   0 Loss: 82.12067 Accuracy: 0.89506\n",
      "Epoch: 623 Batch:   0 Loss: 82.02480 Accuracy: 0.89506\n",
      "Epoch: 624 Batch:   0 Loss: 81.92973 Accuracy: 0.89506\n",
      "Epoch: 625 Batch:   0 Loss: 81.83549 Accuracy: 0.89506\n",
      "Epoch: 626 Batch:   0 Loss: 81.74231 Accuracy: 0.89506\n",
      "Epoch: 627 Batch:   0 Loss: 81.64981 Accuracy: 0.89506\n",
      "Epoch: 628 Batch:   0 Loss: 81.55816 Accuracy: 0.89506\n",
      "Epoch: 629 Batch:   0 Loss: 81.47540 Accuracy: 0.89506\n",
      "Test Loss: 166.95243 Accuracy: 0.76812\n",
      "Epoch: 630 Batch:   0 Loss: 81.37392 Accuracy: 0.89506\n",
      "Epoch: 631 Batch:   0 Loss: 81.28776 Accuracy: 0.89506\n",
      "Epoch: 632 Batch:   0 Loss: 81.19943 Accuracy: 0.89506\n",
      "Epoch: 633 Batch:   0 Loss: 81.11103 Accuracy: 0.89609\n",
      "Epoch: 634 Batch:   0 Loss: 81.02470 Accuracy: 0.89609\n",
      "Epoch: 635 Batch:   0 Loss: 80.93858 Accuracy: 0.89712\n",
      "Epoch: 636 Batch:   0 Loss: 80.85330 Accuracy: 0.89712\n",
      "Epoch: 637 Batch:   0 Loss: 80.76896 Accuracy: 0.89712\n",
      "Epoch: 638 Batch:   0 Loss: 80.68532 Accuracy: 0.89712\n",
      "Epoch: 639 Batch:   0 Loss: 80.60260 Accuracy: 0.89712\n",
      "Test Loss: 166.99430 Accuracy: 0.76812\n",
      "Epoch: 640 Batch:   0 Loss: 80.52073 Accuracy: 0.89712\n",
      "Epoch: 641 Batch:   0 Loss: 80.43958 Accuracy: 0.89712\n",
      "Epoch: 642 Batch:   0 Loss: 80.35929 Accuracy: 0.89918\n",
      "Epoch: 643 Batch:   0 Loss: 80.27970 Accuracy: 0.89918\n",
      "Epoch: 644 Batch:   0 Loss: 80.20094 Accuracy: 0.89918\n",
      "Epoch: 645 Batch:   0 Loss: 80.12278 Accuracy: 0.89918\n",
      "Epoch: 646 Batch:   0 Loss: 80.04547 Accuracy: 0.89918\n",
      "Epoch: 647 Batch:   0 Loss: 79.96898 Accuracy: 0.89918\n",
      "Epoch: 648 Batch:   0 Loss: 79.89320 Accuracy: 0.89918\n",
      "Epoch: 649 Batch:   0 Loss: 79.81818 Accuracy: 0.89918\n",
      "Test Loss: 167.09101 Accuracy: 0.76707\n",
      "Epoch: 650 Batch:   0 Loss: 79.74394 Accuracy: 0.89918\n",
      "Epoch: 651 Batch:   0 Loss: 79.67035 Accuracy: 0.89918\n",
      "Epoch: 652 Batch:   0 Loss: 79.59748 Accuracy: 0.89918\n",
      "Epoch: 653 Batch:   0 Loss: 79.52522 Accuracy: 0.89918\n",
      "Epoch: 654 Batch:   0 Loss: 79.45413 Accuracy: 0.89918\n",
      "Epoch: 655 Batch:   0 Loss: 79.38348 Accuracy: 0.89918\n",
      "Epoch: 656 Batch:   0 Loss: 79.31363 Accuracy: 0.89918\n",
      "Epoch: 657 Batch:   0 Loss: 79.24447 Accuracy: 0.89918\n",
      "Epoch: 658 Batch:   0 Loss: 79.17604 Accuracy: 0.89918\n",
      "Epoch: 659 Batch:   0 Loss: 79.10826 Accuracy: 0.89918\n",
      "Test Loss: 167.22826 Accuracy: 0.76707\n",
      "Epoch: 660 Batch:   0 Loss: 79.04108 Accuracy: 0.89918\n",
      "Epoch: 661 Batch:   0 Loss: 78.97467 Accuracy: 0.89918\n",
      "Epoch: 662 Batch:   0 Loss: 78.90885 Accuracy: 0.89918\n",
      "Epoch: 663 Batch:   0 Loss: 78.84377 Accuracy: 0.89918\n",
      "Epoch: 664 Batch:   0 Loss: 78.77934 Accuracy: 0.89918\n",
      "Epoch: 665 Batch:   0 Loss: 78.71557 Accuracy: 0.89918\n",
      "Epoch: 666 Batch:   0 Loss: 78.65244 Accuracy: 0.89918\n",
      "Epoch: 667 Batch:   0 Loss: 78.59009 Accuracy: 0.89918\n",
      "Epoch: 668 Batch:   0 Loss: 78.52834 Accuracy: 0.89918\n",
      "Epoch: 669 Batch:   0 Loss: 78.46717 Accuracy: 0.89918\n",
      "Test Loss: 167.39936 Accuracy: 0.76773\n",
      "Epoch: 670 Batch:   0 Loss: 78.40664 Accuracy: 0.89918\n",
      "Epoch: 671 Batch:   0 Loss: 78.34672 Accuracy: 0.89918\n",
      "Epoch: 672 Batch:   0 Loss: 78.28728 Accuracy: 0.89918\n",
      "Epoch: 673 Batch:   0 Loss: 78.22855 Accuracy: 0.89918\n",
      "Epoch: 674 Batch:   0 Loss: 78.17042 Accuracy: 0.89918\n",
      "Epoch: 675 Batch:   0 Loss: 78.11295 Accuracy: 0.89918\n",
      "Epoch: 676 Batch:   0 Loss: 78.05611 Accuracy: 0.89918\n",
      "Epoch: 677 Batch:   0 Loss: 77.99979 Accuracy: 0.89815\n",
      "Epoch: 678 Batch:   0 Loss: 77.94418 Accuracy: 0.89815\n",
      "Epoch: 679 Batch:   0 Loss: 77.88906 Accuracy: 0.89815\n",
      "Test Loss: 167.60279 Accuracy: 0.76786\n",
      "Epoch: 680 Batch:   0 Loss: 77.83459 Accuracy: 0.89815\n",
      "Epoch: 681 Batch:   0 Loss: 77.78061 Accuracy: 0.89815\n",
      "Epoch: 682 Batch:   0 Loss: 77.72720 Accuracy: 0.89815\n",
      "Epoch: 683 Batch:   0 Loss: 77.67438 Accuracy: 0.89815\n",
      "Epoch: 684 Batch:   0 Loss: 77.62208 Accuracy: 0.89815\n",
      "Epoch: 685 Batch:   0 Loss: 77.57031 Accuracy: 0.89815\n",
      "Epoch: 686 Batch:   0 Loss: 77.51907 Accuracy: 0.89815\n",
      "Epoch: 687 Batch:   0 Loss: 77.46838 Accuracy: 0.89815\n",
      "Epoch: 688 Batch:   0 Loss: 77.41816 Accuracy: 0.89815\n",
      "Epoch: 689 Batch:   0 Loss: 77.36849 Accuracy: 0.89815\n",
      "Test Loss: 167.83402 Accuracy: 0.76760\n",
      "Epoch: 690 Batch:   0 Loss: 77.31934 Accuracy: 0.89815\n",
      "Epoch: 691 Batch:   0 Loss: 77.27072 Accuracy: 0.89815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 692 Batch:   0 Loss: 77.22261 Accuracy: 0.89815\n",
      "Epoch: 693 Batch:   0 Loss: 77.17500 Accuracy: 0.89815\n",
      "Epoch: 694 Batch:   0 Loss: 77.12791 Accuracy: 0.89918\n",
      "Epoch: 695 Batch:   0 Loss: 77.08129 Accuracy: 0.89918\n",
      "Epoch: 696 Batch:   0 Loss: 77.03519 Accuracy: 0.89918\n",
      "Epoch: 697 Batch:   0 Loss: 76.98958 Accuracy: 0.89918\n",
      "Epoch: 698 Batch:   0 Loss: 76.94447 Accuracy: 0.89918\n",
      "Epoch: 699 Batch:   0 Loss: 76.89973 Accuracy: 0.89918\n",
      "Test Loss: 168.09354 Accuracy: 0.76681\n",
      "Epoch: 700 Batch:   0 Loss: 76.85545 Accuracy: 0.89918\n",
      "Epoch: 701 Batch:   0 Loss: 76.81166 Accuracy: 0.89918\n",
      "Epoch: 702 Batch:   0 Loss: 76.76834 Accuracy: 0.90021\n",
      "Epoch: 703 Batch:   0 Loss: 76.72546 Accuracy: 0.90021\n",
      "Epoch: 704 Batch:   0 Loss: 76.68304 Accuracy: 0.90021\n",
      "Epoch: 705 Batch:   0 Loss: 76.64101 Accuracy: 0.90021\n",
      "Epoch: 706 Batch:   0 Loss: 76.59948 Accuracy: 0.90123\n",
      "Epoch: 707 Batch:   0 Loss: 76.55843 Accuracy: 0.90123\n",
      "Epoch: 708 Batch:   0 Loss: 76.51775 Accuracy: 0.90123\n",
      "Epoch: 709 Batch:   0 Loss: 76.47749 Accuracy: 0.90123\n",
      "Test Loss: 168.37651 Accuracy: 0.76577\n",
      "Epoch: 710 Batch:   0 Loss: 76.43760 Accuracy: 0.90123\n",
      "Epoch: 711 Batch:   0 Loss: 76.39812 Accuracy: 0.90123\n",
      "Epoch: 712 Batch:   0 Loss: 76.35902 Accuracy: 0.90123\n",
      "Epoch: 713 Batch:   0 Loss: 76.32036 Accuracy: 0.90021\n",
      "Epoch: 714 Batch:   0 Loss: 76.28199 Accuracy: 0.90021\n",
      "Epoch: 715 Batch:   0 Loss: 76.24404 Accuracy: 0.90021\n",
      "Epoch: 716 Batch:   0 Loss: 76.20659 Accuracy: 0.90021\n",
      "Epoch: 717 Batch:   0 Loss: 76.16931 Accuracy: 0.90021\n",
      "Epoch: 718 Batch:   0 Loss: 76.13245 Accuracy: 0.90021\n",
      "Epoch: 719 Batch:   0 Loss: 76.09599 Accuracy: 0.90021\n",
      "Test Loss: 168.67809 Accuracy: 0.76551\n",
      "Epoch: 720 Batch:   0 Loss: 76.05987 Accuracy: 0.90021\n",
      "Epoch: 721 Batch:   0 Loss: 76.02420 Accuracy: 0.90123\n",
      "Epoch: 722 Batch:   0 Loss: 75.98882 Accuracy: 0.90123\n",
      "Epoch: 723 Batch:   0 Loss: 75.95386 Accuracy: 0.90123\n",
      "Epoch: 724 Batch:   0 Loss: 75.91924 Accuracy: 0.90123\n",
      "Epoch: 725 Batch:   0 Loss: 75.88506 Accuracy: 0.90123\n",
      "Epoch: 726 Batch:   0 Loss: 75.85111 Accuracy: 0.90123\n",
      "Epoch: 727 Batch:   0 Loss: 75.81750 Accuracy: 0.90123\n",
      "Epoch: 728 Batch:   0 Loss: 75.78434 Accuracy: 0.90123\n",
      "Epoch: 729 Batch:   0 Loss: 75.75174 Accuracy: 0.90021\n",
      "Test Loss: 168.99823 Accuracy: 0.76538\n",
      "Epoch: 730 Batch:   0 Loss: 75.71943 Accuracy: 0.90021\n",
      "Epoch: 731 Batch:   0 Loss: 75.68745 Accuracy: 0.90021\n",
      "Epoch: 732 Batch:   0 Loss: 75.65532 Accuracy: 0.90021\n",
      "Epoch: 733 Batch:   0 Loss: 75.62379 Accuracy: 0.90021\n",
      "Epoch: 734 Batch:   0 Loss: 75.59238 Accuracy: 0.90021\n",
      "Epoch: 735 Batch:   0 Loss: 75.56124 Accuracy: 0.90021\n",
      "Epoch: 736 Batch:   0 Loss: 75.53042 Accuracy: 0.90021\n",
      "Epoch: 737 Batch:   0 Loss: 75.49992 Accuracy: 0.90021\n",
      "Epoch: 738 Batch:   0 Loss: 75.46971 Accuracy: 0.90021\n",
      "Epoch: 739 Batch:   0 Loss: 75.43976 Accuracy: 0.90123\n",
      "Test Loss: 169.32568 Accuracy: 0.76642\n",
      "Epoch: 740 Batch:   0 Loss: 75.41012 Accuracy: 0.90123\n",
      "Epoch: 741 Batch:   0 Loss: 75.38070 Accuracy: 0.90123\n",
      "Epoch: 742 Batch:   0 Loss: 75.35160 Accuracy: 0.90123\n",
      "Epoch: 743 Batch:   0 Loss: 75.32278 Accuracy: 0.90123\n",
      "Epoch: 744 Batch:   0 Loss: 75.29426 Accuracy: 0.90123\n",
      "Epoch: 745 Batch:   0 Loss: 75.26597 Accuracy: 0.90123\n",
      "Epoch: 746 Batch:   0 Loss: 75.23788 Accuracy: 0.90123\n",
      "Epoch: 747 Batch:   0 Loss: 75.21006 Accuracy: 0.90123\n",
      "Epoch: 748 Batch:   0 Loss: 75.18243 Accuracy: 0.90123\n",
      "Epoch: 749 Batch:   0 Loss: 75.15514 Accuracy: 0.90226\n",
      "Test Loss: 169.66174 Accuracy: 0.76642\n",
      "Epoch: 750 Batch:   0 Loss: 75.12820 Accuracy: 0.90226\n",
      "Epoch: 751 Batch:   0 Loss: 75.10135 Accuracy: 0.90226\n",
      "Epoch: 752 Batch:   0 Loss: 75.07476 Accuracy: 0.90226\n",
      "Epoch: 753 Batch:   0 Loss: 75.04850 Accuracy: 0.90226\n",
      "Epoch: 754 Batch:   0 Loss: 75.02232 Accuracy: 0.90226\n",
      "Epoch: 755 Batch:   0 Loss: 74.99653 Accuracy: 0.90226\n",
      "Epoch: 756 Batch:   0 Loss: 74.97090 Accuracy: 0.90226\n",
      "Epoch: 757 Batch:   0 Loss: 74.94547 Accuracy: 0.90226\n",
      "Epoch: 758 Batch:   0 Loss: 74.92029 Accuracy: 0.90226\n",
      "Epoch: 759 Batch:   0 Loss: 74.89539 Accuracy: 0.90226\n",
      "Test Loss: 170.00878 Accuracy: 0.76695\n",
      "Epoch: 760 Batch:   0 Loss: 74.87067 Accuracy: 0.90226\n",
      "Epoch: 761 Batch:   0 Loss: 74.84619 Accuracy: 0.90226\n",
      "Epoch: 762 Batch:   0 Loss: 74.82193 Accuracy: 0.90226\n",
      "Epoch: 763 Batch:   0 Loss: 74.79798 Accuracy: 0.90226\n",
      "Epoch: 764 Batch:   0 Loss: 74.77411 Accuracy: 0.90226\n",
      "Epoch: 765 Batch:   0 Loss: 74.75047 Accuracy: 0.90226\n",
      "Epoch: 766 Batch:   0 Loss: 74.72701 Accuracy: 0.90226\n",
      "Epoch: 767 Batch:   0 Loss: 74.70374 Accuracy: 0.90226\n",
      "Epoch: 768 Batch:   0 Loss: 74.68069 Accuracy: 0.90226\n",
      "Epoch: 769 Batch:   0 Loss: 74.65783 Accuracy: 0.90226\n",
      "Test Loss: 170.35686 Accuracy: 0.76590\n",
      "Epoch: 770 Batch:   0 Loss: 74.63516 Accuracy: 0.90226\n",
      "Epoch: 771 Batch:   0 Loss: 74.61252 Accuracy: 0.90226\n",
      "Epoch: 772 Batch:   0 Loss: 74.59020 Accuracy: 0.90226\n",
      "Epoch: 773 Batch:   0 Loss: 74.56809 Accuracy: 0.90329\n",
      "Epoch: 774 Batch:   0 Loss: 74.54612 Accuracy: 0.90329\n",
      "Epoch: 775 Batch:   0 Loss: 74.52435 Accuracy: 0.90329\n",
      "Epoch: 776 Batch:   0 Loss: 74.50273 Accuracy: 0.90329\n",
      "Epoch: 777 Batch:   0 Loss: 74.48131 Accuracy: 0.90329\n",
      "Epoch: 778 Batch:   0 Loss: 74.45998 Accuracy: 0.90329\n",
      "Epoch: 779 Batch:   0 Loss: 74.43885 Accuracy: 0.90432\n",
      "Test Loss: 170.71009 Accuracy: 0.76603\n",
      "Epoch: 780 Batch:   0 Loss: 74.41783 Accuracy: 0.90432\n",
      "Epoch: 781 Batch:   0 Loss: 74.39708 Accuracy: 0.90432\n",
      "Epoch: 782 Batch:   0 Loss: 74.37647 Accuracy: 0.90432\n",
      "Epoch: 783 Batch:   0 Loss: 74.35598 Accuracy: 0.90432\n",
      "Epoch: 784 Batch:   0 Loss: 74.33549 Accuracy: 0.90432\n",
      "Epoch: 785 Batch:   0 Loss: 74.31516 Accuracy: 0.90432\n",
      "Epoch: 786 Batch:   0 Loss: 74.29500 Accuracy: 0.90432\n",
      "Epoch: 787 Batch:   0 Loss: 74.27505 Accuracy: 0.90432\n",
      "Epoch: 788 Batch:   0 Loss: 74.25516 Accuracy: 0.90432\n",
      "Epoch: 789 Batch:   0 Loss: 74.23543 Accuracy: 0.90432\n",
      "Test Loss: 171.07195 Accuracy: 0.76669\n",
      "Epoch: 790 Batch:   0 Loss: 74.21589 Accuracy: 0.90432\n",
      "Epoch: 791 Batch:   0 Loss: 74.19646 Accuracy: 0.90432\n",
      "Epoch: 792 Batch:   0 Loss: 74.17716 Accuracy: 0.90432\n",
      "Epoch: 793 Batch:   0 Loss: 74.15810 Accuracy: 0.90432\n",
      "Epoch: 794 Batch:   0 Loss: 74.13905 Accuracy: 0.90432\n",
      "Epoch: 795 Batch:   0 Loss: 74.12017 Accuracy: 0.90432\n",
      "Epoch: 796 Batch:   0 Loss: 74.10139 Accuracy: 0.90432\n",
      "Epoch: 797 Batch:   0 Loss: 74.08278 Accuracy: 0.90432\n",
      "Epoch: 798 Batch:   0 Loss: 74.06425 Accuracy: 0.90432\n",
      "Epoch: 799 Batch:   0 Loss: 74.04586 Accuracy: 0.90432\n",
      "Test Loss: 171.43669 Accuracy: 0.76695\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup input and train protoNN\n",
    "# {'REG_W': 2.8881784502872485e-06,\n",
    "#  'REG_B': 0.0035302454747415833,\n",
    "#  'REG_Z': 3.3834584511295385e-05,\n",
    "#  'SPAR_W': 0.7925887710548198,\n",
    "#  'SPAR_B': 0.539183122073297,\n",
    "#  'SPAR_Z': 0.6111926798340185,\n",
    "#  'loss': 'l2',\n",
    "#  'LEARNING_RATE': 0.0008050041923932209,\n",
    "#  'NUM_EPOCHS': 460}\n",
    "X = tf.placeholder(tf.float32, [None, dataDimension], name='X')\n",
    "Y = tf.placeholder(tf.float32, [None, numClasses], name='Y')\n",
    "protoNN = ProtoNN(dataDimension, PROJECTION_DIM,\n",
    "                  NUM_PROTOTYPES, numClasses,\n",
    "                  gamma, W=W, B=B)\n",
    "trainer = ProtoNNTrainer(protoNN,  REG_W, REG_B, REG_Z,\n",
    "                         SPAR_W, SPAR_B, SPAR_Z,\n",
    "                        LEARNING_RATE, X, Y, lossType='l2')\n",
    "sess = tf.Session()\n",
    "\n",
    "trainer.train(2048, 800, sess, x_train, x_test, y_train, y_test,\n",
    "              printStep=600, valStep=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-uypXmz1QJ2h"
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:07:22.671507Z",
     "start_time": "2018-08-15T13:07:22.645050Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 96103,
     "status": "ok",
     "timestamp": 1567154584845,
     "user": {
      "displayName": "Gokul Hari",
      "photoUrl": "",
      "userId": "16159457985484250305"
     },
     "user_tz": -330
    },
    "id": "VYxefAD3QJ2j",
    "outputId": "f8b711a1-bae4-4bf5-9a62-935838844601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy 0.76691926\n",
      "Model size constraint (Bytes):  9580\n",
      "Number of non-zeros:  2395\n"
     ]
    }
   ],
   "source": [
    "acc = sess.run(protoNN.accuracy, feed_dict={X: x_test, Y: y_test})\n",
    "pred = sess.run(protoNN.predictions, feed_dict={X: x_test, Y: y_test})\n",
    "# W, B, Z are tensorflow graph nodes\n",
    "W, B, Z, _ = protoNN.getModelMatrices()\n",
    "matrixList = sess.run([W, B, Z])\n",
    "sparcityList = [SPAR_W, SPAR_B, SPAR_Z]                       \n",
    "nnz, size, sparse = getModelSize(matrixList, sparcityList)\n",
    "print(\"Final test accuracy\", acc)\n",
    "print(\"Model size constraint (Bytes): \", size)\n",
    "print(\"Number of non-zeros: \", nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 91542,
     "status": "ok",
     "timestamp": 1567154584848,
     "user": {
      "displayName": "Gokul Hari",
      "photoUrl": "",
      "userId": "16159457985484250305"
     },
     "user_tz": -330
    },
    "id": "yJM9puhcQJ2t",
    "outputId": "d8e743e0-1068-4681-e3bc-3b650ab66a3c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2521 1306]\n",
      " [ 478 3349]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.84061   0.65874   0.73865      3827\n",
      "           1    0.71944   0.87510   0.78967      3827\n",
      "\n",
      "    accuracy                        0.76692      7654\n",
      "   macro avg    0.78003   0.76692   0.76416      7654\n",
      "weighted avg    0.78003   0.76692   0.76416      7654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "y_test = np.argmax(y_test,axis=1)\n",
    "print (confusion_matrix(y_test,pred))\n",
    "print (classification_report(y_test,pred,digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8750979879801412"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivity = confusion_matrix(y_test,pred)[1][1]/(confusion_matrix(y_test,pred)[1][1] + confusion_matrix(y_test,pred)[1][0])\n",
    "sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6587405278285864"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specificity = confusion_matrix(y_test,pred)[0][0]/(confusion_matrix(y_test,pred)[0][0] + confusion_matrix(y_test,pred)[0][1])\n",
    "specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WINDOW 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT license.\n",
    "\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#sys.path.insert(0, '../../')\n",
    "# from edgeml.trainer.protoNNTrainer import ProtoNNTrainer\n",
    "# from edgeml.graph.protoNN import ProtoNN\n",
    "# import edgeml.utils as utils\n",
    "# import helpermethods as helper\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "sys.path.append(r\"E:\\programming\\practice\\research\\optimized code\\EdgeML\\examples\\tf\\ProtoNN\")\n",
    "import helpermethods as helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper methods\n",
    "sys.path.insert(0, '../')\n",
    "import argparse\n",
    "\n",
    "\n",
    "def getModelSize(matrixList, sparcityList, expected=True, bytesPerVar=4):\n",
    "    '''\n",
    "    expected: Expected size according to the parameters set. The number of\n",
    "        zeros could actually be more than that is required to satisfy the\n",
    "        sparsity constraint.\n",
    "    '''\n",
    "    nnzList, sizeList, isSparseList = [], [], []\n",
    "    hasSparse = False\n",
    "    for i in range(len(matrixList)):\n",
    "        A, s = matrixList[i], sparcityList[i]\n",
    "        assert A.ndim == 2\n",
    "        assert s >= 0\n",
    "        assert s <= 1\n",
    "        nnz, size, sparse = countnnZ(A, s, bytesPerVar=bytesPerVar)\n",
    "        nnzList.append(nnz)\n",
    "        sizeList.append(size)\n",
    "        hasSparse = (hasSparse or sparse)\n",
    "\n",
    "    totalnnZ = np.sum(nnzList)\n",
    "    totalSize = np.sum(sizeList)\n",
    "    if expected:\n",
    "        return totalnnZ, totalSize, hasSparse\n",
    "    numNonZero = 0\n",
    "    totalSize = 0\n",
    "    hasSparse = False\n",
    "    for i in range(len(matrixList)):\n",
    "        A, s = matrixList[i], sparcityList[i]\n",
    "        numNonZero_ = np.count_nonzero(A)\n",
    "        numNonZero += numNonZero_\n",
    "        hasSparse = (hasSparse or (s < 0.5))\n",
    "        if s <= 0.5:\n",
    "            totalSize += numNonZero_ * 2 * bytesPerVar\n",
    "        else:\n",
    "            totalSize += A.size * bytesPerVar\n",
    "    return numNonZero, totalSize, hasSparse\n",
    "\n",
    "\n",
    "def getGamma(gammaInit, projectionDim, dataDim, numPrototypes, x_train):\n",
    "    if gammaInit is None:\n",
    "        print(\"Using median heuristic to estimate gamma.\")\n",
    "        gamma, W, B = medianHeuristic(x_train, projectionDim,\n",
    "                                            numPrototypes)\n",
    "        print(\"Gamma estimate is: %f\" % gamma)\n",
    "        return W, B, gamma\n",
    "    return None, None, gammaInit\n",
    "\n",
    "\n",
    "def preprocessData(dataDir,w):\n",
    "    '''\n",
    "    Loads data from the dataDir and does some initial preprocessing\n",
    "    steps. Data is assumed to be contained in two files,\n",
    "    train.npy and test.npy. Each containing a 2D numpy array of dimension\n",
    "    [numberOfExamples, numberOfFeatures + 1]. The first column of each\n",
    "    matrix is assumed to contain label information.\n",
    "\n",
    "    For an N-Class problem, we assume the labels are integers from 0 through\n",
    "    N-1.\n",
    "    '''\n",
    "    # Uncomment for usual training data\n",
    "    # train = np.load(dataDir + '/train_'+str(w)+'.npy')\n",
    "    # test = np.load(dataDir + '/test_'+str(w)+'.npy')\n",
    "    # Uncomment for time domain training data\n",
    "    train = np.load(dataDir + '/ttrain_'+str(w)+'.npy')\n",
    "    test = np.load(dataDir + '/ttest_'+str(w)+'.npy')\n",
    "    # Uncomment for 1 sensordrop training data\n",
    "    # train = np.load(dataDir + '/train_'+str(w)+'.npy')\n",
    "    # test = np.load(dataDir + '/test_'+str(w)+'.npy')\n",
    "\n",
    "    dataDimension = int(train.shape[1]) - 1\n",
    "    x_train = train[:, 1:dataDimension + 1]\n",
    "    y_train_ = train[:, 0]\n",
    "    x_test = test[:, 1:dataDimension + 1]\n",
    "    y_test_ = test[:, 0]\n",
    "\n",
    "    numClasses = max(y_train_) - min(y_train_) + 1\n",
    "    numClasses = max(numClasses, max(y_test_) - min(y_test_) + 1)\n",
    "    numClasses = int(numClasses)\n",
    "\n",
    "    # mean-var\n",
    "    mean = np.mean(x_train, 0)\n",
    "    std = np.std(x_train, 0)\n",
    "    std[std[:] < 0.000001] = 1\n",
    "    x_train = (x_train - mean) / std\n",
    "    x_test = (x_test - mean) / std\n",
    "\n",
    "    # one hot y-train\n",
    "    lab = y_train_.astype('uint8')\n",
    "    lab = np.array(lab) - min(lab)\n",
    "    lab_ = np.zeros((x_train.shape[0], numClasses))\n",
    "    lab_[np.arange(x_train.shape[0]), lab] = 1\n",
    "    y_train = lab_\n",
    "\n",
    "    # one hot y-test\n",
    "    lab = y_test_.astype('uint8')\n",
    "    lab = np.array(lab) - min(lab)\n",
    "    lab_ = np.zeros((x_test.shape[0], numClasses))\n",
    "    lab_[np.arange(x_test.shape[0]), lab] = 1\n",
    "    y_test = lab_\n",
    "\n",
    "    return dataDimension, numClasses, x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "def getProtoNNArgs():\n",
    "    def checkIntPos(value):\n",
    "        ivalue = int(value)\n",
    "        if ivalue <= 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid positive int value\" % value)\n",
    "        return ivalue\n",
    "\n",
    "    def checkIntNneg(value):\n",
    "        ivalue = int(value)\n",
    "        if ivalue < 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid non-neg int value\" % value)\n",
    "        return ivalue\n",
    "\n",
    "    def checkFloatNneg(value):\n",
    "        fvalue = float(value)\n",
    "        if fvalue < 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid non-neg float value\" % value)\n",
    "        return fvalue\n",
    "\n",
    "    def checkFloatPos(value):\n",
    "        fvalue = float(value)\n",
    "        if fvalue <= 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid positive float value\" % value)\n",
    "        return fvalue\n",
    "\n",
    "    '''\n",
    "    Parse protoNN commandline arguments\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Hyperparameters for ProtoNN Algorithm')\n",
    "\n",
    "    msg = 'Data directory containing train and test data. The '\n",
    "    msg += 'data is assumed to be saved as 2-D numpy matrices with '\n",
    "    msg += 'names `train.npy` and `test.npy`, of dimensions\\n'\n",
    "    msg += '\\t[numberOfInstances, numberOfFeatures + 1].\\n'\n",
    "    msg += 'The first column of each file is assumed to contain label information.'\n",
    "    msg += ' For a N-class problem, labels are assumed to be integers from 0 to'\n",
    "    msg += ' N-1 (inclusive).'\n",
    "    parser.add_argument('-d', '--data-dir', required=True, help=msg)\n",
    "    parser.add_argument('-l', '--projection-dim', type=checkIntPos, default=10,\n",
    "                        help='Projection Dimension.')\n",
    "    parser.add_argument('-p', '--num-prototypes', type=checkIntPos, default=20,\n",
    "                        help='Number of prototypes.')\n",
    "    parser.add_argument('-g', '--gamma', type=checkFloatPos, default=None,\n",
    "                        help='Gamma for Gaussian kernel. If not provided, ' +\n",
    "                        'median heuristic will be used to estimate gamma.')\n",
    "\n",
    "    parser.add_argument('-e', '--epochs', type=checkIntPos, default=100,\n",
    "                        help='Total training epochs.')\n",
    "    parser.add_argument('-b', '--batch-size', type=checkIntPos, default=32,\n",
    "                        help='Batch size for each pass.')\n",
    "    parser.add_argument('-r', '--learning-rate', type=checkFloatPos,\n",
    "                        default=0.001,\n",
    "                        help='Initial Learning rate for ADAM Optimizer.')\n",
    "\n",
    "    parser.add_argument('-rW', type=float, default=0.000,\n",
    "                        help='Coefficient for l2 regularizer for predictor' +\n",
    "                        ' parameter W ' + '(default = 0.0).')\n",
    "    parser.add_argument('-rB', type=float, default=0.00,\n",
    "                        help='Coefficient for l2 regularizer for predictor' +\n",
    "                        ' parameter B ' + '(default = 0.0).')\n",
    "    parser.add_argument('-rZ', type=float, default=0.00,\n",
    "                        help='Coefficient for l2 regularizer for predictor' +\n",
    "                        'parameter Z ' +\n",
    "                        '(default = 0.0).')\n",
    "\n",
    "    parser.add_argument('-sW', type=float, default=1.000,\n",
    "                        help='Sparsity constraint for predictor parameter W ' +\n",
    "                        '(default = 1.0, i.e. dense matrix).')\n",
    "    parser.add_argument('-sB', type=float, default=1.00,\n",
    "                        help='Sparsity constraint for predictor parameter B ' +\n",
    "                        '(default = 1.0, i.e. dense matrix).')\n",
    "    parser.add_argument('-sZ', type=float, default=1.00,\n",
    "                        help='Sparsity constraint for predictor parameter Z ' +\n",
    "                        '(default = 1.0, i.e. dense matrix).')\n",
    "    parser.add_argument('-pS', '--print-step', type=int, default=200,\n",
    "                        help='The number of update steps between print ' +\n",
    "                        'calls to console.')\n",
    "    parser.add_argument('-vS', '--val-step', type=int, default=3,\n",
    "                        help='The number of epochs between validation' +\n",
    "                        'performance evaluation')\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "import scipy.cluster\n",
    "import scipy.spatial\n",
    "import os\n",
    "\n",
    "\n",
    "def medianHeuristic(data, projectionDimension, numPrototypes, W_init=None):\n",
    "    '''\n",
    "    This method can be used to estimate gamma for ProtoNN. An approximation to\n",
    "    median heuristic is used here.\n",
    "    1. First the data is collapsed into the projectionDimension by W_init. If\n",
    "    W_init is not provided, it is initialized from a random normal(0, 1). Hence\n",
    "    data normalization is essential.\n",
    "    2. Prototype are computed by running a  k-means clustering on the projected\n",
    "    data.\n",
    "    3. The median distance is then estimated by calculating median distance\n",
    "    between prototypes and projected data points.\n",
    "\n",
    "    data needs to be [-1, numFeats]\n",
    "    If using this method to initialize gamma, please use the W and B as well.\n",
    "\n",
    "    TODO: Return estimate of Z (prototype labels) based on cluster centroids\n",
    "    andand labels\n",
    "\n",
    "    TODO: Clustering fails due to singularity error if projecting upwards\n",
    "\n",
    "    W [dxd_cap]\n",
    "    B [d_cap, m]\n",
    "    returns gamma, W, B\n",
    "    '''\n",
    "    assert data.ndim == 2\n",
    "    X = data\n",
    "    featDim = data.shape[1]\n",
    "    if projectionDimension > featDim:\n",
    "        print(\"Warning: Projection dimension > feature dimension. Gamma\")\n",
    "        print(\"\\t estimation due to median heuristic could fail.\")\n",
    "        print(\"\\tTo retain the projection dataDimension, provide\")\n",
    "        print(\"\\ta value for gamma.\")\n",
    "\n",
    "    if W_init is None:\n",
    "        W_init = np.random.normal(size=[featDim, projectionDimension])\n",
    "    W = W_init\n",
    "    XW = np.matmul(X, W)\n",
    "    assert XW.shape[1] == projectionDimension\n",
    "    assert XW.shape[0] == len(X)\n",
    "    # Requires [N x d_cap] data matrix of N observations of d_cap-dimension and\n",
    "    # the number of centroids m. Returns, [n x d_cap] centroids and\n",
    "    # elementwise center information.\n",
    "    B, centers = scipy.cluster.vq.kmeans2(XW, numPrototypes)\n",
    "    # Requires two matrices. Number of observations x dimension of observation\n",
    "    # space. Distances[i,j] is the distance between XW[i] and B[j]\n",
    "    distances = scipy.spatial.distance.cdist(XW, B, metric='euclidean')\n",
    "    distances = np.reshape(distances, [-1])\n",
    "    gamma = np.median(distances)\n",
    "    gamma = 1 / (2.5 * gamma)\n",
    "    return gamma.astype('float32'), W.astype('float32'), B.T.astype('float32')\n",
    "\n",
    "\n",
    "def multiClassHingeLoss(logits, label, batch_th):\n",
    "    '''\n",
    "    MultiClassHingeLoss to match C++ Version - No TF internal version\n",
    "    '''\n",
    "    flatLogits = tf.reshape(logits, [-1, ])\n",
    "    label_ = tf.argmax(label, 1)\n",
    "\n",
    "    correctId = tf.range(0, batch_th) * label.shape[1] + label_\n",
    "    correctLogit = tf.gather(flatLogits, correctId)\n",
    "\n",
    "    maxLabel = tf.argmax(logits, 1)\n",
    "    top2, _ = tf.nn.top_k(logits, k=2, sorted=True)\n",
    "\n",
    "    wrongMaxLogit = tf.where(\n",
    "        tf.equal(maxLabel, label_), top2[:, 1], top2[:, 0])\n",
    "\n",
    "    return tf.reduce_mean(tf.nn.relu(1. + wrongMaxLogit - correctLogit))\n",
    "\n",
    "\n",
    "def crossEntropyLoss(logits, label):\n",
    "    '''\n",
    "    Cross Entropy loss for MultiClass case in joint training for\n",
    "    faster convergence\n",
    "    '''\n",
    "    return tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                   labels=tf.stop_gradient(label)))\n",
    "\n",
    "\n",
    "def mean_absolute_error(logits, label):\n",
    "    '''\n",
    "    Function to compute the mean absolute error.\n",
    "    '''\n",
    "    return tf.reduce_mean(tf.abs(tf.subtract(logits, label)))\n",
    "\n",
    "\n",
    "def hardThreshold(A, s):\n",
    "    '''\n",
    "    Hard thresholding function on Tensor A with sparsity s\n",
    "    '''\n",
    "    A_ = np.copy(A)\n",
    "    A_ = A_.ravel()\n",
    "    if len(A_) > 0:\n",
    "        th = np.percentile(np.abs(A_), (1 - s) * 100.0, interpolation='higher')\n",
    "        A_[np.abs(A_) < th] = 0.0\n",
    "    A_ = A_.reshape(A.shape)\n",
    "    return A_\n",
    "\n",
    "\n",
    "def copySupport(src, dest):\n",
    "    '''\n",
    "    copy support of src tensor to dest tensor\n",
    "    '''\n",
    "    support = np.nonzero(src)\n",
    "    dest_ = dest\n",
    "    dest = np.zeros(dest_.shape)\n",
    "    dest[support] = dest_[support]\n",
    "    return dest\n",
    "\n",
    "\n",
    "def countnnZ(A, s, bytesPerVar=4):\n",
    "    '''\n",
    "    Returns # of non-zeros and representative size of the tensor\n",
    "    Uses dense for s >= 0.5 - 4 byte\n",
    "    Else uses sparse - 8 byte\n",
    "    '''\n",
    "    params = 1\n",
    "    hasSparse = False\n",
    "    for i in range(0, len(A.shape)):\n",
    "        params *= int(A.shape[i])\n",
    "    if s < 0.5:\n",
    "        nnZ = np.ceil(params * s)\n",
    "        hasSparse = True\n",
    "        return nnZ, nnZ * 2 * bytesPerVar, hasSparse\n",
    "    else:\n",
    "        nnZ = params\n",
    "        return nnZ, nnZ * bytesPerVar, hasSparse\n",
    "\n",
    "\n",
    "def getConfusionMatrix(predicted, target, numClasses):\n",
    "    '''\n",
    "    Returns a confusion matrix for a multiclass classification\n",
    "    problem. `predicted` is a 1-D array of integers representing\n",
    "    the predicted classes and `target` is the target classes.\n",
    "\n",
    "    confusion[i][j]: Number of elements of class j\n",
    "        predicted as class i\n",
    "    Labels are assumed to be in range(0, numClasses)\n",
    "    Use`printFormattedConfusionMatrix` to echo the confusion matrix\n",
    "    in a user friendly form.\n",
    "    '''\n",
    "    assert(predicted.ndim == 1)\n",
    "    assert(target.ndim == 1)\n",
    "    arr = np.zeros([numClasses, numClasses])\n",
    "\n",
    "    for i in range(len(predicted)):\n",
    "        arr[predicted[i]][target[i]] += 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "def printFormattedConfusionMatrix(matrix):\n",
    "    '''\n",
    "    Given a 2D confusion matrix, prints it in a human readable way.\n",
    "    The confusion matrix is expected to be a 2D numpy array with\n",
    "    square dimensions\n",
    "    '''\n",
    "    assert(matrix.ndim == 2)\n",
    "    assert(matrix.shape[0] == matrix.shape[1])\n",
    "    RECALL = 'Recall'\n",
    "    PRECISION = 'PRECISION'\n",
    "    print(\"|%s|\" % ('True->'), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%7d|\" % i, end='')\n",
    "    print(\"%s|\" % 'Precision')\n",
    "\n",
    "    print(\"|%s|\" % ('-' * len(RECALL)), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%s|\" % ('-' * 7), end='')\n",
    "    print(\"%s|\" % ('-' * len(PRECISION)))\n",
    "\n",
    "    precisionlist = np.sum(matrix, axis=1)\n",
    "    recalllist = np.sum(matrix, axis=0)\n",
    "    precisionlist = [matrix[i][i] / x if x !=\n",
    "                     0 else -1 for i, x in enumerate(precisionlist)]\n",
    "    recalllist = [matrix[i][i] / x if x !=\n",
    "                  0 else -1 for i, x in enumerate(recalllist)]\n",
    "    for i in range(matrix.shape[0]):\n",
    "        # len recall = 6\n",
    "        print(\"|%6d|\" % (i), end='')\n",
    "        for j in range(matrix.shape[0]):\n",
    "            print(\"%7d|\" % (matrix[i][j]), end='')\n",
    "        print(\"%s\" % (\" \" * (len(PRECISION) - 7)), end='')\n",
    "        if precisionlist[i] != -1:\n",
    "            print(\"%1.5f|\" % precisionlist[i])\n",
    "        else:\n",
    "            print(\"%7s|\" % \"nan\")\n",
    "\n",
    "    print(\"|%s|\" % ('-' * len(RECALL)), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%s|\" % ('-' * 7), end='')\n",
    "    print(\"%s|\" % ('-' * len(PRECISION)))\n",
    "    print(\"|%s|\" % ('Recall'), end='')\n",
    "\n",
    "    for i in range(matrix.shape[0]):\n",
    "        if recalllist[i] != -1:\n",
    "            print(\"%1.5f|\" % (recalllist[i]), end='')\n",
    "        else:\n",
    "            print(\"%7s|\" % \"nan\", end='')\n",
    "\n",
    "    print('%s|' % (' ' * len(PRECISION)))\n",
    "\n",
    "\n",
    "def getPrecisionRecall(cmatrix, label=1):\n",
    "    trueP = cmatrix[label][label]\n",
    "    denom = np.sum(cmatrix, axis=0)[label]\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    recall = trueP / denom\n",
    "    denom = np.sum(cmatrix, axis=1)[label]\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    precision = trueP / denom\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMacroPrecisionRecall(cmatrix):\n",
    "    # TP + FP\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    # TP + FN\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    precisionlist__ = [cmatrix[i][i] / x if x !=\n",
    "                       0 else 0 for i, x in enumerate(precisionlist)]\n",
    "    recalllist__ = [cmatrix[i][i] / x if x !=\n",
    "                    0 else 0 for i, x in enumerate(recalllist)]\n",
    "    precision = np.sum(precisionlist__)\n",
    "    precision /= len(precisionlist__)\n",
    "    recall = np.sum(recalllist__)\n",
    "    recall /= len(recalllist__)\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMicroPrecisionRecall(cmatrix):\n",
    "    # TP + FP\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    # TP + FN\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    num = 0.0\n",
    "    for i in range(len(cmatrix)):\n",
    "        num += cmatrix[i][i]\n",
    "\n",
    "    precision = num / np.sum(precisionlist)\n",
    "    recall = num / np.sum(recalllist)\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMacroMicroFScore(cmatrix):\n",
    "    '''\n",
    "    Returns macro and micro f-scores.\n",
    "    Refer: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.104.8244&rep=rep1&type=pdf\n",
    "    '''\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    precisionlist__ = [cmatrix[i][i] / x if x !=\n",
    "                       0 else 0 for i, x in enumerate(precisionlist)]\n",
    "    recalllist__ = [cmatrix[i][i] / x if x !=\n",
    "                    0 else 0 for i, x in enumerate(recalllist)]\n",
    "    macro = 0.0\n",
    "    for i in range(len(precisionlist)):\n",
    "        denom = precisionlist__[i] + recalllist__[i]\n",
    "        numer = precisionlist__[i] * recalllist__[i] * 2\n",
    "        if denom == 0:\n",
    "            denom = 1\n",
    "        macro += numer / denom\n",
    "    macro /= len(precisionlist)\n",
    "\n",
    "    num = 0.0\n",
    "    for i in range(len(precisionlist)):\n",
    "        num += cmatrix[i][i]\n",
    "\n",
    "    denom1 = np.sum(precisionlist)\n",
    "    denom2 = np.sum(recalllist)\n",
    "    pi = num / denom1\n",
    "    rho = num / denom2\n",
    "    denom = pi + rho\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    micro = 2 * pi * rho / denom\n",
    "    return macro, micro\n",
    "\n",
    "\n",
    "class GraphManager:\n",
    "    '''\n",
    "    Manages saving and restoring graphs. Designed to be used with EMI-RNN\n",
    "    though is general enough to be useful otherwise as well.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def checkpointModel(self, saver, sess, modelPrefix,\n",
    "                        globalStep=1000, redirFile=None):\n",
    "        saver.save(sess, modelPrefix, global_step=globalStep)\n",
    "        print('Model saved to %s, global_step %d' % (modelPrefix, globalStep),\n",
    "              file=redirFile)\n",
    "\n",
    "    def loadCheckpoint(self, sess, modelPrefix, globalStep,\n",
    "                       redirFile=None):\n",
    "        metaname = modelPrefix + '-%d.meta' % globalStep\n",
    "        basename = os.path.basename(metaname)\n",
    "        fileList = os.listdir(os.path.dirname(modelPrefix))\n",
    "        fileList = [x for x in fileList if x.startswith(basename)]\n",
    "        assert len(fileList) > 0, 'Checkpoint file not found'\n",
    "        msg = 'Too many or too few checkpoint files for globalStep: %d' % globalStep\n",
    "        assert len(fileList) is 1, msg\n",
    "        chkpt = basename + '/' + fileList[0]\n",
    "        saver = tf.train.import_meta_graph(metaname)\n",
    "        metaname = metaname[:-5]\n",
    "        saver.restore(sess, metaname)\n",
    "        graph = tf.get_default_graph()\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trainer\n",
    "class ProtoNNTrainer:\n",
    "    def __init__(self, protoNNObj, regW, regB, regZ,\n",
    "                 sparcityW, sparcityB, sparcityZ,\n",
    "                 learningRate, X, Y, lossType='l2'):\n",
    "        '''\n",
    "        A wrapper for the various techniques used for training ProtoNN. This\n",
    "        subsumes both the responsibility of loss graph construction and\n",
    "        performing training. The original training routine that is part of the\n",
    "        C++ implementation of EdgeML used iterative hard thresholding (IHT),\n",
    "        gamma estimation through median heuristic and other tricks for\n",
    "        training ProtoNN. This module implements the same in Tensorflow\n",
    "        and python.\n",
    "\n",
    "        protoNNObj: An instance of ProtoNN class defining the forward\n",
    "            computation graph. The loss functions and training routines will be\n",
    "            attached to this instance.\n",
    "        regW, regB, regZ: Regularization constants for W, B, and\n",
    "            Z matrices of protoNN.\n",
    "        sparcityW, sparcityB, sparcityZ: Sparsity constraints\n",
    "            for W, B and Z matrices. A value between 0 (exclusive) and 1\n",
    "            (inclusive) is expected. A value of 1 indicates dense training.\n",
    "        learningRate: Initial learning rate for ADAM optimizer.\n",
    "        X, Y : Placeholders for data and labels.\n",
    "            X [-1, featureDimension]\n",
    "            Y [-1, num Labels]\n",
    "        lossType: ['l2', 'xentropy']\n",
    "        '''\n",
    "        self.protoNNObj = protoNNObj\n",
    "        self.__regW = regW\n",
    "        self.__regB = regB\n",
    "        self.__regZ = regZ\n",
    "        self.__sW = sparcityW\n",
    "        self.__sB = sparcityB\n",
    "        self.__sZ = sparcityZ\n",
    "        self.__lR = learningRate\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.sparseTraining = True\n",
    "        if (sparcityW == 1.0) and (sparcityB == 1.0) and (sparcityZ == 1.0):\n",
    "            self.sparseTraining = False\n",
    "            print(\"Sparse training disabled.\", file=sys.stderr)\n",
    "        # Define placeholders for sparse training\n",
    "        self.W_th = None\n",
    "        self.B_th = None\n",
    "        self.Z_th = None\n",
    "        self.__lossType = lossType\n",
    "        self.__validInit = False\n",
    "        self.__validInit = self.__validateInit()\n",
    "        self.__protoNNOut = protoNNObj(X, Y)\n",
    "        self.loss = self.__lossGraph()\n",
    "        self.trainStep = self.__trainGraph()\n",
    "        self.__hthOp = self.__getHardThresholdOp()\n",
    "        self.accuracy = protoNNObj.getAccuracyOp()\n",
    "\n",
    "    def __validateInit(self):\n",
    "        self.__validInit = False\n",
    "        msg = \"Sparsity value should be between\"\n",
    "        msg += \" 0 and 1 (both inclusive).\"\n",
    "        assert self.__sW >= 0. and self.__sW <= 1., 'W:' + msg\n",
    "        assert self.__sB >= 0. and self.__sB <= 1., 'B:' + msg\n",
    "        assert self.__sZ >= 0. and self.__sZ <= 1., 'Z:' + msg\n",
    "        d, dcap, m, L, _ = self.protoNNObj.getHyperParams()\n",
    "        msg = 'Y should be of dimension [-1, num labels/classes]'\n",
    "        msg += ' specified as part of ProtoNN object.'\n",
    "        assert (len(self.Y.shape)) == 2, msg\n",
    "        assert (self.Y.shape[1] == L), msg\n",
    "        msg = 'X should be of dimension [-1, featureDimension]'\n",
    "        msg += ' specified as part of ProtoNN object.'\n",
    "        assert (len(self.X.shape) == 2), msg\n",
    "        assert (self.X.shape[1] == d), msg\n",
    "        self.__validInit = True\n",
    "        msg = 'Values can be \\'l2\\', or \\'xentropy\\''\n",
    "        if self.__lossType not in ['l2', 'xentropy']:\n",
    "            raise ValueError(msg)\n",
    "        return True\n",
    "\n",
    "    def __lossGraph(self):\n",
    "        pnnOut = self.__protoNNOut\n",
    "        l1, l2, l3 = self.__regW, self.__regB, self.__regZ\n",
    "        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n",
    "        if self.__lossType == 'l2':\n",
    "            with tf.name_scope('protonn-l2-loss'):\n",
    "                loss_0 = tf.nn.l2_loss(self.Y - pnnOut)\n",
    "                reg = l1 * tf.nn.l2_loss(W) + l2 * tf.nn.l2_loss(B)\n",
    "                reg += l3 * tf.nn.l2_loss(Z)\n",
    "                loss = loss_0 + reg\n",
    "        elif self.__lossType == 'xentropy':\n",
    "            with tf.name_scope('protonn-xentropy-loss'):\n",
    "                loss_0 = tf.nn.softmax_cross_entropy_with_logits_v2(logits=pnnOut,\n",
    "                                                         labels=tf.stop_gradient(self.Y))\n",
    "                loss_0 = tf.reduce_mean(loss_0)\n",
    "                reg = l1 * tf.nn.l2_loss(W) + l2 * tf.nn.l2_loss(B)\n",
    "                reg += l3 * tf.nn.l2_loss(Z)\n",
    "                loss = loss_0 + reg\n",
    "        return loss\n",
    "\n",
    "    def __trainGraph(self):\n",
    "        with tf.name_scope('protonn-gradient-adam'):\n",
    "            trainStep = tf.train.AdamOptimizer(self.__lR)\n",
    "            trainStep = trainStep.minimize(self.loss)\n",
    "        return trainStep\n",
    "\n",
    "    def __getHardThresholdOp(self):\n",
    "        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n",
    "        self.W_th = tf.placeholder(tf.float32, name='W_th')\n",
    "        self.B_th = tf.placeholder(tf.float32, name='B_th')\n",
    "        self.Z_th = tf.placeholder(tf.float32, name='Z_th')\n",
    "        with tf.name_scope('hard-threshold-assignments'):\n",
    "            hard_thrsd_W = W.assign(self.W_th)\n",
    "            hard_thrsd_B = B.assign(self.B_th)\n",
    "            hard_thrsd_Z = Z.assign(self.Z_th)\n",
    "            hard_thrsd_op = tf.group(hard_thrsd_W, hard_thrsd_B, hard_thrsd_Z)\n",
    "        return hard_thrsd_op\n",
    "\n",
    "    def train(self, batchSize, totalEpochs, sess,\n",
    "              x_train, x_val, y_train, y_val, noInit=False,\n",
    "              redirFile=None, printStep=10, valStep=3):\n",
    "        '''\n",
    "        Performs dense training of ProtoNN followed by iterative hard\n",
    "        thresholding to enforce sparsity constraints.\n",
    "\n",
    "        batchSize: Batch size per update\n",
    "        totalEpochs: The number of epochs to run training for. One epoch is\n",
    "            defined as one pass over the entire training data.\n",
    "        sess: The Tensorflow session to use for running various graph\n",
    "            operators.\n",
    "        x_train, x_val, y_train, y_val: The numpy array containing train and\n",
    "            validation data. x data is assumed to in of shape [-1,\n",
    "            featureDimension] while y should have shape [-1, numberLabels].\n",
    "        noInit: By default, all the tensors of the computation graph are\n",
    "        initialized at the start of the training session. Set noInit=False to\n",
    "        disable this behaviour.\n",
    "        printStep: Number of batches between echoing of loss and train accuracy.\n",
    "        valStep: Number of epochs between evolutions on validation set.\n",
    "        '''\n",
    "        d, d_cap, m, L, gamma = self.protoNNObj.getHyperParams()\n",
    "        assert batchSize >= 1, 'Batch size should be positive integer'\n",
    "        assert totalEpochs >= 1, 'Total epochs should be positive integer'\n",
    "        assert x_train.ndim == 2, 'Expected training data to be of rank 2'\n",
    "        assert x_train.shape[1] == d, 'Expected x_train to be [-1, %d]' % d\n",
    "        assert x_val.ndim == 2, 'Expected validation data to be of rank 2'\n",
    "        assert x_val.shape[1] == d, 'Expected x_val to be [-1, %d]' % d\n",
    "        assert y_train.ndim == 2, 'Expected training labels to be of rank 2'\n",
    "        assert y_train.shape[1] == L, 'Expected y_train to be [-1, %d]' % L\n",
    "        assert y_val.ndim == 2, 'Expected validation labels to be of rank 2'\n",
    "        assert y_val.shape[1] == L, 'Expected y_val to be [-1, %d]' % L\n",
    "\n",
    "        # Numpy will throw asserts for arrays\n",
    "        if sess is None:\n",
    "            raise ValueError('sess must be valid Tensorflow session.')\n",
    "\n",
    "        trainNumBatches = int(np.ceil(len(x_train) / batchSize))\n",
    "        valNumBatches = int(np.ceil(len(x_val) / batchSize))\n",
    "        x_train_batches = np.array_split(x_train, trainNumBatches)\n",
    "        y_train_batches = np.array_split(y_train, trainNumBatches)\n",
    "        x_val_batches = np.array_split(x_val, valNumBatches)\n",
    "        y_val_batches = np.array_split(y_val, valNumBatches)\n",
    "        if not noInit:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        X, Y = self.X, self.Y\n",
    "        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n",
    "        for epoch in range(totalEpochs):\n",
    "            for i in range(len(x_train_batches)):\n",
    "                batch_x = x_train_batches[i]\n",
    "                batch_y = y_train_batches[i]\n",
    "                feed_dict = {\n",
    "                    X: batch_x,\n",
    "                    Y: batch_y\n",
    "                }\n",
    "                sess.run(self.trainStep, feed_dict=feed_dict)\n",
    "                if i % printStep == 0:\n",
    "                    loss, acc = sess.run([self.loss, self.accuracy],\n",
    "                                         feed_dict=feed_dict)\n",
    "                    msg = \"Epoch: %3d Batch: %3d\" % (epoch, i)\n",
    "                    msg += \" Loss: %3.5f Accuracy: %2.5f\" % (loss, acc)\n",
    "                    print(msg, file=redirFile)\n",
    "\n",
    "            # Perform Hard thresholding\n",
    "            if self.sparseTraining:\n",
    "                W_, B_, Z_ = sess.run([W, B, Z])\n",
    "                fd_thrsd = {\n",
    "                    self.W_th: hardThreshold(W_, self.__sW),\n",
    "                    self.B_th: hardThreshold(B_, self.__sB),\n",
    "                    self.Z_th: hardThreshold(Z_, self.__sZ)\n",
    "                }\n",
    "                sess.run(self.__hthOp, feed_dict=fd_thrsd)\n",
    "\n",
    "            if (epoch + 1) % valStep  == 0:\n",
    "                acc = 0.0\n",
    "                loss = 0.0\n",
    "                for j in range(len(x_val_batches)):\n",
    "                    batch_x = x_val_batches[j]\n",
    "                    batch_y = y_val_batches[j]\n",
    "                    feed_dict = {\n",
    "                        X: batch_x,\n",
    "                        Y: batch_y\n",
    "                    }\n",
    "                    acc_, loss_ = sess.run([self.accuracy, self.loss],\n",
    "                                           feed_dict=feed_dict)\n",
    "                    acc += acc_\n",
    "                    loss += loss_\n",
    "                acc /= len(y_val_batches)\n",
    "                loss /= len(y_val_batches)\n",
    "                print(\"Test Loss: %2.5f Accuracy: %2.5f\" % (loss, acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ProtoNN:\n",
    "    def __init__(self, inputDimension, projectionDimension, numPrototypes,\n",
    "                 numOutputLabels, gamma,\n",
    "                 W = None, B = None, Z = None):\n",
    "        '''\n",
    "        Forward computation graph for ProtoNN.\n",
    "\n",
    "        inputDimension: Input data dimension or feature dimension.\n",
    "        projectionDimension: hyperparameter\n",
    "        numPrototypes: hyperparameter\n",
    "        numOutputLabels: The number of output labels or classes\n",
    "        W, B, Z: Numpy matrices that can be used to initialize\n",
    "            projection matrix(W), prototype matrix (B) and prototype labels\n",
    "            matrix (B).\n",
    "            Expected Dimensions:\n",
    "                W   inputDimension (d) x projectionDimension (d_cap)\n",
    "                B   projectionDimension (d_cap) x numPrototypes (m)\n",
    "                Z   numOutputLabels (L) x numPrototypes (m)\n",
    "        '''\n",
    "        with tf.name_scope('protoNN') as ns:\n",
    "            self.__nscope = ns\n",
    "        self.__d = inputDimension\n",
    "        self.__d_cap = projectionDimension\n",
    "        self.__m = numPrototypes\n",
    "        self.__L = numOutputLabels\n",
    "\n",
    "        self.__inW = W\n",
    "        self.__inB = B\n",
    "        self.__inZ = Z\n",
    "        self.__inGamma = gamma\n",
    "        self.W, self.B, self.Z = None, None, None\n",
    "        self.gamma = None\n",
    "\n",
    "        self.__validInit = False\n",
    "        self.__initWBZ()\n",
    "        self.__initGamma()\n",
    "        self.__validateInit()\n",
    "        self.protoNNOut = None\n",
    "        self.predictions = None\n",
    "        self.accuracy = None\n",
    "\n",
    "    def __validateInit(self):\n",
    "        self.__validInit = False\n",
    "        errmsg = \"Dimensions mismatch! Should be W[d, d_cap]\"\n",
    "        errmsg += \", B[d_cap, m] and Z[L, m]\"\n",
    "        d, d_cap, m, L, _ = self.getHyperParams()\n",
    "        assert self.W.shape[0] == d, errmsg\n",
    "        assert self.W.shape[1] == d_cap, errmsg\n",
    "        assert self.B.shape[0] == d_cap, errmsg\n",
    "        assert self.B.shape[1] == m, errmsg\n",
    "        assert self.Z.shape[0] == L, errmsg\n",
    "        assert self.Z.shape[1] == m, errmsg\n",
    "        self.__validInit = True\n",
    "\n",
    "    def __initWBZ(self):\n",
    "        with tf.name_scope(self.__nscope):\n",
    "            W = self.__inW\n",
    "            if W is None:\n",
    "                W = tf.random_normal_initializer()\n",
    "                W = W([self.__d, self.__d_cap])\n",
    "            self.W = tf.Variable(W, name='W', dtype=tf.float32)\n",
    "\n",
    "            B = self.__inB\n",
    "            if B is None:\n",
    "                B = tf.random_uniform_initializer()\n",
    "                B = B([self.__d_cap, self.__m])\n",
    "            self.B = tf.Variable(B, name='B', dtype=tf.float32)\n",
    "\n",
    "            Z = self.__inZ\n",
    "            if Z is None:\n",
    "                Z = tf.random_normal_initializer()\n",
    "                Z = Z([self.__L, self.__m])\n",
    "            Z = tf.Variable(Z, name='Z', dtype=tf.float32)\n",
    "            self.Z = Z\n",
    "        return self.W, self.B, self.Z\n",
    "\n",
    "    def __initGamma(self):\n",
    "        with tf.name_scope(self.__nscope):\n",
    "            gamma = self.__inGamma\n",
    "            self.gamma = tf.constant(gamma, name='gamma')\n",
    "\n",
    "    def getHyperParams(self):\n",
    "        '''\n",
    "        Returns the model hyperparameters:\n",
    "            [inputDimension, projectionDimension,\n",
    "            numPrototypes, numOutputLabels, gamma]\n",
    "        '''\n",
    "        d = self.__d\n",
    "        dcap = self.__d_cap\n",
    "        m = self.__m\n",
    "        L = self.__L\n",
    "        return d, dcap, m, L, self.gamma\n",
    "\n",
    "    def getModelMatrices(self):\n",
    "        '''\n",
    "        Returns Tensorflow tensors of the model matrices, which\n",
    "        can then be evaluated to obtain corresponding numpy arrays.\n",
    "\n",
    "        These can then be exported as part of other implementations of\n",
    "        ProtonNN, for instance a C++ implementation or pure python\n",
    "        implementation.\n",
    "        Returns\n",
    "            [ProjectionMatrix (W), prototypeMatrix (B),\n",
    "             prototypeLabelsMatrix (Z), gamma]\n",
    "        '''\n",
    "        return self.W, self.B, self.Z, self.gamma\n",
    "\n",
    "    def __call__(self, X, Y=None):\n",
    "        '''\n",
    "        This method is responsible for construction of the forward computation\n",
    "        graph. The end point of the computation graph, or in other words the\n",
    "        output operator for the forward computation is returned. Additionally,\n",
    "        if the argument Y is provided, a classification accuracy operator with\n",
    "        Y as target will also be created. For this, Y is assumed to in one-hot\n",
    "        encoded format and the class with the maximum prediction score is\n",
    "        compared to the encoded class in Y.  This accuracy operator is returned\n",
    "        by getAccuracyOp() method. If a different accuracyOp is required, it\n",
    "        can be defined by overriding the createAccOp(protoNNScoresOut, Y)\n",
    "        method.\n",
    "\n",
    "        X: Input tensor or placeholder of shape [-1, inputDimension]\n",
    "        Y: Optional tensor or placeholder for targets (labels or classes).\n",
    "            Expected shape is [-1, numOutputLabels].\n",
    "        returns: The forward computation outputs, self.protoNNOut\n",
    "        '''\n",
    "        # This should never execute\n",
    "        assert self.__validInit is True, \"Initialization failed!\"\n",
    "        if self.protoNNOut is not None:\n",
    "            return self.protoNNOut\n",
    "\n",
    "        W, B, Z, gamma = self.W, self.B, self.Z, self.gamma\n",
    "        with tf.name_scope(self.__nscope):\n",
    "            WX = tf.matmul(X, W)\n",
    "            # Convert WX to tensor so that broadcasting can work\n",
    "            dim = [-1, WX.shape.as_list()[1], 1]\n",
    "            WX = tf.reshape(WX, dim)\n",
    "            dim = [1, B.shape.as_list()[0], -1]\n",
    "            B = tf.reshape(B, dim)\n",
    "            l2sim = B - WX\n",
    "            l2sim = tf.pow(l2sim, 2)\n",
    "            l2sim = tf.reduce_sum(l2sim, 1, keepdims=True)\n",
    "            self.l2sim = l2sim\n",
    "            gammal2sim = (-1 * gamma * gamma) * l2sim\n",
    "            M = tf.exp(gammal2sim)\n",
    "            dim = [1] + Z.shape.as_list()\n",
    "            Z = tf.reshape(Z, dim)\n",
    "            y = tf.multiply(Z, M)\n",
    "            y = tf.reduce_sum(y, 2, name='protoNNScoreOut')\n",
    "            self.protoNNOut = y\n",
    "            self.predictions = tf.argmax(y, 1, name='protoNNPredictions')\n",
    "            if Y is not None:\n",
    "                self.createAccOp(self.protoNNOut, Y)\n",
    "        return y\n",
    "\n",
    "    def createAccOp(self, outputs, target):\n",
    "        '''\n",
    "        Define an accuracy operation on ProtoNN's output scores and targets.\n",
    "        Here a simple classification accuracy operator is defined. More\n",
    "        complicated operators (for multiple label problems and so forth) can be\n",
    "        defined by overriding this method\n",
    "        '''\n",
    "        assert self.predictions is not None\n",
    "        target = tf.argmax(target, 1)\n",
    "        correctPrediction = tf.equal(self.predictions, target)\n",
    "        acc = tf.reduce_mean(tf.cast(correctPrediction, tf.float32),\n",
    "                             name='protoNNAccuracy')\n",
    "        self.accuracy = acc\n",
    "\n",
    "    def getPredictionsOp(self):\n",
    "        '''\n",
    "        The predictions operator is defined as argmax(protoNNScores) for each\n",
    "        prediction.\n",
    "        '''\n",
    "        return self.predictions\n",
    "\n",
    "    def getAccuracyOp(self):\n",
    "        '''\n",
    "        returns accuracyOp as defined by createAccOp. It defaults to\n",
    "        multi-class classification accuracy.\n",
    "        '''\n",
    "        msg = \"Accuracy operator not defined in graph. Did you provide Y as an\"\n",
    "        msg += \" argument to _call_?\"\n",
    "        assert self.accuracy is not None, msg\n",
    "        return self.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Dimension:  423\n",
      "Num classes:  2\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = r\"./experiments\"\n",
    "windowLen = 'data_w2'\n",
    "out = preprocessData(DATA_DIR,windowLen)\n",
    "dataDimension = out[0]\n",
    "numClasses = out[1]\n",
    "x_train, y_train = out[2], out[3]\n",
    "x_test, y_test = out[4], out[5]\n",
    "print(\"Feature Dimension: \", dataDimension)\n",
    "print(\"Num classes: \", numClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_DIR = r\"./experiments\"\n",
    "train, test = np.load(DATA_DIR + '/ttrain_data_w2.npy'), np.load(DATA_DIR + '/ttest_data_w2.npy')\n",
    "x_train, y_train = train[:, 1:], train[:, 0]\n",
    "x_test, y_test = test[:, 1:], test[:, 0]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "numClasses = max(y_train) - min(y_train) + 1\n",
    "numClasses = max(numClasses, max(y_test) - min(y_test) + 1)\n",
    "numClasses = int(numClasses)\n",
    "\n",
    "y_train = helper.to_onehot(y_train, numClasses)\n",
    "y_test = helper.to_onehot(y_test, numClasses)\n",
    "y_val = helper.to_onehot(y_val, numClasses)\n",
    "\n",
    "dataDimension = x_train.shape[1]\n",
    "numClasses = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nadit\\anaconda3\\envs\\ProtoNN\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py:2825: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "Epoch:   0 Batch:   0 Loss: 46868.41406 Accuracy: 0.47614\n",
      "Epoch:   1 Batch:   0 Loss: 44770.91016 Accuracy: 0.47614\n",
      "Epoch:   2 Batch:   0 Loss: 42837.33203 Accuracy: 0.47614\n",
      "Epoch:   3 Batch:   0 Loss: 40965.57031 Accuracy: 0.47614\n",
      "Epoch:   4 Batch:   0 Loss: 39157.84766 Accuracy: 0.47614\n",
      "Epoch:   5 Batch:   0 Loss: 37414.41797 Accuracy: 0.47614\n",
      "Epoch:   6 Batch:   0 Loss: 35734.51172 Accuracy: 0.47614\n",
      "Epoch:   7 Batch:   0 Loss: 34116.85156 Accuracy: 0.47614\n",
      "Epoch:   8 Batch:   0 Loss: 32560.05664 Accuracy: 0.47614\n",
      "Epoch:   9 Batch:   0 Loss: 31062.67969 Accuracy: 0.47614\n",
      "Test Loss: 28237.13965 Accuracy: 0.49987\n",
      "Epoch:  10 Batch:   0 Loss: 29623.28125 Accuracy: 0.47614\n",
      "Epoch:  11 Batch:   0 Loss: 28240.40820 Accuracy: 0.47614\n",
      "Epoch:  12 Batch:   0 Loss: 26912.63281 Accuracy: 0.47614\n",
      "Epoch:  13 Batch:   0 Loss: 25638.51758 Accuracy: 0.47614\n",
      "Epoch:  14 Batch:   0 Loss: 24416.63477 Accuracy: 0.47614\n",
      "Epoch:  15 Batch:   0 Loss: 23245.52734 Accuracy: 0.47614\n",
      "Epoch:  16 Batch:   0 Loss: 22123.76758 Accuracy: 0.47614\n",
      "Epoch:  17 Batch:   0 Loss: 21049.87695 Accuracy: 0.47614\n",
      "Epoch:  18 Batch:   0 Loss: 20022.40625 Accuracy: 0.47614\n",
      "Epoch:  19 Batch:   0 Loss: 19020.58203 Accuracy: 0.47614\n",
      "Test Loss: 17266.88428 Accuracy: 0.49987\n",
      "Epoch:  20 Batch:   0 Loss: 18075.84570 Accuracy: 0.47614\n",
      "Epoch:  21 Batch:   0 Loss: 17173.51758 Accuracy: 0.47614\n",
      "Epoch:  22 Batch:   0 Loss: 16312.04297 Accuracy: 0.47614\n",
      "Epoch:  23 Batch:   0 Loss: 15489.95508 Accuracy: 0.47614\n",
      "Epoch:  24 Batch:   0 Loss: 14705.81250 Accuracy: 0.47614\n",
      "Epoch:  25 Batch:   0 Loss: 13958.21289 Accuracy: 0.47614\n",
      "Epoch:  26 Batch:   0 Loss: 13245.75000 Accuracy: 0.47614\n",
      "Epoch:  27 Batch:   0 Loss: 12567.07520 Accuracy: 0.47614\n",
      "Epoch:  28 Batch:   0 Loss: 11920.83203 Accuracy: 0.47614\n",
      "Epoch:  29 Batch:   0 Loss: 11305.51562 Accuracy: 0.47614\n",
      "Test Loss: 10200.53888 Accuracy: 0.49987\n",
      "Epoch:  30 Batch:   0 Loss: 10720.20215 Accuracy: 0.47614\n",
      "Epoch:  31 Batch:   0 Loss: 10163.45410 Accuracy: 0.47614\n",
      "Epoch:  32 Batch:   0 Loss: 9634.13770 Accuracy: 0.47614\n",
      "Epoch:  33 Batch:   0 Loss: 9130.97852 Accuracy: 0.47614\n",
      "Epoch:  34 Batch:   0 Loss: 8652.76074 Accuracy: 0.47614\n",
      "Epoch:  35 Batch:   0 Loss: 8198.38086 Accuracy: 0.47614\n",
      "Epoch:  36 Batch:   0 Loss: 7766.84717 Accuracy: 0.47614\n",
      "Epoch:  37 Batch:   0 Loss: 7357.04053 Accuracy: 0.47614\n",
      "Epoch:  38 Batch:   0 Loss: 6968.10596 Accuracy: 0.47614\n",
      "Epoch:  39 Batch:   0 Loss: 6599.03271 Accuracy: 0.47614\n",
      "Test Loss: 5908.94031 Accuracy: 0.49987\n",
      "Epoch:  40 Batch:   0 Loss: 6248.83594 Accuracy: 0.47614\n",
      "Epoch:  41 Batch:   0 Loss: 5916.55908 Accuracy: 0.47614\n",
      "Epoch:  42 Batch:   0 Loss: 5601.48389 Accuracy: 0.47614\n",
      "Epoch:  43 Batch:   0 Loss: 5302.70410 Accuracy: 0.47614\n",
      "Epoch:  44 Batch:   0 Loss: 5019.49121 Accuracy: 0.47614\n",
      "Epoch:  45 Batch:   0 Loss: 4751.04541 Accuracy: 0.47614\n",
      "Epoch:  46 Batch:   0 Loss: 4496.64844 Accuracy: 0.47614\n",
      "Epoch:  47 Batch:   0 Loss: 4255.61377 Accuracy: 0.47614\n",
      "Epoch:  48 Batch:   0 Loss: 4027.24072 Accuracy: 0.47614\n",
      "Epoch:  49 Batch:   0 Loss: 3810.95337 Accuracy: 0.47614\n",
      "Test Loss: 3391.34842 Accuracy: 0.49987\n",
      "Epoch:  50 Batch:   0 Loss: 3606.14551 Accuracy: 0.47614\n",
      "Epoch:  51 Batch:   0 Loss: 3412.28101 Accuracy: 0.47614\n",
      "Epoch:  52 Batch:   0 Loss: 3228.78906 Accuracy: 0.47614\n",
      "Epoch:  53 Batch:   0 Loss: 3055.13843 Accuracy: 0.47614\n",
      "Epoch:  54 Batch:   0 Loss: 2890.84790 Accuracy: 0.47614\n",
      "Epoch:  55 Batch:   0 Loss: 2735.45117 Accuracy: 0.47614\n",
      "Epoch:  56 Batch:   0 Loss: 2588.48047 Accuracy: 0.47614\n",
      "Epoch:  57 Batch:   0 Loss: 2449.53540 Accuracy: 0.47614\n",
      "Epoch:  58 Batch:   0 Loss: 2318.19263 Accuracy: 0.47614\n",
      "Epoch:  59 Batch:   0 Loss: 2194.05908 Accuracy: 0.47614\n",
      "Test Loss: 1948.16261 Accuracy: 0.49987\n",
      "Epoch:  60 Batch:   0 Loss: 2076.77979 Accuracy: 0.47614\n",
      "Epoch:  61 Batch:   0 Loss: 1965.99072 Accuracy: 0.47614\n",
      "Epoch:  62 Batch:   0 Loss: 1861.37256 Accuracy: 0.47614\n",
      "Epoch:  63 Batch:   0 Loss: 1762.59839 Accuracy: 0.47614\n",
      "Epoch:  64 Batch:   0 Loss: 1669.74585 Accuracy: 0.47614\n",
      "Epoch:  65 Batch:   0 Loss: 1583.31494 Accuracy: 0.47614\n",
      "Epoch:  66 Batch:   0 Loss: 1501.73450 Accuracy: 0.47614\n",
      "Epoch:  67 Batch:   0 Loss: 1424.73889 Accuracy: 0.47614\n",
      "Epoch:  68 Batch:   0 Loss: 1352.08411 Accuracy: 0.47614\n",
      "Epoch:  69 Batch:   0 Loss: 1283.54297 Accuracy: 0.47614\n",
      "Test Loss: 1145.57619 Accuracy: 0.49987\n",
      "Epoch:  70 Batch:   0 Loss: 1218.89441 Accuracy: 0.47614\n",
      "Epoch:  71 Batch:   0 Loss: 1157.93677 Accuracy: 0.47614\n",
      "Epoch:  72 Batch:   0 Loss: 1100.47351 Accuracy: 0.47614\n",
      "Epoch:  73 Batch:   0 Loss: 1046.31934 Accuracy: 0.47614\n",
      "Epoch:  74 Batch:   0 Loss: 995.29712 Accuracy: 0.47614\n",
      "Epoch:  75 Batch:   0 Loss: 947.24042 Accuracy: 0.47614\n",
      "Epoch:  76 Batch:   0 Loss: 901.98938 Accuracy: 0.47614\n",
      "Epoch:  77 Batch:   0 Loss: 859.39319 Accuracy: 0.47614\n",
      "Epoch:  78 Batch:   0 Loss: 819.31012 Accuracy: 0.47614\n",
      "Epoch:  79 Batch:   0 Loss: 781.60254 Accuracy: 0.47614\n",
      "Test Loss: 706.61200 Accuracy: 0.49987\n",
      "Epoch:  80 Batch:   0 Loss: 746.14124 Accuracy: 0.47614\n",
      "Epoch:  81 Batch:   0 Loss: 712.80371 Accuracy: 0.47614\n",
      "Epoch:  82 Batch:   0 Loss: 681.47369 Accuracy: 0.47614\n",
      "Epoch:  83 Batch:   0 Loss: 652.03979 Accuracy: 0.47614\n",
      "Epoch:  84 Batch:   0 Loss: 624.39764 Accuracy: 0.47614\n",
      "Epoch:  85 Batch:   0 Loss: 598.44702 Accuracy: 0.47614\n",
      "Epoch:  86 Batch:   0 Loss: 574.09338 Accuracy: 0.47614\n",
      "Epoch:  87 Batch:   0 Loss: 551.24725 Accuracy: 0.47614\n",
      "Epoch:  88 Batch:   0 Loss: 529.82343 Accuracy: 0.47614\n",
      "Epoch:  89 Batch:   0 Loss: 509.74014 Accuracy: 0.47614\n",
      "Test Loss: 471.27427 Accuracy: 0.49987\n",
      "Epoch:  90 Batch:   0 Loss: 490.92148 Accuracy: 0.47614\n",
      "Epoch:  91 Batch:   0 Loss: 473.29468 Accuracy: 0.47614\n",
      "Epoch:  92 Batch:   0 Loss: 456.79080 Accuracy: 0.47614\n",
      "Epoch:  93 Batch:   0 Loss: 441.34393 Accuracy: 0.47614\n",
      "Epoch:  94 Batch:   0 Loss: 426.89240 Accuracy: 0.47614\n",
      "Epoch:  95 Batch:   0 Loss: 413.37762 Accuracy: 0.47614\n",
      "Epoch:  96 Batch:   0 Loss: 400.74503 Accuracy: 0.47614\n",
      "Epoch:  97 Batch:   0 Loss: 388.94037 Accuracy: 0.47822\n",
      "Epoch:  98 Batch:   0 Loss: 377.91470 Accuracy: 0.47822\n",
      "Epoch:  99 Batch:   0 Loss: 367.62112 Accuracy: 0.47822\n",
      "Test Loss: 349.68356 Accuracy: 0.49974\n",
      "Epoch: 100 Batch:   0 Loss: 358.01422 Accuracy: 0.47822\n",
      "Epoch: 101 Batch:   0 Loss: 349.05219 Accuracy: 0.47822\n",
      "Epoch: 102 Batch:   0 Loss: 340.69336 Accuracy: 0.47822\n",
      "Epoch: 103 Batch:   0 Loss: 332.90442 Accuracy: 0.47822\n",
      "Epoch: 104 Batch:   0 Loss: 325.64783 Accuracy: 0.48133\n",
      "Epoch: 105 Batch:   0 Loss: 318.88907 Accuracy: 0.48029\n",
      "Epoch: 106 Batch:   0 Loss: 312.59708 Accuracy: 0.48133\n",
      "Epoch: 107 Batch:   0 Loss: 306.74146 Accuracy: 0.48133\n",
      "Epoch: 108 Batch:   0 Loss: 301.29398 Accuracy: 0.48340\n",
      "Epoch: 109 Batch:   0 Loss: 296.22855 Accuracy: 0.48651\n",
      "Test Loss: 289.51921 Accuracy: 0.49868\n",
      "Epoch: 110 Batch:   0 Loss: 291.51993 Accuracy: 0.48651\n",
      "Epoch: 111 Batch:   0 Loss: 287.14413 Accuracy: 0.48755\n",
      "Epoch: 112 Batch:   0 Loss: 283.07944 Accuracy: 0.48651\n",
      "Epoch: 113 Batch:   0 Loss: 279.30438 Accuracy: 0.48859\n",
      "Epoch: 114 Batch:   0 Loss: 275.80017 Accuracy: 0.49170\n",
      "Epoch: 115 Batch:   0 Loss: 272.54788 Accuracy: 0.49481\n",
      "Epoch: 116 Batch:   0 Loss: 269.53027 Accuracy: 0.49585\n",
      "Epoch: 117 Batch:   0 Loss: 266.72244 Accuracy: 0.50000\n",
      "Epoch: 118 Batch:   0 Loss: 264.12729 Accuracy: 0.50207\n",
      "Epoch: 119 Batch:   0 Loss: 261.72092 Accuracy: 0.50726\n",
      "Test Loss: 261.06153 Accuracy: 0.50781\n",
      "Epoch: 120 Batch:   0 Loss: 259.49002 Accuracy: 0.51037\n",
      "Epoch: 121 Batch:   0 Loss: 257.42206 Accuracy: 0.51349\n",
      "Epoch: 122 Batch:   0 Loss: 255.50517 Accuracy: 0.52282\n",
      "Epoch: 123 Batch:   0 Loss: 253.72855 Accuracy: 0.52282\n",
      "Epoch: 124 Batch:   0 Loss: 252.08203 Accuracy: 0.52593\n",
      "Epoch: 125 Batch:   0 Loss: 250.55589 Accuracy: 0.52697\n",
      "Epoch: 126 Batch:   0 Loss: 249.14160 Accuracy: 0.53527\n",
      "Epoch: 127 Batch:   0 Loss: 247.83052 Accuracy: 0.54357\n",
      "Epoch: 128 Batch:   0 Loss: 246.61494 Accuracy: 0.55083\n",
      "Epoch: 129 Batch:   0 Loss: 245.48770 Accuracy: 0.54772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 248.12094 Accuracy: 0.49554\n",
      "Epoch: 130 Batch:   0 Loss: 244.44221 Accuracy: 0.55083\n",
      "Epoch: 131 Batch:   0 Loss: 243.46843 Accuracy: 0.54979\n",
      "Epoch: 132 Batch:   0 Loss: 242.52629 Accuracy: 0.55498\n",
      "Epoch: 133 Batch:   0 Loss: 241.68901 Accuracy: 0.55602\n",
      "Epoch: 134 Batch:   0 Loss: 240.91435 Accuracy: 0.56846\n",
      "Epoch: 135 Batch:   0 Loss: 240.19415 Accuracy: 0.57158\n",
      "Epoch: 136 Batch:   0 Loss: 239.52411 Accuracy: 0.57365\n",
      "Epoch: 137 Batch:   0 Loss: 238.90012 Accuracy: 0.58402\n",
      "Epoch: 138 Batch:   0 Loss: 238.31854 Accuracy: 0.58506\n",
      "Epoch: 139 Batch:   0 Loss: 237.76915 Accuracy: 0.58921\n",
      "Test Loss: 242.28015 Accuracy: 0.51941\n",
      "Epoch: 140 Batch:   0 Loss: 237.25764 Accuracy: 0.58506\n",
      "Epoch: 141 Batch:   0 Loss: 236.77925 Accuracy: 0.58921\n",
      "Epoch: 142 Batch:   0 Loss: 236.33110 Accuracy: 0.59959\n",
      "Epoch: 143 Batch:   0 Loss: 235.91087 Accuracy: 0.60270\n",
      "Epoch: 144 Batch:   0 Loss: 235.51419 Accuracy: 0.60996\n",
      "Epoch: 145 Batch:   0 Loss: 235.14304 Accuracy: 0.61618\n",
      "Epoch: 146 Batch:   0 Loss: 234.79335 Accuracy: 0.61722\n",
      "Epoch: 147 Batch:   0 Loss: 234.46159 Accuracy: 0.61411\n",
      "Epoch: 148 Batch:   0 Loss: 234.15138 Accuracy: 0.61411\n",
      "Epoch: 149 Batch:   0 Loss: 233.85782 Accuracy: 0.61618\n",
      "Test Loss: 239.55492 Accuracy: 0.57779\n",
      "Epoch: 150 Batch:   0 Loss: 233.57951 Accuracy: 0.62448\n",
      "Epoch: 151 Batch:   0 Loss: 233.31543 Accuracy: 0.62656\n",
      "Epoch: 152 Batch:   0 Loss: 233.06418 Accuracy: 0.63174\n",
      "Epoch: 153 Batch:   0 Loss: 232.82465 Accuracy: 0.63071\n",
      "Epoch: 154 Batch:   0 Loss: 232.59599 Accuracy: 0.63174\n",
      "Epoch: 155 Batch:   0 Loss: 232.37730 Accuracy: 0.63278\n",
      "Epoch: 156 Batch:   0 Loss: 232.16768 Accuracy: 0.63174\n",
      "Epoch: 157 Batch:   0 Loss: 231.96640 Accuracy: 0.63382\n",
      "Epoch: 158 Batch:   0 Loss: 231.77283 Accuracy: 0.63693\n",
      "Epoch: 159 Batch:   0 Loss: 231.58626 Accuracy: 0.63797\n",
      "Test Loss: 238.10637 Accuracy: 0.59832\n",
      "Epoch: 160 Batch:   0 Loss: 231.40619 Accuracy: 0.64315\n",
      "Epoch: 161 Batch:   0 Loss: 231.23201 Accuracy: 0.64627\n",
      "Epoch: 162 Batch:   0 Loss: 231.06329 Accuracy: 0.64730\n",
      "Epoch: 163 Batch:   0 Loss: 230.89961 Accuracy: 0.64938\n",
      "Epoch: 164 Batch:   0 Loss: 230.74042 Accuracy: 0.65353\n",
      "Epoch: 165 Batch:   0 Loss: 230.58557 Accuracy: 0.65768\n",
      "Epoch: 166 Batch:   0 Loss: 230.43448 Accuracy: 0.65768\n",
      "Epoch: 167 Batch:   0 Loss: 230.28699 Accuracy: 0.65871\n",
      "Epoch: 168 Batch:   0 Loss: 230.14276 Accuracy: 0.66079\n",
      "Epoch: 169 Batch:   0 Loss: 229.99641 Accuracy: 0.66390\n",
      "Test Loss: 237.15914 Accuracy: 0.61024\n",
      "Epoch: 170 Batch:   0 Loss: 229.85762 Accuracy: 0.66390\n",
      "Epoch: 171 Batch:   0 Loss: 229.71881 Accuracy: 0.66390\n",
      "Epoch: 172 Batch:   0 Loss: 229.58463 Accuracy: 0.66390\n",
      "Epoch: 173 Batch:   0 Loss: 229.45261 Accuracy: 0.66701\n",
      "Epoch: 174 Batch:   0 Loss: 229.32248 Accuracy: 0.67012\n",
      "Epoch: 175 Batch:   0 Loss: 229.19421 Accuracy: 0.67116\n",
      "Epoch: 176 Batch:   0 Loss: 229.06766 Accuracy: 0.67220\n",
      "Epoch: 177 Batch:   0 Loss: 228.94269 Accuracy: 0.67220\n",
      "Epoch: 178 Batch:   0 Loss: 228.81908 Accuracy: 0.67324\n",
      "Epoch: 179 Batch:   0 Loss: 228.69685 Accuracy: 0.67531\n",
      "Test Loss: 236.39320 Accuracy: 0.61090\n",
      "Epoch: 180 Batch:   0 Loss: 228.57588 Accuracy: 0.67531\n",
      "Epoch: 181 Batch:   0 Loss: 228.45599 Accuracy: 0.67739\n",
      "Epoch: 182 Batch:   0 Loss: 228.33714 Accuracy: 0.67842\n",
      "Epoch: 183 Batch:   0 Loss: 228.21922 Accuracy: 0.67842\n",
      "Epoch: 184 Batch:   0 Loss: 228.10223 Accuracy: 0.67739\n",
      "Epoch: 185 Batch:   0 Loss: 227.98604 Accuracy: 0.67739\n",
      "Epoch: 186 Batch:   0 Loss: 227.87061 Accuracy: 0.67739\n",
      "Epoch: 187 Batch:   0 Loss: 227.75307 Accuracy: 0.67946\n",
      "Epoch: 188 Batch:   0 Loss: 227.63974 Accuracy: 0.67946\n",
      "Epoch: 189 Batch:   0 Loss: 227.52693 Accuracy: 0.68050\n",
      "Test Loss: 235.69683 Accuracy: 0.61302\n",
      "Epoch: 190 Batch:   0 Loss: 227.41473 Accuracy: 0.68154\n",
      "Epoch: 191 Batch:   0 Loss: 227.30293 Accuracy: 0.68050\n",
      "Epoch: 192 Batch:   0 Loss: 227.19167 Accuracy: 0.68050\n",
      "Epoch: 193 Batch:   0 Loss: 227.08081 Accuracy: 0.68154\n",
      "Epoch: 194 Batch:   0 Loss: 226.97031 Accuracy: 0.68154\n",
      "Epoch: 195 Batch:   0 Loss: 226.86006 Accuracy: 0.68050\n",
      "Epoch: 196 Batch:   0 Loss: 226.75021 Accuracy: 0.68154\n",
      "Epoch: 197 Batch:   0 Loss: 226.64064 Accuracy: 0.68154\n",
      "Epoch: 198 Batch:   0 Loss: 226.53127 Accuracy: 0.68154\n",
      "Epoch: 199 Batch:   0 Loss: 226.42215 Accuracy: 0.68154\n",
      "Test Loss: 235.02605 Accuracy: 0.61435\n",
      "Epoch: 200 Batch:   0 Loss: 226.31326 Accuracy: 0.68154\n",
      "Epoch: 201 Batch:   0 Loss: 226.20453 Accuracy: 0.68050\n",
      "Epoch: 202 Batch:   0 Loss: 226.09601 Accuracy: 0.68050\n",
      "Epoch: 203 Batch:   0 Loss: 225.98758 Accuracy: 0.68050\n",
      "Epoch: 204 Batch:   0 Loss: 225.87924 Accuracy: 0.68050\n",
      "Epoch: 205 Batch:   0 Loss: 225.77103 Accuracy: 0.68050\n",
      "Epoch: 206 Batch:   0 Loss: 225.66290 Accuracy: 0.68050\n",
      "Epoch: 207 Batch:   0 Loss: 225.55482 Accuracy: 0.68050\n",
      "Epoch: 208 Batch:   0 Loss: 225.44675 Accuracy: 0.68050\n",
      "Epoch: 209 Batch:   0 Loss: 225.33879 Accuracy: 0.68154\n",
      "Test Loss: 234.35308 Accuracy: 0.61527\n",
      "Epoch: 210 Batch:   0 Loss: 225.23076 Accuracy: 0.68154\n",
      "Epoch: 211 Batch:   0 Loss: 225.12286 Accuracy: 0.68154\n",
      "Epoch: 212 Batch:   0 Loss: 225.01488 Accuracy: 0.68154\n",
      "Epoch: 213 Batch:   0 Loss: 224.90558 Accuracy: 0.68154\n",
      "Epoch: 214 Batch:   0 Loss: 224.79678 Accuracy: 0.68154\n",
      "Epoch: 215 Batch:   0 Loss: 224.68571 Accuracy: 0.68257\n",
      "Epoch: 216 Batch:   0 Loss: 224.57614 Accuracy: 0.68154\n",
      "Epoch: 217 Batch:   0 Loss: 224.46655 Accuracy: 0.68050\n",
      "Epoch: 218 Batch:   0 Loss: 224.35686 Accuracy: 0.68050\n",
      "Epoch: 219 Batch:   0 Loss: 224.24710 Accuracy: 0.68050\n",
      "Test Loss: 233.63471 Accuracy: 0.61620\n",
      "Epoch: 220 Batch:   0 Loss: 224.13419 Accuracy: 0.68154\n",
      "Epoch: 221 Batch:   0 Loss: 224.02348 Accuracy: 0.68050\n",
      "Epoch: 222 Batch:   0 Loss: 223.91273 Accuracy: 0.68050\n",
      "Epoch: 223 Batch:   0 Loss: 223.80182 Accuracy: 0.68154\n",
      "Epoch: 224 Batch:   0 Loss: 223.69073 Accuracy: 0.68154\n",
      "Epoch: 225 Batch:   0 Loss: 223.57951 Accuracy: 0.68154\n",
      "Epoch: 226 Batch:   0 Loss: 223.46820 Accuracy: 0.68154\n",
      "Epoch: 227 Batch:   0 Loss: 223.35663 Accuracy: 0.68154\n",
      "Epoch: 228 Batch:   0 Loss: 223.24490 Accuracy: 0.68257\n",
      "Epoch: 229 Batch:   0 Loss: 223.13303 Accuracy: 0.68257\n",
      "Test Loss: 232.88102 Accuracy: 0.61673\n",
      "Epoch: 230 Batch:   0 Loss: 223.01945 Accuracy: 0.68257\n",
      "Epoch: 231 Batch:   0 Loss: 222.90536 Accuracy: 0.68257\n",
      "Epoch: 232 Batch:   0 Loss: 222.79337 Accuracy: 0.68257\n",
      "Epoch: 233 Batch:   0 Loss: 222.68115 Accuracy: 0.68257\n",
      "Epoch: 234 Batch:   0 Loss: 222.56860 Accuracy: 0.68257\n",
      "Epoch: 235 Batch:   0 Loss: 222.45593 Accuracy: 0.68257\n",
      "Epoch: 236 Batch:   0 Loss: 222.33714 Accuracy: 0.68257\n",
      "Epoch: 237 Batch:   0 Loss: 222.22136 Accuracy: 0.68257\n",
      "Epoch: 238 Batch:   0 Loss: 222.10629 Accuracy: 0.68257\n",
      "Epoch: 239 Batch:   0 Loss: 221.99089 Accuracy: 0.68257\n",
      "Test Loss: 232.10879 Accuracy: 0.61739\n",
      "Epoch: 240 Batch:   0 Loss: 221.87529 Accuracy: 0.68257\n",
      "Epoch: 241 Batch:   0 Loss: 221.75847 Accuracy: 0.68257\n",
      "Epoch: 242 Batch:   0 Loss: 221.64163 Accuracy: 0.68257\n",
      "Epoch: 243 Batch:   0 Loss: 221.52032 Accuracy: 0.68257\n",
      "Epoch: 244 Batch:   0 Loss: 221.39940 Accuracy: 0.68257\n",
      "Epoch: 245 Batch:   0 Loss: 221.27814 Accuracy: 0.68257\n",
      "Epoch: 246 Batch:   0 Loss: 221.15665 Accuracy: 0.68257\n",
      "Epoch: 247 Batch:   0 Loss: 221.03484 Accuracy: 0.68257\n",
      "Epoch: 248 Batch:   0 Loss: 220.91278 Accuracy: 0.68154\n",
      "Epoch: 249 Batch:   0 Loss: 220.79039 Accuracy: 0.68154\n",
      "Test Loss: 231.27050 Accuracy: 0.61752\n",
      "Epoch: 250 Batch:   0 Loss: 220.66766 Accuracy: 0.68154\n",
      "Epoch: 251 Batch:   0 Loss: 220.54465 Accuracy: 0.68154\n",
      "Epoch: 252 Batch:   0 Loss: 220.42091 Accuracy: 0.68154\n",
      "Epoch: 253 Batch:   0 Loss: 220.29727 Accuracy: 0.68154\n",
      "Epoch: 254 Batch:   0 Loss: 220.17329 Accuracy: 0.68154\n",
      "Epoch: 255 Batch:   0 Loss: 220.04900 Accuracy: 0.68154\n",
      "Epoch: 256 Batch:   0 Loss: 219.92438 Accuracy: 0.68050\n",
      "Epoch: 257 Batch:   0 Loss: 219.79941 Accuracy: 0.68050\n",
      "Epoch: 258 Batch:   0 Loss: 219.67409 Accuracy: 0.68050\n",
      "Epoch: 259 Batch:   0 Loss: 219.54839 Accuracy: 0.68050\n",
      "Test Loss: 230.38426 Accuracy: 0.61818\n",
      "Epoch: 260 Batch:   0 Loss: 219.42235 Accuracy: 0.68050\n",
      "Epoch: 261 Batch:   0 Loss: 219.29591 Accuracy: 0.68050\n",
      "Epoch: 262 Batch:   0 Loss: 219.16910 Accuracy: 0.68050\n",
      "Epoch: 263 Batch:   0 Loss: 219.04196 Accuracy: 0.68154\n",
      "Epoch: 264 Batch:   0 Loss: 218.91440 Accuracy: 0.68257\n",
      "Epoch: 265 Batch:   0 Loss: 218.78642 Accuracy: 0.68257\n",
      "Epoch: 266 Batch:   0 Loss: 218.65810 Accuracy: 0.68257\n",
      "Epoch: 267 Batch:   0 Loss: 218.52937 Accuracy: 0.68361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 268 Batch:   0 Loss: 218.40016 Accuracy: 0.68361\n",
      "Epoch: 269 Batch:   0 Loss: 218.27057 Accuracy: 0.68465\n",
      "Test Loss: 229.44937 Accuracy: 0.61911\n",
      "Epoch: 270 Batch:   0 Loss: 218.14020 Accuracy: 0.68361\n",
      "Epoch: 271 Batch:   0 Loss: 218.00954 Accuracy: 0.68361\n",
      "Epoch: 272 Batch:   0 Loss: 217.87834 Accuracy: 0.68568\n",
      "Epoch: 273 Batch:   0 Loss: 217.74568 Accuracy: 0.68672\n",
      "Epoch: 274 Batch:   0 Loss: 217.61333 Accuracy: 0.68672\n",
      "Epoch: 275 Batch:   0 Loss: 217.48044 Accuracy: 0.68672\n",
      "Epoch: 276 Batch:   0 Loss: 217.34714 Accuracy: 0.68672\n",
      "Epoch: 277 Batch:   0 Loss: 217.21292 Accuracy: 0.68672\n",
      "Epoch: 278 Batch:   0 Loss: 217.07838 Accuracy: 0.68672\n",
      "Epoch: 279 Batch:   0 Loss: 216.94331 Accuracy: 0.68672\n",
      "Test Loss: 228.45618 Accuracy: 0.62003\n",
      "Epoch: 280 Batch:   0 Loss: 216.80774 Accuracy: 0.68672\n",
      "Epoch: 281 Batch:   0 Loss: 216.67168 Accuracy: 0.68672\n",
      "Epoch: 282 Batch:   0 Loss: 216.53508 Accuracy: 0.68672\n",
      "Epoch: 283 Batch:   0 Loss: 216.39487 Accuracy: 0.68672\n",
      "Epoch: 284 Batch:   0 Loss: 216.25758 Accuracy: 0.68672\n",
      "Epoch: 285 Batch:   0 Loss: 216.11984 Accuracy: 0.68880\n",
      "Epoch: 286 Batch:   0 Loss: 215.98158 Accuracy: 0.68880\n",
      "Epoch: 287 Batch:   0 Loss: 215.84273 Accuracy: 0.68983\n",
      "Epoch: 288 Batch:   0 Loss: 215.70338 Accuracy: 0.68983\n",
      "Epoch: 289 Batch:   0 Loss: 215.56345 Accuracy: 0.68983\n",
      "Test Loss: 227.38775 Accuracy: 0.62175\n",
      "Epoch: 290 Batch:   0 Loss: 215.42259 Accuracy: 0.68983\n",
      "Epoch: 291 Batch:   0 Loss: 215.28136 Accuracy: 0.68983\n",
      "Epoch: 292 Batch:   0 Loss: 215.13943 Accuracy: 0.69087\n",
      "Epoch: 293 Batch:   0 Loss: 214.99692 Accuracy: 0.69087\n",
      "Epoch: 294 Batch:   0 Loss: 214.85379 Accuracy: 0.69191\n",
      "Epoch: 295 Batch:   0 Loss: 214.71008 Accuracy: 0.69191\n",
      "Epoch: 296 Batch:   0 Loss: 214.56577 Accuracy: 0.69191\n",
      "Epoch: 297 Batch:   0 Loss: 214.42079 Accuracy: 0.69191\n",
      "Epoch: 298 Batch:   0 Loss: 214.27510 Accuracy: 0.69191\n",
      "Epoch: 299 Batch:   0 Loss: 214.12889 Accuracy: 0.69191\n",
      "Test Loss: 226.24950 Accuracy: 0.62440\n",
      "Epoch: 300 Batch:   0 Loss: 213.98085 Accuracy: 0.69191\n",
      "Epoch: 301 Batch:   0 Loss: 213.83339 Accuracy: 0.69191\n",
      "Epoch: 302 Batch:   0 Loss: 213.68524 Accuracy: 0.69191\n",
      "Epoch: 303 Batch:   0 Loss: 213.53627 Accuracy: 0.69295\n",
      "Epoch: 304 Batch:   0 Loss: 213.38506 Accuracy: 0.69398\n",
      "Epoch: 305 Batch:   0 Loss: 213.23337 Accuracy: 0.69398\n",
      "Epoch: 306 Batch:   0 Loss: 213.08098 Accuracy: 0.69398\n",
      "Epoch: 307 Batch:   0 Loss: 212.92789 Accuracy: 0.69398\n",
      "Epoch: 308 Batch:   0 Loss: 212.77411 Accuracy: 0.69398\n",
      "Epoch: 309 Batch:   0 Loss: 212.61958 Accuracy: 0.69398\n",
      "Test Loss: 225.01360 Accuracy: 0.62850\n",
      "Epoch: 310 Batch:   0 Loss: 212.46434 Accuracy: 0.69398\n",
      "Epoch: 311 Batch:   0 Loss: 212.30830 Accuracy: 0.69398\n",
      "Epoch: 312 Batch:   0 Loss: 212.15152 Accuracy: 0.69398\n",
      "Epoch: 313 Batch:   0 Loss: 211.99397 Accuracy: 0.69606\n",
      "Epoch: 314 Batch:   0 Loss: 211.83569 Accuracy: 0.69606\n",
      "Epoch: 315 Batch:   0 Loss: 211.67659 Accuracy: 0.69606\n",
      "Epoch: 316 Batch:   0 Loss: 211.51669 Accuracy: 0.69606\n",
      "Epoch: 317 Batch:   0 Loss: 211.35530 Accuracy: 0.69606\n",
      "Epoch: 318 Batch:   0 Loss: 211.19328 Accuracy: 0.69502\n",
      "Epoch: 319 Batch:   0 Loss: 211.03041 Accuracy: 0.69502\n",
      "Test Loss: 223.69219 Accuracy: 0.63115\n",
      "Epoch: 320 Batch:   0 Loss: 210.86670 Accuracy: 0.69502\n",
      "Epoch: 321 Batch:   0 Loss: 210.70042 Accuracy: 0.69502\n",
      "Epoch: 322 Batch:   0 Loss: 210.53474 Accuracy: 0.69502\n",
      "Epoch: 323 Batch:   0 Loss: 210.36818 Accuracy: 0.69502\n",
      "Epoch: 324 Batch:   0 Loss: 210.20055 Accuracy: 0.69710\n",
      "Epoch: 325 Batch:   0 Loss: 210.03235 Accuracy: 0.69813\n",
      "Epoch: 326 Batch:   0 Loss: 209.86322 Accuracy: 0.69917\n",
      "Epoch: 327 Batch:   0 Loss: 209.69327 Accuracy: 0.69917\n",
      "Epoch: 328 Batch:   0 Loss: 209.52234 Accuracy: 0.70021\n",
      "Epoch: 329 Batch:   0 Loss: 209.34929 Accuracy: 0.70021\n",
      "Test Loss: 222.27467 Accuracy: 0.63485\n",
      "Epoch: 330 Batch:   0 Loss: 209.17645 Accuracy: 0.70021\n",
      "Epoch: 331 Batch:   0 Loss: 209.00269 Accuracy: 0.70021\n",
      "Epoch: 332 Batch:   0 Loss: 208.82791 Accuracy: 0.70021\n",
      "Epoch: 333 Batch:   0 Loss: 208.65225 Accuracy: 0.70228\n",
      "Epoch: 334 Batch:   0 Loss: 208.47560 Accuracy: 0.70332\n",
      "Epoch: 335 Batch:   0 Loss: 208.29802 Accuracy: 0.70332\n",
      "Epoch: 336 Batch:   0 Loss: 208.11945 Accuracy: 0.70332\n",
      "Epoch: 337 Batch:   0 Loss: 207.93983 Accuracy: 0.70332\n",
      "Epoch: 338 Batch:   0 Loss: 207.75932 Accuracy: 0.70332\n",
      "Epoch: 339 Batch:   0 Loss: 207.57779 Accuracy: 0.70332\n",
      "Test Loss: 220.76782 Accuracy: 0.63882\n",
      "Epoch: 340 Batch:   0 Loss: 207.39488 Accuracy: 0.70332\n",
      "Epoch: 341 Batch:   0 Loss: 207.21104 Accuracy: 0.70436\n",
      "Epoch: 342 Batch:   0 Loss: 207.02614 Accuracy: 0.70539\n",
      "Epoch: 343 Batch:   0 Loss: 206.84018 Accuracy: 0.70539\n",
      "Epoch: 344 Batch:   0 Loss: 206.65321 Accuracy: 0.70747\n",
      "Epoch: 345 Batch:   0 Loss: 206.46519 Accuracy: 0.70747\n",
      "Epoch: 346 Batch:   0 Loss: 206.27615 Accuracy: 0.70747\n",
      "Epoch: 347 Batch:   0 Loss: 206.08459 Accuracy: 0.70747\n",
      "Epoch: 348 Batch:   0 Loss: 205.89296 Accuracy: 0.70747\n",
      "Epoch: 349 Batch:   0 Loss: 205.69971 Accuracy: 0.70747\n",
      "Test Loss: 219.16296 Accuracy: 0.64491\n",
      "Epoch: 350 Batch:   0 Loss: 205.50279 Accuracy: 0.70851\n",
      "Epoch: 351 Batch:   0 Loss: 205.30629 Accuracy: 0.70851\n",
      "Epoch: 352 Batch:   0 Loss: 205.10844 Accuracy: 0.70851\n",
      "Epoch: 353 Batch:   0 Loss: 204.90955 Accuracy: 0.70851\n",
      "Epoch: 354 Batch:   0 Loss: 204.70959 Accuracy: 0.70851\n",
      "Epoch: 355 Batch:   0 Loss: 204.50839 Accuracy: 0.70851\n",
      "Epoch: 356 Batch:   0 Loss: 204.30515 Accuracy: 0.70954\n",
      "Epoch: 357 Batch:   0 Loss: 204.10083 Accuracy: 0.71058\n",
      "Epoch: 358 Batch:   0 Loss: 203.89528 Accuracy: 0.71058\n",
      "Epoch: 359 Batch:   0 Loss: 203.68779 Accuracy: 0.71162\n",
      "Test Loss: 217.47005 Accuracy: 0.65338\n",
      "Epoch: 360 Batch:   0 Loss: 203.47925 Accuracy: 0.71162\n",
      "Epoch: 361 Batch:   0 Loss: 203.26926 Accuracy: 0.71266\n",
      "Epoch: 362 Batch:   0 Loss: 203.05794 Accuracy: 0.71473\n",
      "Epoch: 363 Batch:   0 Loss: 202.84535 Accuracy: 0.71473\n",
      "Epoch: 364 Batch:   0 Loss: 202.63155 Accuracy: 0.71680\n",
      "Epoch: 365 Batch:   0 Loss: 202.41647 Accuracy: 0.71784\n",
      "Epoch: 366 Batch:   0 Loss: 202.19923 Accuracy: 0.71784\n",
      "Epoch: 367 Batch:   0 Loss: 201.98074 Accuracy: 0.71784\n",
      "Epoch: 368 Batch:   0 Loss: 201.76099 Accuracy: 0.71888\n",
      "Epoch: 369 Batch:   0 Loss: 201.53954 Accuracy: 0.71888\n",
      "Test Loss: 215.66565 Accuracy: 0.66197\n",
      "Epoch: 370 Batch:   0 Loss: 201.31685 Accuracy: 0.71888\n",
      "Epoch: 371 Batch:   0 Loss: 201.09291 Accuracy: 0.71888\n",
      "Epoch: 372 Batch:   0 Loss: 200.86513 Accuracy: 0.71888\n",
      "Epoch: 373 Batch:   0 Loss: 200.63820 Accuracy: 0.71992\n",
      "Epoch: 374 Batch:   0 Loss: 200.41005 Accuracy: 0.71992\n",
      "Epoch: 375 Batch:   0 Loss: 200.18042 Accuracy: 0.71992\n",
      "Epoch: 376 Batch:   0 Loss: 199.94847 Accuracy: 0.72095\n",
      "Epoch: 377 Batch:   0 Loss: 199.71548 Accuracy: 0.72303\n",
      "Epoch: 378 Batch:   0 Loss: 199.48109 Accuracy: 0.72303\n",
      "Epoch: 379 Batch:   0 Loss: 199.24536 Accuracy: 0.72303\n",
      "Test Loss: 213.74484 Accuracy: 0.67004\n",
      "Epoch: 380 Batch:   0 Loss: 199.00827 Accuracy: 0.72303\n",
      "Epoch: 381 Batch:   0 Loss: 198.76982 Accuracy: 0.72510\n",
      "Epoch: 382 Batch:   0 Loss: 198.52995 Accuracy: 0.72510\n",
      "Epoch: 383 Batch:   0 Loss: 198.28728 Accuracy: 0.72510\n",
      "Epoch: 384 Batch:   0 Loss: 198.04407 Accuracy: 0.72614\n",
      "Epoch: 385 Batch:   0 Loss: 197.79942 Accuracy: 0.72822\n",
      "Epoch: 386 Batch:   0 Loss: 197.55347 Accuracy: 0.73133\n",
      "Epoch: 387 Batch:   0 Loss: 197.30603 Accuracy: 0.73133\n",
      "Epoch: 388 Batch:   0 Loss: 197.05725 Accuracy: 0.73237\n",
      "Epoch: 389 Batch:   0 Loss: 196.80693 Accuracy: 0.73444\n",
      "Test Loss: 211.70098 Accuracy: 0.67652\n",
      "Epoch: 390 Batch:   0 Loss: 196.55525 Accuracy: 0.73444\n",
      "Epoch: 391 Batch:   0 Loss: 196.30162 Accuracy: 0.73444\n",
      "Epoch: 392 Batch:   0 Loss: 196.04770 Accuracy: 0.73548\n",
      "Epoch: 393 Batch:   0 Loss: 195.79173 Accuracy: 0.73651\n",
      "Epoch: 394 Batch:   0 Loss: 195.53436 Accuracy: 0.73755\n",
      "Epoch: 395 Batch:   0 Loss: 195.27556 Accuracy: 0.73651\n",
      "Epoch: 396 Batch:   0 Loss: 195.01302 Accuracy: 0.73859\n",
      "Epoch: 397 Batch:   0 Loss: 194.74898 Accuracy: 0.74170\n",
      "Epoch: 398 Batch:   0 Loss: 194.48296 Accuracy: 0.74170\n",
      "Epoch: 399 Batch:   0 Loss: 194.21568 Accuracy: 0.74170\n",
      "Test Loss: 209.52428 Accuracy: 0.68499\n",
      "Epoch: 400 Batch:   0 Loss: 193.94696 Accuracy: 0.74274\n",
      "Epoch: 401 Batch:   0 Loss: 193.67683 Accuracy: 0.74378\n",
      "Epoch: 402 Batch:   0 Loss: 193.40523 Accuracy: 0.74378\n",
      "Epoch: 403 Batch:   0 Loss: 193.13206 Accuracy: 0.74585\n",
      "Epoch: 404 Batch:   0 Loss: 192.85745 Accuracy: 0.74585\n",
      "Epoch: 405 Batch:   0 Loss: 192.58134 Accuracy: 0.74689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 406 Batch:   0 Loss: 192.30379 Accuracy: 0.74896\n",
      "Epoch: 407 Batch:   0 Loss: 192.02461 Accuracy: 0.75104\n",
      "Epoch: 408 Batch:   0 Loss: 191.74397 Accuracy: 0.75104\n",
      "Epoch: 409 Batch:   0 Loss: 191.46185 Accuracy: 0.75104\n",
      "Test Loss: 207.21646 Accuracy: 0.69756\n",
      "Epoch: 410 Batch:   0 Loss: 191.17833 Accuracy: 0.75207\n",
      "Epoch: 411 Batch:   0 Loss: 190.89317 Accuracy: 0.75207\n",
      "Epoch: 412 Batch:   0 Loss: 190.60661 Accuracy: 0.75519\n",
      "Epoch: 413 Batch:   0 Loss: 190.31862 Accuracy: 0.75415\n",
      "Epoch: 414 Batch:   0 Loss: 190.02911 Accuracy: 0.75622\n",
      "Epoch: 415 Batch:   0 Loss: 189.73817 Accuracy: 0.75622\n",
      "Epoch: 416 Batch:   0 Loss: 189.44516 Accuracy: 0.75726\n",
      "Epoch: 417 Batch:   0 Loss: 189.15073 Accuracy: 0.75830\n",
      "Epoch: 418 Batch:   0 Loss: 188.85408 Accuracy: 0.76141\n",
      "Epoch: 419 Batch:   0 Loss: 188.55598 Accuracy: 0.76349\n",
      "Test Loss: 204.79952 Accuracy: 0.70721\n",
      "Epoch: 420 Batch:   0 Loss: 188.25641 Accuracy: 0.76452\n",
      "Epoch: 421 Batch:   0 Loss: 187.95532 Accuracy: 0.76556\n",
      "Epoch: 422 Batch:   0 Loss: 187.65298 Accuracy: 0.76556\n",
      "Epoch: 423 Batch:   0 Loss: 187.34933 Accuracy: 0.76556\n",
      "Epoch: 424 Batch:   0 Loss: 187.04367 Accuracy: 0.76556\n",
      "Epoch: 425 Batch:   0 Loss: 186.73679 Accuracy: 0.76660\n",
      "Epoch: 426 Batch:   0 Loss: 186.42851 Accuracy: 0.76763\n",
      "Epoch: 427 Batch:   0 Loss: 186.11876 Accuracy: 0.76763\n",
      "Epoch: 428 Batch:   0 Loss: 185.80754 Accuracy: 0.76971\n",
      "Epoch: 429 Batch:   0 Loss: 185.49434 Accuracy: 0.77075\n",
      "Test Loss: 202.28356 Accuracy: 0.71674\n",
      "Epoch: 430 Batch:   0 Loss: 185.18007 Accuracy: 0.77075\n",
      "Epoch: 431 Batch:   0 Loss: 184.86443 Accuracy: 0.77282\n",
      "Epoch: 432 Batch:   0 Loss: 184.54739 Accuracy: 0.77490\n",
      "Epoch: 433 Batch:   0 Loss: 184.22897 Accuracy: 0.77593\n",
      "Epoch: 434 Batch:   0 Loss: 183.90916 Accuracy: 0.78008\n",
      "Epoch: 435 Batch:   0 Loss: 183.58797 Accuracy: 0.78320\n",
      "Epoch: 436 Batch:   0 Loss: 183.26544 Accuracy: 0.78216\n",
      "Epoch: 437 Batch:   0 Loss: 182.94115 Accuracy: 0.78320\n",
      "Epoch: 438 Batch:   0 Loss: 182.61607 Accuracy: 0.78320\n",
      "Epoch: 439 Batch:   0 Loss: 182.28949 Accuracy: 0.78423\n",
      "Test Loss: 199.68566 Accuracy: 0.72838\n",
      "Epoch: 440 Batch:   0 Loss: 181.96156 Accuracy: 0.78527\n",
      "Epoch: 441 Batch:   0 Loss: 181.63245 Accuracy: 0.78838\n",
      "Epoch: 442 Batch:   0 Loss: 181.30188 Accuracy: 0.79149\n",
      "Epoch: 443 Batch:   0 Loss: 180.96999 Accuracy: 0.79253\n",
      "Epoch: 444 Batch:   0 Loss: 180.63705 Accuracy: 0.79357\n",
      "Epoch: 445 Batch:   0 Loss: 180.30289 Accuracy: 0.79357\n",
      "Epoch: 446 Batch:   0 Loss: 179.96738 Accuracy: 0.79461\n",
      "Epoch: 447 Batch:   0 Loss: 179.63062 Accuracy: 0.79564\n",
      "Epoch: 448 Batch:   0 Loss: 179.29271 Accuracy: 0.79564\n",
      "Epoch: 449 Batch:   0 Loss: 178.95277 Accuracy: 0.79668\n",
      "Test Loss: 197.02439 Accuracy: 0.73711\n",
      "Epoch: 450 Batch:   0 Loss: 178.61272 Accuracy: 0.79668\n",
      "Epoch: 451 Batch:   0 Loss: 178.27144 Accuracy: 0.79772\n",
      "Epoch: 452 Batch:   0 Loss: 177.92906 Accuracy: 0.79979\n",
      "Epoch: 453 Batch:   0 Loss: 177.58435 Accuracy: 0.79979\n",
      "Epoch: 454 Batch:   0 Loss: 177.24004 Accuracy: 0.79979\n",
      "Epoch: 455 Batch:   0 Loss: 176.89462 Accuracy: 0.79979\n",
      "Epoch: 456 Batch:   0 Loss: 176.54814 Accuracy: 0.79979\n",
      "Epoch: 457 Batch:   0 Loss: 176.20053 Accuracy: 0.79979\n",
      "Epoch: 458 Batch:   0 Loss: 175.85193 Accuracy: 0.79979\n",
      "Epoch: 459 Batch:   0 Loss: 175.50221 Accuracy: 0.80083\n",
      "Test Loss: 194.30054 Accuracy: 0.74373\n",
      "Epoch: 460 Batch:   0 Loss: 175.15149 Accuracy: 0.80083\n",
      "Epoch: 461 Batch:   0 Loss: 174.79976 Accuracy: 0.80083\n",
      "Epoch: 462 Batch:   0 Loss: 174.44647 Accuracy: 0.80290\n",
      "Epoch: 463 Batch:   0 Loss: 174.09236 Accuracy: 0.80394\n",
      "Epoch: 464 Batch:   0 Loss: 173.73605 Accuracy: 0.80705\n",
      "Epoch: 465 Batch:   0 Loss: 173.37906 Accuracy: 0.80809\n",
      "Epoch: 466 Batch:   0 Loss: 173.02112 Accuracy: 0.81017\n",
      "Epoch: 467 Batch:   0 Loss: 172.66231 Accuracy: 0.81120\n",
      "Epoch: 468 Batch:   0 Loss: 172.30251 Accuracy: 0.81120\n",
      "Epoch: 469 Batch:   0 Loss: 171.94043 Accuracy: 0.81120\n",
      "Test Loss: 191.54887 Accuracy: 0.75127\n",
      "Epoch: 470 Batch:   0 Loss: 171.57753 Accuracy: 0.81120\n",
      "Epoch: 471 Batch:   0 Loss: 171.21397 Accuracy: 0.81328\n",
      "Epoch: 472 Batch:   0 Loss: 170.84947 Accuracy: 0.81432\n",
      "Epoch: 473 Batch:   0 Loss: 170.48431 Accuracy: 0.81535\n",
      "Epoch: 474 Batch:   0 Loss: 170.11844 Accuracy: 0.81535\n",
      "Epoch: 475 Batch:   0 Loss: 169.75169 Accuracy: 0.81639\n",
      "Epoch: 476 Batch:   0 Loss: 169.38408 Accuracy: 0.81639\n",
      "Epoch: 477 Batch:   0 Loss: 169.01588 Accuracy: 0.81846\n",
      "Epoch: 478 Batch:   0 Loss: 168.64699 Accuracy: 0.81846\n",
      "Epoch: 479 Batch:   0 Loss: 168.27742 Accuracy: 0.82054\n",
      "Test Loss: 188.78529 Accuracy: 0.76026\n",
      "Epoch: 480 Batch:   0 Loss: 167.90723 Accuracy: 0.82158\n",
      "Epoch: 481 Batch:   0 Loss: 167.53577 Accuracy: 0.82158\n",
      "Epoch: 482 Batch:   0 Loss: 167.16386 Accuracy: 0.82054\n",
      "Epoch: 483 Batch:   0 Loss: 166.79147 Accuracy: 0.82365\n",
      "Epoch: 484 Batch:   0 Loss: 166.41859 Accuracy: 0.82573\n",
      "Epoch: 485 Batch:   0 Loss: 166.04506 Accuracy: 0.82676\n",
      "Epoch: 486 Batch:   0 Loss: 165.67096 Accuracy: 0.82780\n",
      "Epoch: 487 Batch:   0 Loss: 165.29597 Accuracy: 0.82884\n",
      "Epoch: 488 Batch:   0 Loss: 164.92056 Accuracy: 0.83091\n",
      "Epoch: 489 Batch:   0 Loss: 164.54469 Accuracy: 0.83195\n",
      "Test Loss: 186.03806 Accuracy: 0.76661\n",
      "Epoch: 490 Batch:   0 Loss: 164.16814 Accuracy: 0.83299\n",
      "Epoch: 491 Batch:   0 Loss: 163.79158 Accuracy: 0.83299\n",
      "Epoch: 492 Batch:   0 Loss: 163.41460 Accuracy: 0.83610\n",
      "Epoch: 493 Batch:   0 Loss: 163.03723 Accuracy: 0.83714\n",
      "Epoch: 494 Batch:   0 Loss: 162.65945 Accuracy: 0.83817\n",
      "Epoch: 495 Batch:   0 Loss: 162.28087 Accuracy: 0.83921\n",
      "Epoch: 496 Batch:   0 Loss: 161.90254 Accuracy: 0.83921\n",
      "Epoch: 497 Batch:   0 Loss: 161.52390 Accuracy: 0.84025\n",
      "Epoch: 498 Batch:   0 Loss: 161.14426 Accuracy: 0.84025\n",
      "Epoch: 499 Batch:   0 Loss: 160.76479 Accuracy: 0.84025\n",
      "Test Loss: 183.34616 Accuracy: 0.77005\n",
      "Epoch: 500 Batch:   0 Loss: 160.38507 Accuracy: 0.84025\n",
      "Epoch: 501 Batch:   0 Loss: 160.00519 Accuracy: 0.84025\n",
      "Epoch: 502 Batch:   0 Loss: 159.62508 Accuracy: 0.84232\n",
      "Epoch: 503 Batch:   0 Loss: 159.24429 Accuracy: 0.84232\n",
      "Epoch: 504 Batch:   0 Loss: 158.86366 Accuracy: 0.84232\n",
      "Epoch: 505 Batch:   0 Loss: 158.48279 Accuracy: 0.84440\n",
      "Epoch: 506 Batch:   0 Loss: 158.10190 Accuracy: 0.84440\n",
      "Epoch: 507 Batch:   0 Loss: 157.72090 Accuracy: 0.84440\n",
      "Epoch: 508 Batch:   0 Loss: 157.33987 Accuracy: 0.84544\n",
      "Epoch: 509 Batch:   0 Loss: 156.95871 Accuracy: 0.84647\n",
      "Test Loss: 180.72546 Accuracy: 0.77323\n",
      "Epoch: 510 Batch:   0 Loss: 156.57758 Accuracy: 0.84751\n",
      "Epoch: 511 Batch:   0 Loss: 156.19516 Accuracy: 0.84647\n",
      "Epoch: 512 Batch:   0 Loss: 155.81400 Accuracy: 0.84647\n",
      "Epoch: 513 Batch:   0 Loss: 155.43367 Accuracy: 0.84647\n",
      "Epoch: 514 Batch:   0 Loss: 155.05276 Accuracy: 0.84647\n",
      "Epoch: 515 Batch:   0 Loss: 154.67212 Accuracy: 0.84647\n",
      "Epoch: 516 Batch:   0 Loss: 154.29152 Accuracy: 0.84751\n",
      "Epoch: 517 Batch:   0 Loss: 153.91103 Accuracy: 0.84959\n",
      "Epoch: 518 Batch:   0 Loss: 153.53069 Accuracy: 0.85062\n",
      "Epoch: 519 Batch:   0 Loss: 153.14983 Accuracy: 0.85373\n",
      "Test Loss: 178.20457 Accuracy: 0.77706\n",
      "Epoch: 520 Batch:   0 Loss: 152.76909 Accuracy: 0.85477\n",
      "Epoch: 521 Batch:   0 Loss: 152.38835 Accuracy: 0.85477\n",
      "Epoch: 522 Batch:   0 Loss: 152.00858 Accuracy: 0.85581\n",
      "Epoch: 523 Batch:   0 Loss: 151.62900 Accuracy: 0.85581\n",
      "Epoch: 524 Batch:   0 Loss: 151.24905 Accuracy: 0.85581\n",
      "Epoch: 525 Batch:   0 Loss: 150.86949 Accuracy: 0.85581\n",
      "Epoch: 526 Batch:   0 Loss: 150.49008 Accuracy: 0.85581\n",
      "Epoch: 527 Batch:   0 Loss: 150.11130 Accuracy: 0.85685\n",
      "Epoch: 528 Batch:   0 Loss: 149.73306 Accuracy: 0.85788\n",
      "Epoch: 529 Batch:   0 Loss: 149.35527 Accuracy: 0.85788\n",
      "Test Loss: 175.80202 Accuracy: 0.78169\n",
      "Epoch: 530 Batch:   0 Loss: 148.97775 Accuracy: 0.85788\n",
      "Epoch: 531 Batch:   0 Loss: 148.60075 Accuracy: 0.85788\n",
      "Epoch: 532 Batch:   0 Loss: 148.22359 Accuracy: 0.85788\n",
      "Epoch: 533 Batch:   0 Loss: 147.84709 Accuracy: 0.85788\n",
      "Epoch: 534 Batch:   0 Loss: 147.47108 Accuracy: 0.85892\n",
      "Epoch: 535 Batch:   0 Loss: 147.09550 Accuracy: 0.85892\n",
      "Epoch: 536 Batch:   0 Loss: 146.72047 Accuracy: 0.85996\n",
      "Epoch: 537 Batch:   0 Loss: 146.34570 Accuracy: 0.85996\n",
      "Epoch: 538 Batch:   0 Loss: 145.97202 Accuracy: 0.86203\n",
      "Epoch: 539 Batch:   0 Loss: 145.59888 Accuracy: 0.86411\n",
      "Test Loss: 173.55009 Accuracy: 0.78434\n",
      "Epoch: 540 Batch:   0 Loss: 145.22629 Accuracy: 0.86411\n",
      "Epoch: 541 Batch:   0 Loss: 144.85385 Accuracy: 0.86411\n",
      "Epoch: 542 Batch:   0 Loss: 144.48244 Accuracy: 0.86515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 543 Batch:   0 Loss: 144.11165 Accuracy: 0.86618\n",
      "Epoch: 544 Batch:   0 Loss: 143.74059 Accuracy: 0.86618\n",
      "Epoch: 545 Batch:   0 Loss: 143.37239 Accuracy: 0.86618\n",
      "Epoch: 546 Batch:   0 Loss: 143.00478 Accuracy: 0.86618\n",
      "Epoch: 547 Batch:   0 Loss: 142.63782 Accuracy: 0.86722\n",
      "Epoch: 548 Batch:   0 Loss: 142.27159 Accuracy: 0.86722\n",
      "Epoch: 549 Batch:   0 Loss: 141.90604 Accuracy: 0.86722\n",
      "Test Loss: 171.47847 Accuracy: 0.78646\n",
      "Epoch: 550 Batch:   0 Loss: 141.54085 Accuracy: 0.86722\n",
      "Epoch: 551 Batch:   0 Loss: 141.17703 Accuracy: 0.86722\n",
      "Epoch: 552 Batch:   0 Loss: 140.81406 Accuracy: 0.86722\n",
      "Epoch: 553 Batch:   0 Loss: 140.45151 Accuracy: 0.86722\n",
      "Epoch: 554 Batch:   0 Loss: 140.08975 Accuracy: 0.86722\n",
      "Epoch: 555 Batch:   0 Loss: 139.72899 Accuracy: 0.86722\n",
      "Epoch: 556 Batch:   0 Loss: 139.36913 Accuracy: 0.86722\n",
      "Epoch: 557 Batch:   0 Loss: 139.00999 Accuracy: 0.86722\n",
      "Epoch: 558 Batch:   0 Loss: 138.65157 Accuracy: 0.86618\n",
      "Epoch: 559 Batch:   0 Loss: 138.29478 Accuracy: 0.86618\n",
      "Test Loss: 169.57636 Accuracy: 0.78725\n",
      "Epoch: 560 Batch:   0 Loss: 137.93878 Accuracy: 0.86722\n",
      "Epoch: 561 Batch:   0 Loss: 137.58333 Accuracy: 0.86722\n",
      "Epoch: 562 Batch:   0 Loss: 137.22887 Accuracy: 0.86722\n",
      "Epoch: 563 Batch:   0 Loss: 136.87526 Accuracy: 0.86722\n",
      "Epoch: 564 Batch:   0 Loss: 136.52238 Accuracy: 0.86722\n",
      "Epoch: 565 Batch:   0 Loss: 136.17079 Accuracy: 0.86826\n",
      "Epoch: 566 Batch:   0 Loss: 135.82016 Accuracy: 0.86826\n",
      "Epoch: 567 Batch:   0 Loss: 135.47055 Accuracy: 0.86826\n",
      "Epoch: 568 Batch:   0 Loss: 135.12192 Accuracy: 0.86929\n",
      "Epoch: 569 Batch:   0 Loss: 134.77429 Accuracy: 0.86929\n",
      "Test Loss: 167.84929 Accuracy: 0.78791\n",
      "Epoch: 570 Batch:   0 Loss: 134.42769 Accuracy: 0.87033\n",
      "Epoch: 571 Batch:   0 Loss: 134.08218 Accuracy: 0.87137\n",
      "Epoch: 572 Batch:   0 Loss: 133.73766 Accuracy: 0.87137\n",
      "Epoch: 573 Batch:   0 Loss: 133.39421 Accuracy: 0.87033\n",
      "Epoch: 574 Batch:   0 Loss: 133.05183 Accuracy: 0.87033\n",
      "Epoch: 575 Batch:   0 Loss: 132.71057 Accuracy: 0.87033\n",
      "Epoch: 576 Batch:   0 Loss: 132.37035 Accuracy: 0.87033\n",
      "Epoch: 577 Batch:   0 Loss: 132.03085 Accuracy: 0.87033\n",
      "Epoch: 578 Batch:   0 Loss: 131.69232 Accuracy: 0.87137\n",
      "Epoch: 579 Batch:   0 Loss: 131.35507 Accuracy: 0.87137\n",
      "Test Loss: 166.30178 Accuracy: 0.78593\n",
      "Epoch: 580 Batch:   0 Loss: 131.01912 Accuracy: 0.87344\n",
      "Epoch: 581 Batch:   0 Loss: 130.68433 Accuracy: 0.87344\n",
      "Epoch: 582 Batch:   0 Loss: 130.35068 Accuracy: 0.87344\n",
      "Epoch: 583 Batch:   0 Loss: 130.01791 Accuracy: 0.87344\n",
      "Epoch: 584 Batch:   0 Loss: 129.68660 Accuracy: 0.87344\n",
      "Epoch: 585 Batch:   0 Loss: 129.35645 Accuracy: 0.87448\n",
      "Epoch: 586 Batch:   0 Loss: 129.02750 Accuracy: 0.87552\n",
      "Epoch: 587 Batch:   0 Loss: 128.69971 Accuracy: 0.87552\n",
      "Epoch: 588 Batch:   0 Loss: 128.37308 Accuracy: 0.87656\n",
      "Epoch: 589 Batch:   0 Loss: 128.04742 Accuracy: 0.87759\n",
      "Test Loss: 164.93917 Accuracy: 0.78579\n",
      "Epoch: 590 Batch:   0 Loss: 127.72332 Accuracy: 0.87863\n",
      "Epoch: 591 Batch:   0 Loss: 127.40032 Accuracy: 0.87863\n",
      "Epoch: 592 Batch:   0 Loss: 127.07850 Accuracy: 0.87863\n",
      "Epoch: 593 Batch:   0 Loss: 126.75793 Accuracy: 0.87863\n",
      "Epoch: 594 Batch:   0 Loss: 126.43865 Accuracy: 0.87863\n",
      "Epoch: 595 Batch:   0 Loss: 126.12066 Accuracy: 0.87967\n",
      "Epoch: 596 Batch:   0 Loss: 125.80389 Accuracy: 0.88071\n",
      "Epoch: 597 Batch:   0 Loss: 125.48840 Accuracy: 0.88071\n",
      "Epoch: 598 Batch:   0 Loss: 125.17414 Accuracy: 0.88071\n",
      "Epoch: 599 Batch:   0 Loss: 124.86121 Accuracy: 0.88071\n",
      "Test Loss: 163.75105 Accuracy: 0.78818\n",
      "Epoch: 600 Batch:   0 Loss: 124.54942 Accuracy: 0.88071\n",
      "Epoch: 601 Batch:   0 Loss: 124.23913 Accuracy: 0.88071\n",
      "Epoch: 602 Batch:   0 Loss: 123.93016 Accuracy: 0.88071\n",
      "Epoch: 603 Batch:   0 Loss: 123.62235 Accuracy: 0.88071\n",
      "Epoch: 604 Batch:   0 Loss: 123.31583 Accuracy: 0.88071\n",
      "Epoch: 605 Batch:   0 Loss: 123.01061 Accuracy: 0.88071\n",
      "Epoch: 606 Batch:   0 Loss: 122.70596 Accuracy: 0.88071\n",
      "Epoch: 607 Batch:   0 Loss: 122.40281 Accuracy: 0.88174\n",
      "Epoch: 608 Batch:   0 Loss: 122.10129 Accuracy: 0.88174\n",
      "Epoch: 609 Batch:   0 Loss: 121.80106 Accuracy: 0.88174\n",
      "Test Loss: 162.72564 Accuracy: 0.78619\n",
      "Epoch: 610 Batch:   0 Loss: 121.50186 Accuracy: 0.88174\n",
      "Epoch: 611 Batch:   0 Loss: 121.20308 Accuracy: 0.88174\n",
      "Epoch: 612 Batch:   0 Loss: 120.90639 Accuracy: 0.88174\n",
      "Epoch: 613 Batch:   0 Loss: 120.61109 Accuracy: 0.88174\n",
      "Epoch: 614 Batch:   0 Loss: 120.31641 Accuracy: 0.88174\n",
      "Epoch: 615 Batch:   0 Loss: 120.02351 Accuracy: 0.88278\n",
      "Epoch: 616 Batch:   0 Loss: 119.73157 Accuracy: 0.88174\n",
      "Epoch: 617 Batch:   0 Loss: 119.44131 Accuracy: 0.88174\n",
      "Epoch: 618 Batch:   0 Loss: 119.15215 Accuracy: 0.88174\n",
      "Epoch: 619 Batch:   0 Loss: 118.86420 Accuracy: 0.88174\n",
      "Test Loss: 161.86820 Accuracy: 0.78500\n",
      "Epoch: 620 Batch:   0 Loss: 118.57880 Accuracy: 0.88278\n",
      "Epoch: 621 Batch:   0 Loss: 118.29367 Accuracy: 0.88278\n",
      "Epoch: 622 Batch:   0 Loss: 118.01012 Accuracy: 0.88278\n",
      "Epoch: 623 Batch:   0 Loss: 117.72794 Accuracy: 0.88278\n",
      "Epoch: 624 Batch:   0 Loss: 117.44716 Accuracy: 0.88485\n",
      "Epoch: 625 Batch:   0 Loss: 117.16732 Accuracy: 0.88485\n",
      "Epoch: 626 Batch:   0 Loss: 116.88891 Accuracy: 0.88485\n",
      "Epoch: 627 Batch:   0 Loss: 116.61201 Accuracy: 0.88485\n",
      "Epoch: 628 Batch:   0 Loss: 116.33601 Accuracy: 0.88485\n",
      "Epoch: 629 Batch:   0 Loss: 116.06191 Accuracy: 0.88485\n",
      "Test Loss: 161.16672 Accuracy: 0.78262\n",
      "Epoch: 630 Batch:   0 Loss: 115.78889 Accuracy: 0.88485\n",
      "Epoch: 631 Batch:   0 Loss: 115.51690 Accuracy: 0.88589\n",
      "Epoch: 632 Batch:   0 Loss: 115.24626 Accuracy: 0.88589\n",
      "Epoch: 633 Batch:   0 Loss: 114.97710 Accuracy: 0.88693\n",
      "Epoch: 634 Batch:   0 Loss: 114.70924 Accuracy: 0.88693\n",
      "Epoch: 635 Batch:   0 Loss: 114.44260 Accuracy: 0.88797\n",
      "Epoch: 636 Batch:   0 Loss: 114.17863 Accuracy: 0.88797\n",
      "Epoch: 637 Batch:   0 Loss: 113.91504 Accuracy: 0.88797\n",
      "Epoch: 638 Batch:   0 Loss: 113.65276 Accuracy: 0.88900\n",
      "Epoch: 639 Batch:   0 Loss: 113.39149 Accuracy: 0.88900\n",
      "Test Loss: 160.60060 Accuracy: 0.78315\n",
      "Epoch: 640 Batch:   0 Loss: 113.13164 Accuracy: 0.88900\n",
      "Epoch: 641 Batch:   0 Loss: 112.87315 Accuracy: 0.88900\n",
      "Epoch: 642 Batch:   0 Loss: 112.61566 Accuracy: 0.88900\n",
      "Epoch: 643 Batch:   0 Loss: 112.35963 Accuracy: 0.88900\n",
      "Epoch: 644 Batch:   0 Loss: 112.10488 Accuracy: 0.88797\n",
      "Epoch: 645 Batch:   0 Loss: 111.85146 Accuracy: 0.88797\n",
      "Epoch: 646 Batch:   0 Loss: 111.59933 Accuracy: 0.88797\n",
      "Epoch: 647 Batch:   0 Loss: 111.34852 Accuracy: 0.88797\n",
      "Epoch: 648 Batch:   0 Loss: 111.09894 Accuracy: 0.88797\n",
      "Epoch: 649 Batch:   0 Loss: 110.85053 Accuracy: 0.88797\n",
      "Test Loss: 160.15844 Accuracy: 0.78288\n",
      "Epoch: 650 Batch:   0 Loss: 110.60415 Accuracy: 0.88797\n",
      "Epoch: 651 Batch:   0 Loss: 110.35910 Accuracy: 0.88693\n",
      "Epoch: 652 Batch:   0 Loss: 110.11462 Accuracy: 0.88693\n",
      "Epoch: 653 Batch:   0 Loss: 109.87159 Accuracy: 0.88693\n",
      "Epoch: 654 Batch:   0 Loss: 109.62991 Accuracy: 0.88589\n",
      "Epoch: 655 Batch:   0 Loss: 109.38960 Accuracy: 0.88693\n",
      "Epoch: 656 Batch:   0 Loss: 109.15044 Accuracy: 0.88693\n",
      "Epoch: 657 Batch:   0 Loss: 108.91276 Accuracy: 0.88693\n",
      "Epoch: 658 Batch:   0 Loss: 108.67645 Accuracy: 0.88693\n",
      "Epoch: 659 Batch:   0 Loss: 108.44137 Accuracy: 0.88693\n",
      "Test Loss: 159.82737 Accuracy: 0.78235\n",
      "Epoch: 660 Batch:   0 Loss: 108.20765 Accuracy: 0.88693\n",
      "Epoch: 661 Batch:   0 Loss: 107.97516 Accuracy: 0.88693\n",
      "Epoch: 662 Batch:   0 Loss: 107.74378 Accuracy: 0.88693\n",
      "Epoch: 663 Batch:   0 Loss: 107.51362 Accuracy: 0.88693\n",
      "Epoch: 664 Batch:   0 Loss: 107.28487 Accuracy: 0.88693\n",
      "Epoch: 665 Batch:   0 Loss: 107.05708 Accuracy: 0.88693\n",
      "Epoch: 666 Batch:   0 Loss: 106.83098 Accuracy: 0.88693\n",
      "Epoch: 667 Batch:   0 Loss: 106.60611 Accuracy: 0.88797\n",
      "Epoch: 668 Batch:   0 Loss: 106.38243 Accuracy: 0.88797\n",
      "Epoch: 669 Batch:   0 Loss: 106.15996 Accuracy: 0.88797\n",
      "Test Loss: 159.59070 Accuracy: 0.78275\n",
      "Epoch: 670 Batch:   0 Loss: 105.93877 Accuracy: 0.88797\n",
      "Epoch: 671 Batch:   0 Loss: 105.71859 Accuracy: 0.88797\n",
      "Epoch: 672 Batch:   0 Loss: 105.49985 Accuracy: 0.88797\n",
      "Epoch: 673 Batch:   0 Loss: 105.28249 Accuracy: 0.88797\n",
      "Epoch: 674 Batch:   0 Loss: 105.06613 Accuracy: 0.88797\n",
      "Epoch: 675 Batch:   0 Loss: 104.85123 Accuracy: 0.88797\n",
      "Epoch: 676 Batch:   0 Loss: 104.63753 Accuracy: 0.88797\n",
      "Epoch: 677 Batch:   0 Loss: 104.42511 Accuracy: 0.88797\n",
      "Epoch: 678 Batch:   0 Loss: 104.21389 Accuracy: 0.88797\n",
      "Epoch: 679 Batch:   0 Loss: 104.00388 Accuracy: 0.88797\n",
      "Test Loss: 159.42834 Accuracy: 0.78196\n",
      "Epoch: 680 Batch:   0 Loss: 103.79509 Accuracy: 0.88797\n",
      "Epoch: 681 Batch:   0 Loss: 103.58746 Accuracy: 0.88900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 682 Batch:   0 Loss: 103.38098 Accuracy: 0.88900\n",
      "Epoch: 683 Batch:   0 Loss: 103.17718 Accuracy: 0.88900\n",
      "Epoch: 684 Batch:   0 Loss: 102.97262 Accuracy: 0.88797\n",
      "Epoch: 685 Batch:   0 Loss: 102.76967 Accuracy: 0.88797\n",
      "Epoch: 686 Batch:   0 Loss: 102.56738 Accuracy: 0.88797\n",
      "Epoch: 687 Batch:   0 Loss: 102.36708 Accuracy: 0.88797\n",
      "Epoch: 688 Batch:   0 Loss: 102.16743 Accuracy: 0.88797\n",
      "Epoch: 689 Batch:   0 Loss: 101.96894 Accuracy: 0.88797\n",
      "Test Loss: 159.32807 Accuracy: 0.78063\n",
      "Epoch: 690 Batch:   0 Loss: 101.77213 Accuracy: 0.88797\n",
      "Epoch: 691 Batch:   0 Loss: 101.57639 Accuracy: 0.88797\n",
      "Epoch: 692 Batch:   0 Loss: 101.38161 Accuracy: 0.88900\n",
      "Epoch: 693 Batch:   0 Loss: 101.18822 Accuracy: 0.88900\n",
      "Epoch: 694 Batch:   0 Loss: 100.99547 Accuracy: 0.88900\n",
      "Epoch: 695 Batch:   0 Loss: 100.80372 Accuracy: 0.88900\n",
      "Epoch: 696 Batch:   0 Loss: 100.61311 Accuracy: 0.88900\n",
      "Epoch: 697 Batch:   0 Loss: 100.42360 Accuracy: 0.88900\n",
      "Epoch: 698 Batch:   0 Loss: 100.23495 Accuracy: 0.88900\n",
      "Epoch: 699 Batch:   0 Loss: 100.04790 Accuracy: 0.88900\n",
      "Test Loss: 159.26507 Accuracy: 0.78010\n",
      "Epoch: 700 Batch:   0 Loss: 99.86200 Accuracy: 0.88900\n",
      "Epoch: 701 Batch:   0 Loss: 99.67700 Accuracy: 0.88900\n",
      "Epoch: 702 Batch:   0 Loss: 99.49337 Accuracy: 0.88900\n",
      "Epoch: 703 Batch:   0 Loss: 99.31078 Accuracy: 0.88900\n",
      "Epoch: 704 Batch:   0 Loss: 99.12885 Accuracy: 0.88900\n",
      "Epoch: 705 Batch:   0 Loss: 98.94787 Accuracy: 0.88900\n",
      "Epoch: 706 Batch:   0 Loss: 98.76808 Accuracy: 0.89004\n",
      "Epoch: 707 Batch:   0 Loss: 98.58937 Accuracy: 0.89004\n",
      "Epoch: 708 Batch:   0 Loss: 98.41171 Accuracy: 0.89004\n",
      "Epoch: 709 Batch:   0 Loss: 98.23470 Accuracy: 0.89004\n",
      "Test Loss: 159.23590 Accuracy: 0.77865\n",
      "Epoch: 710 Batch:   0 Loss: 98.05900 Accuracy: 0.89004\n",
      "Epoch: 711 Batch:   0 Loss: 97.88433 Accuracy: 0.89108\n",
      "Epoch: 712 Batch:   0 Loss: 97.71074 Accuracy: 0.89108\n",
      "Epoch: 713 Batch:   0 Loss: 97.53812 Accuracy: 0.89108\n",
      "Epoch: 714 Batch:   0 Loss: 97.36655 Accuracy: 0.89108\n",
      "Epoch: 715 Batch:   0 Loss: 97.19590 Accuracy: 0.89212\n",
      "Epoch: 716 Batch:   0 Loss: 97.02614 Accuracy: 0.89212\n",
      "Epoch: 717 Batch:   0 Loss: 96.85724 Accuracy: 0.89212\n",
      "Epoch: 718 Batch:   0 Loss: 96.68970 Accuracy: 0.89212\n",
      "Epoch: 719 Batch:   0 Loss: 96.52284 Accuracy: 0.89212\n",
      "Test Loss: 159.23570 Accuracy: 0.77799\n",
      "Epoch: 720 Batch:   0 Loss: 96.35701 Accuracy: 0.89212\n",
      "Epoch: 721 Batch:   0 Loss: 96.19215 Accuracy: 0.89212\n",
      "Epoch: 722 Batch:   0 Loss: 96.02824 Accuracy: 0.89212\n",
      "Epoch: 723 Batch:   0 Loss: 95.86507 Accuracy: 0.89315\n",
      "Epoch: 724 Batch:   0 Loss: 95.70312 Accuracy: 0.89315\n",
      "Epoch: 725 Batch:   0 Loss: 95.54234 Accuracy: 0.89315\n",
      "Epoch: 726 Batch:   0 Loss: 95.38250 Accuracy: 0.89315\n",
      "Epoch: 727 Batch:   0 Loss: 95.22352 Accuracy: 0.89315\n",
      "Epoch: 728 Batch:   0 Loss: 95.06536 Accuracy: 0.89315\n",
      "Epoch: 729 Batch:   0 Loss: 94.90807 Accuracy: 0.89212\n",
      "Test Loss: 159.24731 Accuracy: 0.77812\n",
      "Epoch: 730 Batch:   0 Loss: 94.75176 Accuracy: 0.89212\n",
      "Epoch: 731 Batch:   0 Loss: 94.59640 Accuracy: 0.89212\n",
      "Epoch: 732 Batch:   0 Loss: 94.44195 Accuracy: 0.89212\n",
      "Epoch: 733 Batch:   0 Loss: 94.28841 Accuracy: 0.89212\n",
      "Epoch: 734 Batch:   0 Loss: 94.13564 Accuracy: 0.89212\n",
      "Epoch: 735 Batch:   0 Loss: 93.98378 Accuracy: 0.89212\n",
      "Epoch: 736 Batch:   0 Loss: 93.83279 Accuracy: 0.89212\n",
      "Epoch: 737 Batch:   0 Loss: 93.68272 Accuracy: 0.89212\n",
      "Epoch: 738 Batch:   0 Loss: 93.53388 Accuracy: 0.89212\n",
      "Epoch: 739 Batch:   0 Loss: 93.38581 Accuracy: 0.89212\n",
      "Test Loss: 159.26319 Accuracy: 0.77812\n",
      "Epoch: 740 Batch:   0 Loss: 93.23818 Accuracy: 0.89212\n",
      "Epoch: 741 Batch:   0 Loss: 93.09140 Accuracy: 0.89212\n",
      "Epoch: 742 Batch:   0 Loss: 92.94527 Accuracy: 0.89212\n",
      "Epoch: 743 Batch:   0 Loss: 92.80049 Accuracy: 0.89212\n",
      "Epoch: 744 Batch:   0 Loss: 92.65665 Accuracy: 0.89212\n",
      "Epoch: 745 Batch:   0 Loss: 92.51325 Accuracy: 0.89212\n",
      "Epoch: 746 Batch:   0 Loss: 92.37067 Accuracy: 0.89212\n",
      "Epoch: 747 Batch:   0 Loss: 92.22894 Accuracy: 0.89212\n",
      "Epoch: 748 Batch:   0 Loss: 92.08811 Accuracy: 0.89212\n",
      "Epoch: 749 Batch:   0 Loss: 91.94810 Accuracy: 0.89108\n",
      "Test Loss: 159.27600 Accuracy: 0.77878\n",
      "Epoch: 750 Batch:   0 Loss: 91.80888 Accuracy: 0.89108\n",
      "Epoch: 751 Batch:   0 Loss: 91.67049 Accuracy: 0.89108\n",
      "Epoch: 752 Batch:   0 Loss: 91.53283 Accuracy: 0.89004\n",
      "Epoch: 753 Batch:   0 Loss: 91.39610 Accuracy: 0.89004\n",
      "Epoch: 754 Batch:   0 Loss: 91.26016 Accuracy: 0.89004\n",
      "Epoch: 755 Batch:   0 Loss: 91.12467 Accuracy: 0.89004\n",
      "Epoch: 756 Batch:   0 Loss: 90.99029 Accuracy: 0.89004\n",
      "Epoch: 757 Batch:   0 Loss: 90.85665 Accuracy: 0.89004\n",
      "Epoch: 758 Batch:   0 Loss: 90.72374 Accuracy: 0.89004\n",
      "Epoch: 759 Batch:   0 Loss: 90.59142 Accuracy: 0.88900\n",
      "Test Loss: 159.28591 Accuracy: 0.77878\n",
      "Epoch: 760 Batch:   0 Loss: 90.46007 Accuracy: 0.88900\n",
      "Epoch: 761 Batch:   0 Loss: 90.32943 Accuracy: 0.88900\n",
      "Epoch: 762 Batch:   0 Loss: 90.19957 Accuracy: 0.88900\n",
      "Epoch: 763 Batch:   0 Loss: 90.07041 Accuracy: 0.88900\n",
      "Epoch: 764 Batch:   0 Loss: 89.94202 Accuracy: 0.88900\n",
      "Epoch: 765 Batch:   0 Loss: 89.81433 Accuracy: 0.89004\n",
      "Epoch: 766 Batch:   0 Loss: 89.68732 Accuracy: 0.89004\n",
      "Epoch: 767 Batch:   0 Loss: 89.56100 Accuracy: 0.89004\n",
      "Epoch: 768 Batch:   0 Loss: 89.43559 Accuracy: 0.89004\n",
      "Epoch: 769 Batch:   0 Loss: 89.31084 Accuracy: 0.89004\n",
      "Test Loss: 159.28901 Accuracy: 0.77772\n",
      "Epoch: 770 Batch:   0 Loss: 89.18670 Accuracy: 0.89004\n",
      "Epoch: 771 Batch:   0 Loss: 89.06332 Accuracy: 0.89004\n",
      "Epoch: 772 Batch:   0 Loss: 88.94067 Accuracy: 0.89108\n",
      "Epoch: 773 Batch:   0 Loss: 88.81868 Accuracy: 0.89004\n",
      "Epoch: 774 Batch:   0 Loss: 88.69728 Accuracy: 0.89004\n",
      "Epoch: 775 Batch:   0 Loss: 88.57678 Accuracy: 0.89004\n",
      "Epoch: 776 Batch:   0 Loss: 88.45679 Accuracy: 0.89004\n",
      "Epoch: 777 Batch:   0 Loss: 88.33747 Accuracy: 0.89004\n",
      "Epoch: 778 Batch:   0 Loss: 88.21869 Accuracy: 0.89004\n",
      "Epoch: 779 Batch:   0 Loss: 88.10082 Accuracy: 0.89004\n",
      "Test Loss: 159.28286 Accuracy: 0.77640\n",
      "Epoch: 780 Batch:   0 Loss: 87.98361 Accuracy: 0.89004\n",
      "Epoch: 781 Batch:   0 Loss: 87.86703 Accuracy: 0.89004\n",
      "Epoch: 782 Batch:   0 Loss: 87.75110 Accuracy: 0.89004\n",
      "Epoch: 783 Batch:   0 Loss: 87.63578 Accuracy: 0.89004\n",
      "Epoch: 784 Batch:   0 Loss: 87.52108 Accuracy: 0.89004\n",
      "Epoch: 785 Batch:   0 Loss: 87.40701 Accuracy: 0.89108\n",
      "Epoch: 786 Batch:   0 Loss: 87.29353 Accuracy: 0.89108\n",
      "Epoch: 787 Batch:   0 Loss: 87.18037 Accuracy: 0.89315\n",
      "Epoch: 788 Batch:   0 Loss: 87.06802 Accuracy: 0.89315\n",
      "Epoch: 789 Batch:   0 Loss: 86.95687 Accuracy: 0.89315\n",
      "Test Loss: 159.25119 Accuracy: 0.77574\n",
      "Epoch: 790 Batch:   0 Loss: 86.84630 Accuracy: 0.89419\n",
      "Epoch: 791 Batch:   0 Loss: 86.73596 Accuracy: 0.89523\n",
      "Epoch: 792 Batch:   0 Loss: 86.62618 Accuracy: 0.89523\n",
      "Epoch: 793 Batch:   0 Loss: 86.51699 Accuracy: 0.89523\n",
      "Epoch: 794 Batch:   0 Loss: 86.40839 Accuracy: 0.89523\n",
      "Epoch: 795 Batch:   0 Loss: 86.30032 Accuracy: 0.89523\n",
      "Epoch: 796 Batch:   0 Loss: 86.19241 Accuracy: 0.89523\n",
      "Epoch: 797 Batch:   0 Loss: 86.08527 Accuracy: 0.89627\n",
      "Epoch: 798 Batch:   0 Loss: 85.97865 Accuracy: 0.89627\n",
      "Epoch: 799 Batch:   0 Loss: 85.87259 Accuracy: 0.89730\n",
      "Test Loss: 159.20470 Accuracy: 0.77600\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = tf.placeholder(tf.float32, [None, dataDimension], name='X')\n",
    "Y = tf.placeholder(tf.float32, [None, numClasses], name='Y')\n",
    "protoNN = ProtoNN(dataDimension, PROJECTION_DIM,\n",
    "                  NUM_PROTOTYPES, numClasses,\n",
    "                  gamma, W=W, B=B)\n",
    "trainer = ProtoNNTrainer(protoNN,  REG_W, REG_B, REG_Z,\n",
    "                         SPAR_W, SPAR_B, SPAR_Z,\n",
    "                        LEARNING_RATE, X, Y, lossType='l2')\n",
    "sess = tf.Session()\n",
    "\n",
    "trainer.train(2048, 800, sess, x_train, x_test, y_train, y_test,\n",
    "              printStep=600, valStep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy 0.77599895\n",
      "Model size constraint (Bytes):  9580\n",
      "Number of non-zeros:  2395\n"
     ]
    }
   ],
   "source": [
    "acc = sess.run(protoNN.accuracy, feed_dict={X: x_test, Y: y_test})\n",
    "pred = sess.run(protoNN.predictions, feed_dict={X: x_test, Y: y_test})\n",
    "# W, B, Z are tensorflow graph nodes\n",
    "W, B, Z, _ = protoNN.getModelMatrices()\n",
    "matrixList = sess.run([W, B, Z])\n",
    "sparcityList = [SPAR_W, SPAR_B, SPAR_Z]                       \n",
    "nnz, size, sparse = getModelSize(matrixList, sparcityList)\n",
    "print(\"Final test accuracy\", acc)\n",
    "print(\"Model size constraint (Bytes): \", size)\n",
    "print(\"Number of non-zeros: \", nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2876  903]\n",
      " [ 790 2989]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.78451   0.76105   0.77260      3779\n",
      "           1    0.76799   0.79095   0.77930      3779\n",
      "\n",
      "    accuracy                        0.77600      7558\n",
      "   macro avg    0.77625   0.77600   0.77595      7558\n",
      "weighted avg    0.77625   0.77600   0.77595      7558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "y_test = np.argmax(y_test,axis=1)\n",
    "print (confusion_matrix(y_test,pred))\n",
    "print (classification_report(y_test,pred,digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7909499867689865"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivity = confusion_matrix(y_test,pred)[1][1]/(confusion_matrix(y_test,pred)[1][1] + confusion_matrix(y_test,pred)[1][0])\n",
    "sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7610478962688542"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specificity = confusion_matrix(y_test,pred)[0][0]/(confusion_matrix(y_test,pred)[0][0] + confusion_matrix(y_test,pred)[0][1])\n",
    "specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WINDOW 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "restartkernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'study' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15652\\1793003059.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mSPAR_Z\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SPAR_Z'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mLEARNING_RATE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'LEARNING_RATE'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'NUM_EPOCHS'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'study' is not defined"
     ]
    }
   ],
   "source": [
    "hp = {'REG_W': 2.150499128114348e-06,\n",
    " 'REG_B': 0.0013123991795708923,\n",
    " 'REG_Z': 2.7678514122803996e-05,\n",
    " 'SPAR_W': 0.9022371274865826,\n",
    " 'SPAR_B': 0.5539345534584593,\n",
    " 'SPAR_Z': 0.9993522107302426,\n",
    " 'loss': 'l2',\n",
    " 'LEARNING_RATE': 0.00031975053586635394,\n",
    " 'NUM_EPOCHS': 300,\n",
    " 'alpha': 0.31386241034032014}\n",
    "PROJECTION_DIM = 5 #d^\n",
    "NUM_PROTOTYPES = 40 #m\n",
    "REG_W = hp['REG_W']\n",
    "REG_B = hp['REG_B']\n",
    "REG_Z = hp['REG_Z']\n",
    "SPAR_W = hp['SPAR_W']\n",
    "SPAR_B = hp['SPAR_B']\n",
    "SPAR_Z = hp['SPAR_Z']\n",
    "loss = hp['loss']\n",
    "LEARNING_RATE = study.best_params['LEARNING_RATE']\n",
    "NUM_EPOCHS = study.best_params['NUM_EPOCHS']\n",
    "BATCH_SIZE = 1024\n",
    "GAMMA = gamma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nadit\\anaconda3\\envs\\ProtoNN\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT license.\n",
    "\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#sys.path.insert(0, '../../')\n",
    "# from edgeml.trainer.protoNNTrainer import ProtoNNTrainer\n",
    "# from edgeml.graph.protoNN import ProtoNN\n",
    "# import edgeml.utils as utils\n",
    "# import helpermethods as helper\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "sys.path.append(r\"E:\\programming\\practice\\research\\optimized code\\EdgeML\\examples\\tf\\ProtoNN\")\n",
    "import helpermethods as helper\n",
    "\n",
    "#helper methods\n",
    "sys.path.insert(0, '../')\n",
    "import argparse\n",
    "\n",
    "\n",
    "def getModelSize(matrixList, sparcityList, expected=True, bytesPerVar=4):\n",
    "    '''\n",
    "    expected: Expected size according to the parameters set. The number of\n",
    "        zeros could actually be more than that is required to satisfy the\n",
    "        sparsity constraint.\n",
    "    '''\n",
    "    nnzList, sizeList, isSparseList = [], [], []\n",
    "    hasSparse = False\n",
    "    for i in range(len(matrixList)):\n",
    "        A, s = matrixList[i], sparcityList[i]\n",
    "        assert A.ndim == 2\n",
    "        assert s >= 0\n",
    "        assert s <= 1\n",
    "        nnz, size, sparse = countnnZ(A, s, bytesPerVar=bytesPerVar)\n",
    "        nnzList.append(nnz)\n",
    "        sizeList.append(size)\n",
    "        hasSparse = (hasSparse or sparse)\n",
    "\n",
    "    totalnnZ = np.sum(nnzList)\n",
    "    totalSize = np.sum(sizeList)\n",
    "    if expected:\n",
    "        return totalnnZ, totalSize, hasSparse\n",
    "    numNonZero = 0\n",
    "    totalSize = 0\n",
    "    hasSparse = False\n",
    "    for i in range(len(matrixList)):\n",
    "        A, s = matrixList[i], sparcityList[i]\n",
    "        numNonZero_ = np.count_nonzero(A)\n",
    "        numNonZero += numNonZero_\n",
    "        hasSparse = (hasSparse or (s < 0.5))\n",
    "        if s <= 0.5:\n",
    "            totalSize += numNonZero_ * 2 * bytesPerVar\n",
    "        else:\n",
    "            totalSize += A.size * bytesPerVar\n",
    "    return numNonZero, totalSize, hasSparse\n",
    "\n",
    "\n",
    "def getGamma(gammaInit, projectionDim, dataDim, numPrototypes, x_train):\n",
    "    if gammaInit is None:\n",
    "        print(\"Using median heuristic to estimate gamma.\")\n",
    "        gamma, W, B = medianHeuristic(x_train, projectionDim,\n",
    "                                            numPrototypes)\n",
    "        print(\"Gamma estimate is: %f\" % gamma)\n",
    "        return W, B, gamma\n",
    "    return None, None, gammaInit\n",
    "\n",
    "\n",
    "def preprocessData(dataDir,w):\n",
    "    '''\n",
    "    Loads data from the dataDir and does some initial preprocessing\n",
    "    steps. Data is assumed to be contained in two files,\n",
    "    train.npy and test.npy. Each containing a 2D numpy array of dimension\n",
    "    [numberOfExamples, numberOfFeatures + 1]. The first column of each\n",
    "    matrix is assumed to contain label information.\n",
    "\n",
    "    For an N-Class problem, we assume the labels are integers from 0 through\n",
    "    N-1.\n",
    "    '''\n",
    "    # Uncomment for usual training data\n",
    "    # train = np.load(dataDir + '/train_'+str(w)+'.npy')\n",
    "    # test = np.load(dataDir + '/test_'+str(w)+'.npy')\n",
    "    # Uncomment for time domain training data\n",
    "    train = np.load(dataDir + '/ttrain_'+str(w)+'.npy')\n",
    "    test = np.load(dataDir + '/ttest_'+str(w)+'.npy')\n",
    "    # Uncomment for 1 sensordrop training data\n",
    "    # train = np.load(dataDir + '/train_'+str(w)+'.npy')\n",
    "    # test = np.load(dataDir + '/test_'+str(w)+'.npy')\n",
    "\n",
    "    dataDimension = int(train.shape[1]) - 1\n",
    "    x_train = train[:, 1:dataDimension + 1]\n",
    "    y_train_ = train[:, 0]\n",
    "    x_test = test[:, 1:dataDimension + 1]\n",
    "    y_test_ = test[:, 0]\n",
    "\n",
    "    numClasses = max(y_train_) - min(y_train_) + 1\n",
    "    numClasses = max(numClasses, max(y_test_) - min(y_test_) + 1)\n",
    "    numClasses = int(numClasses)\n",
    "\n",
    "    # mean-var\n",
    "    mean = np.mean(x_train, 0)\n",
    "    std = np.std(x_train, 0)\n",
    "    std[std[:] < 0.000001] = 1\n",
    "    x_train = (x_train - mean) / std\n",
    "    x_test = (x_test - mean) / std\n",
    "\n",
    "    # one hot y-train\n",
    "    lab = y_train_.astype('uint8')\n",
    "    lab = np.array(lab) - min(lab)\n",
    "    lab_ = np.zeros((x_train.shape[0], numClasses))\n",
    "    lab_[np.arange(x_train.shape[0]), lab] = 1\n",
    "    y_train = lab_\n",
    "\n",
    "    # one hot y-test\n",
    "    lab = y_test_.astype('uint8')\n",
    "    lab = np.array(lab) - min(lab)\n",
    "    lab_ = np.zeros((x_test.shape[0], numClasses))\n",
    "    lab_[np.arange(x_test.shape[0]), lab] = 1\n",
    "    y_test = lab_\n",
    "\n",
    "    return dataDimension, numClasses, x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "def getProtoNNArgs():\n",
    "    def checkIntPos(value):\n",
    "        ivalue = int(value)\n",
    "        if ivalue <= 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid positive int value\" % value)\n",
    "        return ivalue\n",
    "\n",
    "    def checkIntNneg(value):\n",
    "        ivalue = int(value)\n",
    "        if ivalue < 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid non-neg int value\" % value)\n",
    "        return ivalue\n",
    "\n",
    "    def checkFloatNneg(value):\n",
    "        fvalue = float(value)\n",
    "        if fvalue < 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid non-neg float value\" % value)\n",
    "        return fvalue\n",
    "\n",
    "    def checkFloatPos(value):\n",
    "        fvalue = float(value)\n",
    "        if fvalue <= 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid positive float value\" % value)\n",
    "        return fvalue\n",
    "\n",
    "    '''\n",
    "    Parse protoNN commandline arguments\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Hyperparameters for ProtoNN Algorithm')\n",
    "\n",
    "    msg = 'Data directory containing train and test data. The '\n",
    "    msg += 'data is assumed to be saved as 2-D numpy matrices with '\n",
    "    msg += 'names `train.npy` and `test.npy`, of dimensions\\n'\n",
    "    msg += '\\t[numberOfInstances, numberOfFeatures + 1].\\n'\n",
    "    msg += 'The first column of each file is assumed to contain label information.'\n",
    "    msg += ' For a N-class problem, labels are assumed to be integers from 0 to'\n",
    "    msg += ' N-1 (inclusive).'\n",
    "    parser.add_argument('-d', '--data-dir', required=True, help=msg)\n",
    "    parser.add_argument('-l', '--projection-dim', type=checkIntPos, default=10,\n",
    "                        help='Projection Dimension.')\n",
    "    parser.add_argument('-p', '--num-prototypes', type=checkIntPos, default=20,\n",
    "                        help='Number of prototypes.')\n",
    "    parser.add_argument('-g', '--gamma', type=checkFloatPos, default=None,\n",
    "                        help='Gamma for Gaussian kernel. If not provided, ' +\n",
    "                        'median heuristic will be used to estimate gamma.')\n",
    "\n",
    "    parser.add_argument('-e', '--epochs', type=checkIntPos, default=100,\n",
    "                        help='Total training epochs.')\n",
    "    parser.add_argument('-b', '--batch-size', type=checkIntPos, default=32,\n",
    "                        help='Batch size for each pass.')\n",
    "    parser.add_argument('-r', '--learning-rate', type=checkFloatPos,\n",
    "                        default=0.001,\n",
    "                        help='Initial Learning rate for ADAM Optimizer.')\n",
    "\n",
    "    parser.add_argument('-rW', type=float, default=0.000,\n",
    "                        help='Coefficient for l2 regularizer for predictor' +\n",
    "                        ' parameter W ' + '(default = 0.0).')\n",
    "    parser.add_argument('-rB', type=float, default=0.00,\n",
    "                        help='Coefficient for l2 regularizer for predictor' +\n",
    "                        ' parameter B ' + '(default = 0.0).')\n",
    "    parser.add_argument('-rZ', type=float, default=0.00,\n",
    "                        help='Coefficient for l2 regularizer for predictor' +\n",
    "                        'parameter Z ' +\n",
    "                        '(default = 0.0).')\n",
    "\n",
    "    parser.add_argument('-sW', type=float, default=1.000,\n",
    "                        help='Sparsity constraint for predictor parameter W ' +\n",
    "                        '(default = 1.0, i.e. dense matrix).')\n",
    "    parser.add_argument('-sB', type=float, default=1.00,\n",
    "                        help='Sparsity constraint for predictor parameter B ' +\n",
    "                        '(default = 1.0, i.e. dense matrix).')\n",
    "    parser.add_argument('-sZ', type=float, default=1.00,\n",
    "                        help='Sparsity constraint for predictor parameter Z ' +\n",
    "                        '(default = 1.0, i.e. dense matrix).')\n",
    "    parser.add_argument('-pS', '--print-step', type=int, default=200,\n",
    "                        help='The number of update steps between print ' +\n",
    "                        'calls to console.')\n",
    "    parser.add_argument('-vS', '--val-step', type=int, default=3,\n",
    "                        help='The number of epochs between validation' +\n",
    "                        'performance evaluation')\n",
    "    return parser.parse_args()\n",
    "\n",
    "#utils\n",
    "import scipy.cluster\n",
    "import scipy.spatial\n",
    "import os\n",
    "\n",
    "\n",
    "def medianHeuristic(data, projectionDimension, numPrototypes, W_init=None):\n",
    "    '''\n",
    "    This method can be used to estimate gamma for ProtoNN. An approximation to\n",
    "    median heuristic is used here.\n",
    "    1. First the data is collapsed into the projectionDimension by W_init. If\n",
    "    W_init is not provided, it is initialized from a random normal(0, 1). Hence\n",
    "    data normalization is essential.\n",
    "    2. Prototype are computed by running a  k-means clustering on the projected\n",
    "    data.\n",
    "    3. The median distance is then estimated by calculating median distance\n",
    "    between prototypes and projected data points.\n",
    "\n",
    "    data needs to be [-1, numFeats]\n",
    "    If using this method to initialize gamma, please use the W and B as well.\n",
    "\n",
    "    TODO: Return estimate of Z (prototype labels) based on cluster centroids\n",
    "    andand labels\n",
    "\n",
    "    TODO: Clustering fails due to singularity error if projecting upwards\n",
    "\n",
    "    W [dxd_cap]\n",
    "    B [d_cap, m]\n",
    "    returns gamma, W, B\n",
    "    '''\n",
    "    assert data.ndim == 2\n",
    "    X = data\n",
    "    featDim = data.shape[1]\n",
    "    if projectionDimension > featDim:\n",
    "        print(\"Warning: Projection dimension > feature dimension. Gamma\")\n",
    "        print(\"\\t estimation due to median heuristic could fail.\")\n",
    "        print(\"\\tTo retain the projection dataDimension, provide\")\n",
    "        print(\"\\ta value for gamma.\")\n",
    "\n",
    "    if W_init is None:\n",
    "        W_init = np.random.normal(size=[featDim, projectionDimension])\n",
    "    W = W_init\n",
    "    XW = np.matmul(X, W)\n",
    "    assert XW.shape[1] == projectionDimension\n",
    "    assert XW.shape[0] == len(X)\n",
    "    # Requires [N x d_cap] data matrix of N observations of d_cap-dimension and\n",
    "    # the number of centroids m. Returns, [n x d_cap] centroids and\n",
    "    # elementwise center information.\n",
    "    B, centers = scipy.cluster.vq.kmeans2(XW, numPrototypes)\n",
    "    # Requires two matrices. Number of observations x dimension of observation\n",
    "    # space. Distances[i,j] is the distance between XW[i] and B[j]\n",
    "    distances = scipy.spatial.distance.cdist(XW, B, metric='euclidean')\n",
    "    distances = np.reshape(distances, [-1])\n",
    "    gamma = np.median(distances)\n",
    "    gamma = 1 / (2.5 * gamma)\n",
    "    return gamma.astype('float32'), W.astype('float32'), B.T.astype('float32')\n",
    "\n",
    "\n",
    "def multiClassHingeLoss(logits, label, batch_th):\n",
    "    '''\n",
    "    MultiClassHingeLoss to match C++ Version - No TF internal version\n",
    "    '''\n",
    "    flatLogits = tf.reshape(logits, [-1, ])\n",
    "    label_ = tf.argmax(label, 1)\n",
    "\n",
    "    correctId = tf.range(0, batch_th) * label.shape[1] + label_\n",
    "    correctLogit = tf.gather(flatLogits, correctId)\n",
    "\n",
    "    maxLabel = tf.argmax(logits, 1)\n",
    "    top2, _ = tf.nn.top_k(logits, k=2, sorted=True)\n",
    "\n",
    "    wrongMaxLogit = tf.where(\n",
    "        tf.equal(maxLabel, label_), top2[:, 1], top2[:, 0])\n",
    "\n",
    "    return tf.reduce_mean(tf.nn.relu(1. + wrongMaxLogit - correctLogit))\n",
    "\n",
    "\n",
    "def crossEntropyLoss(logits, label):\n",
    "    '''\n",
    "    Cross Entropy loss for MultiClass case in joint training for\n",
    "    faster convergence\n",
    "    '''\n",
    "    return tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                   labels=tf.stop_gradient(label)))\n",
    "\n",
    "\n",
    "def mean_absolute_error(logits, label):\n",
    "    '''\n",
    "    Function to compute the mean absolute error.\n",
    "    '''\n",
    "    return tf.reduce_mean(tf.abs(tf.subtract(logits, label)))\n",
    "\n",
    "\n",
    "def hardThreshold(A, s):\n",
    "    '''\n",
    "    Hard thresholding function on Tensor A with sparsity s\n",
    "    '''\n",
    "    A_ = np.copy(A)\n",
    "    A_ = A_.ravel()\n",
    "    if len(A_) > 0:\n",
    "        th = np.percentile(np.abs(A_), (1 - s) * 100.0, interpolation='higher')\n",
    "        A_[np.abs(A_) < th] = 0.0\n",
    "    A_ = A_.reshape(A.shape)\n",
    "    return A_\n",
    "\n",
    "\n",
    "def copySupport(src, dest):\n",
    "    '''\n",
    "    copy support of src tensor to dest tensor\n",
    "    '''\n",
    "    support = np.nonzero(src)\n",
    "    dest_ = dest\n",
    "    dest = np.zeros(dest_.shape)\n",
    "    dest[support] = dest_[support]\n",
    "    return dest\n",
    "\n",
    "\n",
    "def countnnZ(A, s, bytesPerVar=4):\n",
    "    '''\n",
    "    Returns # of non-zeros and representative size of the tensor\n",
    "    Uses dense for s >= 0.5 - 4 byte\n",
    "    Else uses sparse - 8 byte\n",
    "    '''\n",
    "    params = 1\n",
    "    hasSparse = False\n",
    "    for i in range(0, len(A.shape)):\n",
    "        params *= int(A.shape[i])\n",
    "    if s < 0.5:\n",
    "        nnZ = np.ceil(params * s)\n",
    "        hasSparse = True\n",
    "        return nnZ, nnZ * 2 * bytesPerVar, hasSparse\n",
    "    else:\n",
    "        nnZ = params\n",
    "        return nnZ, nnZ * bytesPerVar, hasSparse\n",
    "\n",
    "\n",
    "def getConfusionMatrix(predicted, target, numClasses):\n",
    "    '''\n",
    "    Returns a confusion matrix for a multiclass classification\n",
    "    problem. `predicted` is a 1-D array of integers representing\n",
    "    the predicted classes and `target` is the target classes.\n",
    "\n",
    "    confusion[i][j]: Number of elements of class j\n",
    "        predicted as class i\n",
    "    Labels are assumed to be in range(0, numClasses)\n",
    "    Use`printFormattedConfusionMatrix` to echo the confusion matrix\n",
    "    in a user friendly form.\n",
    "    '''\n",
    "    assert(predicted.ndim == 1)\n",
    "    assert(target.ndim == 1)\n",
    "    arr = np.zeros([numClasses, numClasses])\n",
    "\n",
    "    for i in range(len(predicted)):\n",
    "        arr[predicted[i]][target[i]] += 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "def printFormattedConfusionMatrix(matrix):\n",
    "    '''\n",
    "    Given a 2D confusion matrix, prints it in a human readable way.\n",
    "    The confusion matrix is expected to be a 2D numpy array with\n",
    "    square dimensions\n",
    "    '''\n",
    "    assert(matrix.ndim == 2)\n",
    "    assert(matrix.shape[0] == matrix.shape[1])\n",
    "    RECALL = 'Recall'\n",
    "    PRECISION = 'PRECISION'\n",
    "    print(\"|%s|\" % ('True->'), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%7d|\" % i, end='')\n",
    "    print(\"%s|\" % 'Precision')\n",
    "\n",
    "    print(\"|%s|\" % ('-' * len(RECALL)), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%s|\" % ('-' * 7), end='')\n",
    "    print(\"%s|\" % ('-' * len(PRECISION)))\n",
    "\n",
    "    precisionlist = np.sum(matrix, axis=1)\n",
    "    recalllist = np.sum(matrix, axis=0)\n",
    "    precisionlist = [matrix[i][i] / x if x !=\n",
    "                     0 else -1 for i, x in enumerate(precisionlist)]\n",
    "    recalllist = [matrix[i][i] / x if x !=\n",
    "                  0 else -1 for i, x in enumerate(recalllist)]\n",
    "    for i in range(matrix.shape[0]):\n",
    "        # len recall = 6\n",
    "        print(\"|%6d|\" % (i), end='')\n",
    "        for j in range(matrix.shape[0]):\n",
    "            print(\"%7d|\" % (matrix[i][j]), end='')\n",
    "        print(\"%s\" % (\" \" * (len(PRECISION) - 7)), end='')\n",
    "        if precisionlist[i] != -1:\n",
    "            print(\"%1.5f|\" % precisionlist[i])\n",
    "        else:\n",
    "            print(\"%7s|\" % \"nan\")\n",
    "\n",
    "    print(\"|%s|\" % ('-' * len(RECALL)), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%s|\" % ('-' * 7), end='')\n",
    "    print(\"%s|\" % ('-' * len(PRECISION)))\n",
    "    print(\"|%s|\" % ('Recall'), end='')\n",
    "\n",
    "    for i in range(matrix.shape[0]):\n",
    "        if recalllist[i] != -1:\n",
    "            print(\"%1.5f|\" % (recalllist[i]), end='')\n",
    "        else:\n",
    "            print(\"%7s|\" % \"nan\", end='')\n",
    "\n",
    "    print('%s|' % (' ' * len(PRECISION)))\n",
    "\n",
    "\n",
    "def getPrecisionRecall(cmatrix, label=1):\n",
    "    trueP = cmatrix[label][label]\n",
    "    denom = np.sum(cmatrix, axis=0)[label]\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    recall = trueP / denom\n",
    "    denom = np.sum(cmatrix, axis=1)[label]\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    precision = trueP / denom\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMacroPrecisionRecall(cmatrix):\n",
    "    # TP + FP\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    # TP + FN\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    precisionlist__ = [cmatrix[i][i] / x if x !=\n",
    "                       0 else 0 for i, x in enumerate(precisionlist)]\n",
    "    recalllist__ = [cmatrix[i][i] / x if x !=\n",
    "                    0 else 0 for i, x in enumerate(recalllist)]\n",
    "    precision = np.sum(precisionlist__)\n",
    "    precision /= len(precisionlist__)\n",
    "    recall = np.sum(recalllist__)\n",
    "    recall /= len(recalllist__)\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMicroPrecisionRecall(cmatrix):\n",
    "    # TP + FP\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    # TP + FN\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    num = 0.0\n",
    "    for i in range(len(cmatrix)):\n",
    "        num += cmatrix[i][i]\n",
    "\n",
    "    precision = num / np.sum(precisionlist)\n",
    "    recall = num / np.sum(recalllist)\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMacroMicroFScore(cmatrix):\n",
    "    '''\n",
    "    Returns macro and micro f-scores.\n",
    "    Refer: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.104.8244&rep=rep1&type=pdf\n",
    "    '''\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    precisionlist__ = [cmatrix[i][i] / x if x !=\n",
    "                       0 else 0 for i, x in enumerate(precisionlist)]\n",
    "    recalllist__ = [cmatrix[i][i] / x if x !=\n",
    "                    0 else 0 for i, x in enumerate(recalllist)]\n",
    "    macro = 0.0\n",
    "    for i in range(len(precisionlist)):\n",
    "        denom = precisionlist__[i] + recalllist__[i]\n",
    "        numer = precisionlist__[i] * recalllist__[i] * 2\n",
    "        if denom == 0:\n",
    "            denom = 1\n",
    "        macro += numer / denom\n",
    "    macro /= len(precisionlist)\n",
    "\n",
    "    num = 0.0\n",
    "    for i in range(len(precisionlist)):\n",
    "        num += cmatrix[i][i]\n",
    "\n",
    "    denom1 = np.sum(precisionlist)\n",
    "    denom2 = np.sum(recalllist)\n",
    "    pi = num / denom1\n",
    "    rho = num / denom2\n",
    "    denom = pi + rho\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    micro = 2 * pi * rho / denom\n",
    "    return macro, micro\n",
    "\n",
    "\n",
    "class GraphManager:\n",
    "    '''\n",
    "    Manages saving and restoring graphs. Designed to be used with EMI-RNN\n",
    "    though is general enough to be useful otherwise as well.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def checkpointModel(self, saver, sess, modelPrefix,\n",
    "                        globalStep=1000, redirFile=None):\n",
    "        saver.save(sess, modelPrefix, global_step=globalStep)\n",
    "        print('Model saved to %s, global_step %d' % (modelPrefix, globalStep),\n",
    "              file=redirFile)\n",
    "\n",
    "    def loadCheckpoint(self, sess, modelPrefix, globalStep,\n",
    "                       redirFile=None):\n",
    "        metaname = modelPrefix + '-%d.meta' % globalStep\n",
    "        basename = os.path.basename(metaname)\n",
    "        fileList = os.listdir(os.path.dirname(modelPrefix))\n",
    "        fileList = [x for x in fileList if x.startswith(basename)]\n",
    "        assert len(fileList) > 0, 'Checkpoint file not found'\n",
    "        msg = 'Too many or too few checkpoint files for globalStep: %d' % globalStep\n",
    "        assert len(fileList) is 1, msg\n",
    "        chkpt = basename + '/' + fileList[0]\n",
    "        saver = tf.train.import_meta_graph(metaname)\n",
    "        metaname = metaname[:-5]\n",
    "        saver.restore(sess, metaname)\n",
    "        graph = tf.get_default_graph()\n",
    "        return graph\n",
    "\n",
    "#Trainer\n",
    "class ProtoNNTrainer:\n",
    "    def __init__(self, protoNNObj, regW, regB, regZ,\n",
    "                 sparcityW, sparcityB, sparcityZ,\n",
    "                 learningRate, X, Y, lossType='l2'):\n",
    "        '''\n",
    "        A wrapper for the various techniques used for training ProtoNN. This\n",
    "        subsumes both the responsibility of loss graph construction and\n",
    "        performing training. The original training routine that is part of the\n",
    "        C++ implementation of EdgeML used iterative hard thresholding (IHT),\n",
    "        gamma estimation through median heuristic and other tricks for\n",
    "        training ProtoNN. This module implements the same in Tensorflow\n",
    "        and python.\n",
    "\n",
    "        protoNNObj: An instance of ProtoNN class defining the forward\n",
    "            computation graph. The loss functions and training routines will be\n",
    "            attached to this instance.\n",
    "        regW, regB, regZ: Regularization constants for W, B, and\n",
    "            Z matrices of protoNN.\n",
    "        sparcityW, sparcityB, sparcityZ: Sparsity constraints\n",
    "            for W, B and Z matrices. A value between 0 (exclusive) and 1\n",
    "            (inclusive) is expected. A value of 1 indicates dense training.\n",
    "        learningRate: Initial learning rate for ADAM optimizer.\n",
    "        X, Y : Placeholders for data and labels.\n",
    "            X [-1, featureDimension]\n",
    "            Y [-1, num Labels]\n",
    "        lossType: ['l2', 'xentropy']\n",
    "        '''\n",
    "        self.protoNNObj = protoNNObj\n",
    "        self.__regW = regW\n",
    "        self.__regB = regB\n",
    "        self.__regZ = regZ\n",
    "        self.__sW = sparcityW\n",
    "        self.__sB = sparcityB\n",
    "        self.__sZ = sparcityZ\n",
    "        self.__lR = learningRate\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.sparseTraining = True\n",
    "        if (sparcityW == 1.0) and (sparcityB == 1.0) and (sparcityZ == 1.0):\n",
    "            self.sparseTraining = False\n",
    "            print(\"Sparse training disabled.\", file=sys.stderr)\n",
    "        # Define placeholders for sparse training\n",
    "        self.W_th = None\n",
    "        self.B_th = None\n",
    "        self.Z_th = None\n",
    "        self.__lossType = lossType\n",
    "        self.__validInit = False\n",
    "        self.__validInit = self.__validateInit()\n",
    "        self.__protoNNOut = protoNNObj(X, Y)\n",
    "        self.loss = self.__lossGraph()\n",
    "        self.trainStep = self.__trainGraph()\n",
    "        self.__hthOp = self.__getHardThresholdOp()\n",
    "        self.accuracy = protoNNObj.getAccuracyOp()\n",
    "\n",
    "    def __validateInit(self):\n",
    "        self.__validInit = False\n",
    "        msg = \"Sparsity value should be between\"\n",
    "        msg += \" 0 and 1 (both inclusive).\"\n",
    "        assert self.__sW >= 0. and self.__sW <= 1., 'W:' + msg\n",
    "        assert self.__sB >= 0. and self.__sB <= 1., 'B:' + msg\n",
    "        assert self.__sZ >= 0. and self.__sZ <= 1., 'Z:' + msg\n",
    "        d, dcap, m, L, _ = self.protoNNObj.getHyperParams()\n",
    "        msg = 'Y should be of dimension [-1, num labels/classes]'\n",
    "        msg += ' specified as part of ProtoNN object.'\n",
    "        assert (len(self.Y.shape)) == 2, msg\n",
    "        assert (self.Y.shape[1] == L), msg\n",
    "        msg = 'X should be of dimension [-1, featureDimension]'\n",
    "        msg += ' specified as part of ProtoNN object.'\n",
    "        assert (len(self.X.shape) == 2), msg\n",
    "        assert (self.X.shape[1] == d), msg\n",
    "        self.__validInit = True\n",
    "        msg = 'Values can be \\'l2\\', or \\'xentropy\\''\n",
    "        if self.__lossType not in ['l2', 'xentropy']:\n",
    "            raise ValueError(msg)\n",
    "        return True\n",
    "\n",
    "    def __lossGraph(self):\n",
    "        pnnOut = self.__protoNNOut\n",
    "        l1, l2, l3 = self.__regW, self.__regB, self.__regZ\n",
    "        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n",
    "        if self.__lossType == 'l2':\n",
    "            with tf.name_scope('protonn-l2-loss'):\n",
    "                loss_0 = tf.nn.l2_loss(self.Y - pnnOut)\n",
    "                reg = l1 * tf.nn.l2_loss(W) + l2 * tf.nn.l2_loss(B)\n",
    "                reg += l3 * tf.nn.l2_loss(Z)\n",
    "                loss = loss_0 + reg\n",
    "        elif self.__lossType == 'xentropy':\n",
    "            with tf.name_scope('protonn-xentropy-loss'):\n",
    "                loss_0 = tf.nn.softmax_cross_entropy_with_logits_v2(logits=pnnOut,\n",
    "                                                         labels=tf.stop_gradient(self.Y))\n",
    "                loss_0 = tf.reduce_mean(loss_0)\n",
    "                reg = l1 * tf.nn.l2_loss(W) + l2 * tf.nn.l2_loss(B)\n",
    "                reg += l3 * tf.nn.l2_loss(Z)\n",
    "                loss = loss_0 + reg\n",
    "        return loss\n",
    "\n",
    "    def __trainGraph(self):\n",
    "        with tf.name_scope('protonn-gradient-adam'):\n",
    "            trainStep = tf.train.AdamOptimizer(self.__lR)\n",
    "            trainStep = trainStep.minimize(self.loss)\n",
    "        return trainStep\n",
    "\n",
    "    def __getHardThresholdOp(self):\n",
    "        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n",
    "        self.W_th = tf.placeholder(tf.float32, name='W_th')\n",
    "        self.B_th = tf.placeholder(tf.float32, name='B_th')\n",
    "        self.Z_th = tf.placeholder(tf.float32, name='Z_th')\n",
    "        with tf.name_scope('hard-threshold-assignments'):\n",
    "            hard_thrsd_W = W.assign(self.W_th)\n",
    "            hard_thrsd_B = B.assign(self.B_th)\n",
    "            hard_thrsd_Z = Z.assign(self.Z_th)\n",
    "            hard_thrsd_op = tf.group(hard_thrsd_W, hard_thrsd_B, hard_thrsd_Z)\n",
    "        return hard_thrsd_op\n",
    "\n",
    "    def train(self, batchSize, totalEpochs, sess,\n",
    "              x_train, x_val, y_train, y_val, noInit=False,\n",
    "              redirFile=None, printStep=10, valStep=3):\n",
    "        '''\n",
    "        Performs dense training of ProtoNN followed by iterative hard\n",
    "        thresholding to enforce sparsity constraints.\n",
    "\n",
    "        batchSize: Batch size per update\n",
    "        totalEpochs: The number of epochs to run training for. One epoch is\n",
    "            defined as one pass over the entire training data.\n",
    "        sess: The Tensorflow session to use for running various graph\n",
    "            operators.\n",
    "        x_train, x_val, y_train, y_val: The numpy array containing train and\n",
    "            validation data. x data is assumed to in of shape [-1,\n",
    "            featureDimension] while y should have shape [-1, numberLabels].\n",
    "        noInit: By default, all the tensors of the computation graph are\n",
    "        initialized at the start of the training session. Set noInit=False to\n",
    "        disable this behaviour.\n",
    "        printStep: Number of batches between echoing of loss and train accuracy.\n",
    "        valStep: Number of epochs between evolutions on validation set.\n",
    "        '''\n",
    "        d, d_cap, m, L, gamma = self.protoNNObj.getHyperParams()\n",
    "        assert batchSize >= 1, 'Batch size should be positive integer'\n",
    "        assert totalEpochs >= 1, 'Total epochs should be positive integer'\n",
    "        assert x_train.ndim == 2, 'Expected training data to be of rank 2'\n",
    "        assert x_train.shape[1] == d, 'Expected x_train to be [-1, %d]' % d\n",
    "        assert x_val.ndim == 2, 'Expected validation data to be of rank 2'\n",
    "        assert x_val.shape[1] == d, 'Expected x_val to be [-1, %d]' % d\n",
    "        assert y_train.ndim == 2, 'Expected training labels to be of rank 2'\n",
    "        assert y_train.shape[1] == L, 'Expected y_train to be [-1, %d]' % L\n",
    "        assert y_val.ndim == 2, 'Expected validation labels to be of rank 2'\n",
    "        assert y_val.shape[1] == L, 'Expected y_val to be [-1, %d]' % L\n",
    "\n",
    "        # Numpy will throw asserts for arrays\n",
    "        if sess is None:\n",
    "            raise ValueError('sess must be valid Tensorflow session.')\n",
    "\n",
    "        trainNumBatches = int(np.ceil(len(x_train) / batchSize))\n",
    "        valNumBatches = int(np.ceil(len(x_val) / batchSize))\n",
    "        x_train_batches = np.array_split(x_train, trainNumBatches)\n",
    "        y_train_batches = np.array_split(y_train, trainNumBatches)\n",
    "        x_val_batches = np.array_split(x_val, valNumBatches)\n",
    "        y_val_batches = np.array_split(y_val, valNumBatches)\n",
    "        if not noInit:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        X, Y = self.X, self.Y\n",
    "        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n",
    "        for epoch in range(totalEpochs):\n",
    "            for i in range(len(x_train_batches)):\n",
    "                batch_x = x_train_batches[i]\n",
    "                batch_y = y_train_batches[i]\n",
    "                feed_dict = {\n",
    "                    X: batch_x,\n",
    "                    Y: batch_y\n",
    "                }\n",
    "                sess.run(self.trainStep, feed_dict=feed_dict)\n",
    "                if i % printStep == 0:\n",
    "                    loss, acc = sess.run([self.loss, self.accuracy],\n",
    "                                         feed_dict=feed_dict)\n",
    "                    msg = \"Epoch: %3d Batch: %3d\" % (epoch, i)\n",
    "                    msg += \" Loss: %3.5f Accuracy: %2.5f\" % (loss, acc)\n",
    "                    print(msg, file=redirFile)\n",
    "\n",
    "            # Perform Hard thresholding\n",
    "            if self.sparseTraining:\n",
    "                W_, B_, Z_ = sess.run([W, B, Z])\n",
    "                fd_thrsd = {\n",
    "                    self.W_th: hardThreshold(W_, self.__sW),\n",
    "                    self.B_th: hardThreshold(B_, self.__sB),\n",
    "                    self.Z_th: hardThreshold(Z_, self.__sZ)\n",
    "                }\n",
    "                sess.run(self.__hthOp, feed_dict=fd_thrsd)\n",
    "\n",
    "            if (epoch + 1) % valStep  == 0:\n",
    "                acc = 0.0\n",
    "                loss = 0.0\n",
    "                for j in range(len(x_val_batches)):\n",
    "                    batch_x = x_val_batches[j]\n",
    "                    batch_y = y_val_batches[j]\n",
    "                    feed_dict = {\n",
    "                        X: batch_x,\n",
    "                        Y: batch_y\n",
    "                    }\n",
    "                    acc_, loss_ = sess.run([self.accuracy, self.loss],\n",
    "                                           feed_dict=feed_dict)\n",
    "                    acc += acc_\n",
    "                    loss += loss_\n",
    "                acc /= len(y_val_batches)\n",
    "                loss /= len(y_val_batches)\n",
    "                print(\"Test Loss: %2.5f Accuracy: %2.5f\" % (loss, acc))\n",
    "\n",
    "\n",
    "\n",
    "class ProtoNN:\n",
    "    def __init__(self, inputDimension, projectionDimension, numPrototypes,\n",
    "                 numOutputLabels, gamma,\n",
    "                 W = None, B = None, Z = None):\n",
    "        '''\n",
    "        Forward computation graph for ProtoNN.\n",
    "\n",
    "        inputDimension: Input data dimension or feature dimension.\n",
    "        projectionDimension: hyperparameter\n",
    "        numPrototypes: hyperparameter\n",
    "        numOutputLabels: The number of output labels or classes\n",
    "        W, B, Z: Numpy matrices that can be used to initialize\n",
    "            projection matrix(W), prototype matrix (B) and prototype labels\n",
    "            matrix (B).\n",
    "            Expected Dimensions:\n",
    "                W   inputDimension (d) x projectionDimension (d_cap)\n",
    "                B   projectionDimension (d_cap) x numPrototypes (m)\n",
    "                Z   numOutputLabels (L) x numPrototypes (m)\n",
    "        '''\n",
    "        with tf.name_scope('protoNN') as ns:\n",
    "            self.__nscope = ns\n",
    "        self.__d = inputDimension\n",
    "        self.__d_cap = projectionDimension\n",
    "        self.__m = numPrototypes\n",
    "        self.__L = numOutputLabels\n",
    "\n",
    "        self.__inW = W\n",
    "        self.__inB = B\n",
    "        self.__inZ = Z\n",
    "        self.__inGamma = gamma\n",
    "        self.W, self.B, self.Z = None, None, None\n",
    "        self.gamma = None\n",
    "\n",
    "        self.__validInit = False\n",
    "        self.__initWBZ()\n",
    "        self.__initGamma()\n",
    "        self.__validateInit()\n",
    "        self.protoNNOut = None\n",
    "        self.predictions = None\n",
    "        self.accuracy = None\n",
    "\n",
    "    def __validateInit(self):\n",
    "        self.__validInit = False\n",
    "        errmsg = \"Dimensions mismatch! Should be W[d, d_cap]\"\n",
    "        errmsg += \", B[d_cap, m] and Z[L, m]\"\n",
    "        d, d_cap, m, L, _ = self.getHyperParams()\n",
    "        assert self.W.shape[0] == d, errmsg\n",
    "        assert self.W.shape[1] == d_cap, errmsg\n",
    "        assert self.B.shape[0] == d_cap, errmsg\n",
    "        assert self.B.shape[1] == m, errmsg\n",
    "        assert self.Z.shape[0] == L, errmsg\n",
    "        assert self.Z.shape[1] == m, errmsg\n",
    "        self.__validInit = True\n",
    "\n",
    "    def __initWBZ(self):\n",
    "        with tf.name_scope(self.__nscope):\n",
    "            W = self.__inW\n",
    "            if W is None:\n",
    "                W = tf.random_normal_initializer()\n",
    "                W = W([self.__d, self.__d_cap])\n",
    "            self.W = tf.Variable(W, name='W', dtype=tf.float32)\n",
    "\n",
    "            B = self.__inB\n",
    "            if B is None:\n",
    "                B = tf.random_uniform_initializer()\n",
    "                B = B([self.__d_cap, self.__m])\n",
    "            self.B = tf.Variable(B, name='B', dtype=tf.float32)\n",
    "\n",
    "            Z = self.__inZ\n",
    "            if Z is None:\n",
    "                Z = tf.random_normal_initializer()\n",
    "                Z = Z([self.__L, self.__m])\n",
    "            Z = tf.Variable(Z, name='Z', dtype=tf.float32)\n",
    "            self.Z = Z\n",
    "        return self.W, self.B, self.Z\n",
    "\n",
    "    def __initGamma(self):\n",
    "        with tf.name_scope(self.__nscope):\n",
    "            gamma = self.__inGamma\n",
    "            self.gamma = tf.constant(gamma, name='gamma')\n",
    "\n",
    "    def getHyperParams(self):\n",
    "        '''\n",
    "        Returns the model hyperparameters:\n",
    "            [inputDimension, projectionDimension,\n",
    "            numPrototypes, numOutputLabels, gamma]\n",
    "        '''\n",
    "        d = self.__d\n",
    "        dcap = self.__d_cap\n",
    "        m = self.__m\n",
    "        L = self.__L\n",
    "        return d, dcap, m, L, self.gamma\n",
    "\n",
    "    def getModelMatrices(self):\n",
    "        '''\n",
    "        Returns Tensorflow tensors of the model matrices, which\n",
    "        can then be evaluated to obtain corresponding numpy arrays.\n",
    "\n",
    "        These can then be exported as part of other implementations of\n",
    "        ProtonNN, for instance a C++ implementation or pure python\n",
    "        implementation.\n",
    "        Returns\n",
    "            [ProjectionMatrix (W), prototypeMatrix (B),\n",
    "             prototypeLabelsMatrix (Z), gamma]\n",
    "        '''\n",
    "        return self.W, self.B, self.Z, self.gamma\n",
    "\n",
    "    def __call__(self, X, Y=None):\n",
    "        '''\n",
    "        This method is responsible for construction of the forward computation\n",
    "        graph. The end point of the computation graph, or in other words the\n",
    "        output operator for the forward computation is returned. Additionally,\n",
    "        if the argument Y is provided, a classification accuracy operator with\n",
    "        Y as target will also be created. For this, Y is assumed to in one-hot\n",
    "        encoded format and the class with the maximum prediction score is\n",
    "        compared to the encoded class in Y.  This accuracy operator is returned\n",
    "        by getAccuracyOp() method. If a different accuracyOp is required, it\n",
    "        can be defined by overriding the createAccOp(protoNNScoresOut, Y)\n",
    "        method.\n",
    "\n",
    "        X: Input tensor or placeholder of shape [-1, inputDimension]\n",
    "        Y: Optional tensor or placeholder for targets (labels or classes).\n",
    "            Expected shape is [-1, numOutputLabels].\n",
    "        returns: The forward computation outputs, self.protoNNOut\n",
    "        '''\n",
    "        # This should never execute\n",
    "        assert self.__validInit is True, \"Initialization failed!\"\n",
    "        if self.protoNNOut is not None:\n",
    "            return self.protoNNOut\n",
    "\n",
    "        W, B, Z, gamma = self.W, self.B, self.Z, self.gamma\n",
    "        with tf.name_scope(self.__nscope):\n",
    "            WX = tf.matmul(X, W)\n",
    "            # Convert WX to tensor so that broadcasting can work\n",
    "            dim = [-1, WX.shape.as_list()[1], 1]\n",
    "            WX = tf.reshape(WX, dim)\n",
    "            dim = [1, B.shape.as_list()[0], -1]\n",
    "            B = tf.reshape(B, dim)\n",
    "            l2sim = B - WX\n",
    "            l2sim = tf.pow(l2sim, 2)\n",
    "            l2sim = tf.reduce_sum(l2sim, 1, keepdims=True)\n",
    "            self.l2sim = l2sim\n",
    "            gammal2sim = (-1 * gamma * gamma) * l2sim\n",
    "            M = tf.exp(gammal2sim)\n",
    "            dim = [1] + Z.shape.as_list()\n",
    "            Z = tf.reshape(Z, dim)\n",
    "            y = tf.multiply(Z, M)\n",
    "            y = tf.reduce_sum(y, 2, name='protoNNScoreOut')\n",
    "            self.protoNNOut = y\n",
    "            self.predictions = tf.argmax(y, 1, name='protoNNPredictions')\n",
    "            if Y is not None:\n",
    "                self.createAccOp(self.protoNNOut, Y)\n",
    "        return y\n",
    "\n",
    "    def createAccOp(self, outputs, target):\n",
    "        '''\n",
    "        Define an accuracy operation on ProtoNN's output scores and targets.\n",
    "        Here a simple classification accuracy operator is defined. More\n",
    "        complicated operators (for multiple label problems and so forth) can be\n",
    "        defined by overriding this method\n",
    "        '''\n",
    "        assert self.predictions is not None\n",
    "        target = tf.argmax(target, 1)\n",
    "        correctPrediction = tf.equal(self.predictions, target)\n",
    "        acc = tf.reduce_mean(tf.cast(correctPrediction, tf.float32),\n",
    "                             name='protoNNAccuracy')\n",
    "        self.accuracy = acc\n",
    "\n",
    "    def getPredictionsOp(self):\n",
    "        '''\n",
    "        The predictions operator is defined as argmax(protoNNScores) for each\n",
    "        prediction.\n",
    "        '''\n",
    "        return self.predictions\n",
    "\n",
    "    def getAccuracyOp(self):\n",
    "        '''\n",
    "        returns accuracyOp as defined by createAccOp. It defaults to\n",
    "        multi-class classification accuracy.\n",
    "        '''\n",
    "        msg = \"Accuracy operator not defined in graph. Did you provide Y as an\"\n",
    "        msg += \" argument to _call_?\"\n",
    "        assert self.accuracy is not None, msg\n",
    "        return self.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Dimension:  423\n",
      "Num classes:  2\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = r\"./experiments\"\n",
    "windowLen = 'data_w3'\n",
    "out = preprocessData(DATA_DIR,windowLen)\n",
    "dataDimension = out[0]\n",
    "numClasses = out[1]\n",
    "x_train, y_train = out[2], out[3]\n",
    "x_test, y_test = out[4], out[5]\n",
    "print(\"Feature Dimension: \", dataDimension)\n",
    "print(\"Num classes: \", numClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_DIR = r\"./experiments\"\n",
    "train, test = np.load(DATA_DIR + '/ttrain_data_w3.npy'), np.load(DATA_DIR + '/ttest_data_w3.npy')\n",
    "x_train, y_train = train[:, 1:], train[:, 0]\n",
    "x_test, y_test = test[:, 1:], test[:, 0]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "numClasses = max(y_train) - min(y_train) + 1\n",
    "numClasses = max(numClasses, max(y_test) - min(y_test) + 1)\n",
    "numClasses = int(numClasses)\n",
    "\n",
    "y_train = helper.to_onehot(y_train, numClasses)\n",
    "y_test = helper.to_onehot(y_test, numClasses)\n",
    "y_val = helper.to_onehot(y_val, numClasses)\n",
    "\n",
    "dataDimension = x_train.shape[1]\n",
    "numClasses = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007586"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROJECTION_DIM = 5 #d^\n",
    "NUM_PROTOTYPES = 40 #m\n",
    "REG_W = 0.000005\n",
    "REG_B = 0.0\n",
    "REG_Z = 0.00005\n",
    "SPAR_W = 1.0\n",
    "SPAR_B = 0.8\n",
    "SPAR_Z = 0.8\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 600\n",
    "BATCH_SIZE = 2048\n",
    "GAMMA = 0.007586\n",
    "W, B, gamma = getGamma(GAMMA, PROJECTION_DIM, dataDimension,\n",
    "                       NUM_PROTOTYPES, x_train)\n",
    "\n",
    "gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nadit\\anaconda3\\envs\\ProtoNN\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch:   0 Batch:   0 Loss: 4993.86279 Accuracy: 0.49634\n",
      "Epoch:   1 Batch:   0 Loss: 2749.55737 Accuracy: 0.49634\n",
      "Epoch:   2 Batch:   0 Loss: 1998.62317 Accuracy: 0.49634\n",
      "Epoch:   3 Batch:   0 Loss: 1437.14758 Accuracy: 0.49634\n",
      "Epoch:   4 Batch:   0 Loss: 1033.77112 Accuracy: 0.49634\n",
      "Epoch:   5 Batch:   0 Loss: 749.09021 Accuracy: 0.49634\n",
      "Epoch:   6 Batch:   0 Loss: 555.66095 Accuracy: 0.49634\n",
      "Epoch:   7 Batch:   0 Loss: 428.96310 Accuracy: 0.49634\n",
      "Epoch:   8 Batch:   0 Loss: 349.11774 Accuracy: 0.49634\n",
      "Epoch:   9 Batch:   0 Loss: 300.78848 Accuracy: 0.49634\n",
      "Test Loss: 270.03414 Accuracy: 0.50013\n",
      "Epoch:  10 Batch:   0 Loss: 272.72180 Accuracy: 0.49634\n",
      "Epoch:  11 Batch:   0 Loss: 257.06842 Accuracy: 0.49634\n",
      "Epoch:  12 Batch:   0 Loss: 248.63916 Accuracy: 0.49634\n",
      "Epoch:  13 Batch:   0 Loss: 244.17857 Accuracy: 0.49634\n",
      "Epoch:  14 Batch:   0 Loss: 241.80022 Accuracy: 0.49634\n",
      "Epoch:  15 Batch:   0 Loss: 240.44452 Accuracy: 0.55799\n",
      "Epoch:  16 Batch:   0 Loss: 239.56340 Accuracy: 0.61338\n",
      "Epoch:  17 Batch:   0 Loss: 238.86784 Accuracy: 0.64786\n",
      "Epoch:  18 Batch:   0 Loss: 238.25085 Accuracy: 0.67398\n",
      "Epoch:  19 Batch:   0 Loss: 237.66809 Accuracy: 0.68339\n",
      "Test Loss: 234.46708 Accuracy: 0.61513\n",
      "Epoch:  20 Batch:   0 Loss: 237.08467 Accuracy: 0.69175\n",
      "Epoch:  21 Batch:   0 Loss: 236.49968 Accuracy: 0.70219\n",
      "Epoch:  22 Batch:   0 Loss: 235.91508 Accuracy: 0.70010\n",
      "Epoch:  23 Batch:   0 Loss: 235.32047 Accuracy: 0.70219\n",
      "Epoch:  24 Batch:   0 Loss: 234.72258 Accuracy: 0.70324\n",
      "Epoch:  25 Batch:   0 Loss: 234.11441 Accuracy: 0.70219\n",
      "Epoch:  26 Batch:   0 Loss: 233.49335 Accuracy: 0.70428\n",
      "Epoch:  27 Batch:   0 Loss: 232.84534 Accuracy: 0.70637\n",
      "Epoch:  28 Batch:   0 Loss: 232.19351 Accuracy: 0.71264\n",
      "Epoch:  29 Batch:   0 Loss: 231.52650 Accuracy: 0.71264\n",
      "Test Loss: 230.83628 Accuracy: 0.62622\n",
      "Epoch:  30 Batch:   0 Loss: 230.83481 Accuracy: 0.71160\n",
      "Epoch:  31 Batch:   0 Loss: 230.12355 Accuracy: 0.71160\n",
      "Epoch:  32 Batch:   0 Loss: 229.40221 Accuracy: 0.70742\n",
      "Epoch:  33 Batch:   0 Loss: 228.66486 Accuracy: 0.70742\n",
      "Epoch:  34 Batch:   0 Loss: 227.91028 Accuracy: 0.70637\n",
      "Epoch:  35 Batch:   0 Loss: 227.12080 Accuracy: 0.70428\n",
      "Epoch:  36 Batch:   0 Loss: 226.31847 Accuracy: 0.70533\n",
      "Epoch:  37 Batch:   0 Loss: 225.48512 Accuracy: 0.70533\n",
      "Epoch:  38 Batch:   0 Loss: 224.63513 Accuracy: 0.70428\n",
      "Epoch:  39 Batch:   0 Loss: 223.75633 Accuracy: 0.70637\n",
      "Test Loss: 226.41135 Accuracy: 0.62916\n",
      "Epoch:  40 Batch:   0 Loss: 222.86334 Accuracy: 0.70637\n",
      "Epoch:  41 Batch:   0 Loss: 221.93924 Accuracy: 0.70742\n",
      "Epoch:  42 Batch:   0 Loss: 220.99910 Accuracy: 0.71160\n",
      "Epoch:  43 Batch:   0 Loss: 220.04152 Accuracy: 0.70742\n",
      "Epoch:  44 Batch:   0 Loss: 219.05353 Accuracy: 0.70742\n",
      "Epoch:  45 Batch:   0 Loss: 218.03905 Accuracy: 0.70846\n",
      "Epoch:  46 Batch:   0 Loss: 217.00914 Accuracy: 0.70951\n",
      "Epoch:  47 Batch:   0 Loss: 215.95482 Accuracy: 0.71160\n",
      "Epoch:  48 Batch:   0 Loss: 214.87555 Accuracy: 0.71055\n",
      "Epoch:  49 Batch:   0 Loss: 213.76242 Accuracy: 0.71578\n",
      "Test Loss: 220.40401 Accuracy: 0.63451\n",
      "Epoch:  50 Batch:   0 Loss: 212.62213 Accuracy: 0.71891\n",
      "Epoch:  51 Batch:   0 Loss: 211.45370 Accuracy: 0.72100\n",
      "Epoch:  52 Batch:   0 Loss: 210.26555 Accuracy: 0.72205\n",
      "Epoch:  53 Batch:   0 Loss: 209.04910 Accuracy: 0.73041\n",
      "Epoch:  54 Batch:   0 Loss: 207.80077 Accuracy: 0.73772\n",
      "Epoch:  55 Batch:   0 Loss: 206.52106 Accuracy: 0.73981\n",
      "Epoch:  56 Batch:   0 Loss: 205.21077 Accuracy: 0.74399\n",
      "Epoch:  57 Batch:   0 Loss: 203.87328 Accuracy: 0.75026\n",
      "Epoch:  58 Batch:   0 Loss: 202.51180 Accuracy: 0.75653\n",
      "Epoch:  59 Batch:   0 Loss: 201.11278 Accuracy: 0.76071\n",
      "Test Loss: 212.15929 Accuracy: 0.64817\n",
      "Epoch:  60 Batch:   0 Loss: 199.68068 Accuracy: 0.76385\n",
      "Epoch:  61 Batch:   0 Loss: 198.23412 Accuracy: 0.76594\n",
      "Epoch:  62 Batch:   0 Loss: 196.74600 Accuracy: 0.77534\n",
      "Epoch:  63 Batch:   0 Loss: 195.22778 Accuracy: 0.77952\n",
      "Epoch:  64 Batch:   0 Loss: 193.67773 Accuracy: 0.78265\n",
      "Epoch:  65 Batch:   0 Loss: 192.10312 Accuracy: 0.78788\n",
      "Epoch:  66 Batch:   0 Loss: 190.49942 Accuracy: 0.79310\n",
      "Epoch:  67 Batch:   0 Loss: 188.87070 Accuracy: 0.79833\n",
      "Epoch:  68 Batch:   0 Loss: 187.22421 Accuracy: 0.80146\n",
      "Epoch:  69 Batch:   0 Loss: 185.55643 Accuracy: 0.80982\n",
      "Test Loss: 201.83920 Accuracy: 0.69169\n",
      "Epoch:  70 Batch:   0 Loss: 183.86925 Accuracy: 0.81296\n",
      "Epoch:  71 Batch:   0 Loss: 182.16460 Accuracy: 0.82027\n",
      "Epoch:  72 Batch:   0 Loss: 180.44447 Accuracy: 0.82341\n",
      "Epoch:  73 Batch:   0 Loss: 178.70424 Accuracy: 0.82654\n",
      "Epoch:  74 Batch:   0 Loss: 176.95084 Accuracy: 0.83490\n",
      "Epoch:  75 Batch:   0 Loss: 175.19724 Accuracy: 0.84013\n",
      "Epoch:  76 Batch:   0 Loss: 173.43121 Accuracy: 0.84431\n",
      "Epoch:  77 Batch:   0 Loss: 171.66452 Accuracy: 0.85162\n",
      "Epoch:  78 Batch:   0 Loss: 169.89458 Accuracy: 0.85684\n",
      "Epoch:  79 Batch:   0 Loss: 168.11366 Accuracy: 0.85893\n",
      "Test Loss: 191.06265 Accuracy: 0.72584\n",
      "Epoch:  80 Batch:   0 Loss: 166.33786 Accuracy: 0.86207\n",
      "Epoch:  81 Batch:   0 Loss: 164.55142 Accuracy: 0.87043\n",
      "Epoch:  82 Batch:   0 Loss: 162.78859 Accuracy: 0.87252\n",
      "Epoch:  83 Batch:   0 Loss: 161.03091 Accuracy: 0.87879\n",
      "Epoch:  84 Batch:   0 Loss: 159.27979 Accuracy: 0.88192\n",
      "Epoch:  85 Batch:   0 Loss: 157.53546 Accuracy: 0.88506\n",
      "Epoch:  86 Batch:   0 Loss: 155.79594 Accuracy: 0.88715\n",
      "Epoch:  87 Batch:   0 Loss: 154.06038 Accuracy: 0.89133\n",
      "Epoch:  88 Batch:   0 Loss: 152.33081 Accuracy: 0.89237\n",
      "Epoch:  89 Batch:   0 Loss: 150.61591 Accuracy: 0.89342\n",
      "Test Loss: 181.65363 Accuracy: 0.74418\n",
      "Epoch:  90 Batch:   0 Loss: 148.91380 Accuracy: 0.89760\n",
      "Epoch:  91 Batch:   0 Loss: 147.22658 Accuracy: 0.89864\n",
      "Epoch:  92 Batch:   0 Loss: 145.55188 Accuracy: 0.89864\n",
      "Epoch:  93 Batch:   0 Loss: 143.89438 Accuracy: 0.89864\n",
      "Epoch:  94 Batch:   0 Loss: 142.25250 Accuracy: 0.89969\n",
      "Epoch:  95 Batch:   0 Loss: 140.62679 Accuracy: 0.90073\n",
      "Epoch:  96 Batch:   0 Loss: 139.00995 Accuracy: 0.90073\n",
      "Epoch:  97 Batch:   0 Loss: 137.41667 Accuracy: 0.90073\n",
      "Epoch:  98 Batch:   0 Loss: 135.83379 Accuracy: 0.90073\n",
      "Epoch:  99 Batch:   0 Loss: 134.27809 Accuracy: 0.90073\n",
      "Test Loss: 174.32729 Accuracy: 0.75502\n",
      "Epoch: 100 Batch:   0 Loss: 132.73499 Accuracy: 0.90073\n",
      "Epoch: 101 Batch:   0 Loss: 131.21385 Accuracy: 0.90282\n",
      "Epoch: 102 Batch:   0 Loss: 129.70670 Accuracy: 0.90282\n",
      "Epoch: 103 Batch:   0 Loss: 128.21835 Accuracy: 0.90282\n",
      "Epoch: 104 Batch:   0 Loss: 126.75143 Accuracy: 0.90282\n",
      "Epoch: 105 Batch:   0 Loss: 125.30426 Accuracy: 0.90282\n",
      "Epoch: 106 Batch:   0 Loss: 123.87708 Accuracy: 0.90282\n",
      "Epoch: 107 Batch:   0 Loss: 122.46992 Accuracy: 0.90282\n",
      "Epoch: 108 Batch:   0 Loss: 121.08298 Accuracy: 0.90282\n",
      "Epoch: 109 Batch:   0 Loss: 119.71638 Accuracy: 0.90491\n",
      "Test Loss: 169.04307 Accuracy: 0.75890\n",
      "Epoch: 110 Batch:   0 Loss: 118.37011 Accuracy: 0.90491\n",
      "Epoch: 111 Batch:   0 Loss: 117.04427 Accuracy: 0.90491\n",
      "Epoch: 112 Batch:   0 Loss: 115.73890 Accuracy: 0.90596\n",
      "Epoch: 113 Batch:   0 Loss: 114.45397 Accuracy: 0.90596\n",
      "Epoch: 114 Batch:   0 Loss: 113.18950 Accuracy: 0.90805\n",
      "Epoch: 115 Batch:   0 Loss: 111.94548 Accuracy: 0.91014\n",
      "Epoch: 116 Batch:   0 Loss: 110.72182 Accuracy: 0.91014\n",
      "Epoch: 117 Batch:   0 Loss: 109.51849 Accuracy: 0.91118\n",
      "Epoch: 118 Batch:   0 Loss: 108.33544 Accuracy: 0.91118\n",
      "Epoch: 119 Batch:   0 Loss: 107.17253 Accuracy: 0.91118\n",
      "Test Loss: 165.47218 Accuracy: 0.75569\n",
      "Epoch: 120 Batch:   0 Loss: 106.02970 Accuracy: 0.91118\n",
      "Epoch: 121 Batch:   0 Loss: 104.90680 Accuracy: 0.91327\n",
      "Epoch: 122 Batch:   0 Loss: 103.80369 Accuracy: 0.91327\n",
      "Epoch: 123 Batch:   0 Loss: 102.72034 Accuracy: 0.91327\n",
      "Epoch: 124 Batch:   0 Loss: 101.65646 Accuracy: 0.91327\n",
      "Epoch: 125 Batch:   0 Loss: 100.60964 Accuracy: 0.91327\n",
      "Epoch: 126 Batch:   0 Loss: 99.58389 Accuracy: 0.91327\n",
      "Epoch: 127 Batch:   0 Loss: 98.57716 Accuracy: 0.91432\n",
      "Epoch: 128 Batch:   0 Loss: 97.58929 Accuracy: 0.91536\n",
      "Epoch: 129 Batch:   0 Loss: 96.62003 Accuracy: 0.91641\n",
      "Test Loss: 163.19832 Accuracy: 0.75636\n",
      "Epoch: 130 Batch:   0 Loss: 95.66925 Accuracy: 0.91641\n",
      "Epoch: 131 Batch:   0 Loss: 94.73673 Accuracy: 0.91745\n",
      "Epoch: 132 Batch:   0 Loss: 93.82223 Accuracy: 0.91745\n",
      "Epoch: 133 Batch:   0 Loss: 92.92555 Accuracy: 0.91745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 134 Batch:   0 Loss: 92.04651 Accuracy: 0.91850\n",
      "Epoch: 135 Batch:   0 Loss: 91.18482 Accuracy: 0.91954\n",
      "Epoch: 136 Batch:   0 Loss: 90.34028 Accuracy: 0.92059\n",
      "Epoch: 137 Batch:   0 Loss: 89.51270 Accuracy: 0.92059\n",
      "Epoch: 138 Batch:   0 Loss: 88.70174 Accuracy: 0.92059\n",
      "Epoch: 139 Batch:   0 Loss: 87.90727 Accuracy: 0.92268\n",
      "Test Loss: 161.81674 Accuracy: 0.75622\n",
      "Epoch: 140 Batch:   0 Loss: 87.12898 Accuracy: 0.92268\n",
      "Epoch: 141 Batch:   0 Loss: 86.36668 Accuracy: 0.92372\n",
      "Epoch: 142 Batch:   0 Loss: 85.62008 Accuracy: 0.92372\n",
      "Epoch: 143 Batch:   0 Loss: 84.88896 Accuracy: 0.92372\n",
      "Epoch: 144 Batch:   0 Loss: 84.17307 Accuracy: 0.92372\n",
      "Epoch: 145 Batch:   0 Loss: 83.47214 Accuracy: 0.92476\n",
      "Epoch: 146 Batch:   0 Loss: 82.78599 Accuracy: 0.92476\n",
      "Epoch: 147 Batch:   0 Loss: 82.11426 Accuracy: 0.92476\n",
      "Epoch: 148 Batch:   0 Loss: 81.45680 Accuracy: 0.92581\n",
      "Epoch: 149 Batch:   0 Loss: 80.81331 Accuracy: 0.92685\n",
      "Test Loss: 161.00150 Accuracy: 0.75622\n",
      "Epoch: 150 Batch:   0 Loss: 80.18355 Accuracy: 0.92685\n",
      "Epoch: 151 Batch:   0 Loss: 79.56727 Accuracy: 0.92685\n",
      "Epoch: 152 Batch:   0 Loss: 78.96423 Accuracy: 0.92685\n",
      "Epoch: 153 Batch:   0 Loss: 78.37415 Accuracy: 0.92790\n",
      "Epoch: 154 Batch:   0 Loss: 77.79682 Accuracy: 0.92790\n",
      "Epoch: 155 Batch:   0 Loss: 77.23199 Accuracy: 0.92790\n",
      "Epoch: 156 Batch:   0 Loss: 76.67941 Accuracy: 0.92790\n",
      "Epoch: 157 Batch:   0 Loss: 76.13882 Accuracy: 0.92685\n",
      "Epoch: 158 Batch:   0 Loss: 75.61000 Accuracy: 0.92685\n",
      "Epoch: 159 Batch:   0 Loss: 75.09271 Accuracy: 0.92581\n",
      "Test Loss: 160.52489 Accuracy: 0.75596\n",
      "Epoch: 160 Batch:   0 Loss: 74.58669 Accuracy: 0.92581\n",
      "Epoch: 161 Batch:   0 Loss: 74.09174 Accuracy: 0.92581\n",
      "Epoch: 162 Batch:   0 Loss: 73.60761 Accuracy: 0.92685\n",
      "Epoch: 163 Batch:   0 Loss: 73.13405 Accuracy: 0.92894\n",
      "Epoch: 164 Batch:   0 Loss: 72.67085 Accuracy: 0.92894\n",
      "Epoch: 165 Batch:   0 Loss: 72.21780 Accuracy: 0.92999\n",
      "Epoch: 166 Batch:   0 Loss: 71.77466 Accuracy: 0.92999\n",
      "Epoch: 167 Batch:   0 Loss: 71.34119 Accuracy: 0.92999\n",
      "Epoch: 168 Batch:   0 Loss: 70.91716 Accuracy: 0.92999\n",
      "Epoch: 169 Batch:   0 Loss: 70.50245 Accuracy: 0.92999\n",
      "Test Loss: 160.25177 Accuracy: 0.75850\n",
      "Epoch: 170 Batch:   0 Loss: 70.09673 Accuracy: 0.92999\n",
      "Epoch: 171 Batch:   0 Loss: 69.69986 Accuracy: 0.92999\n",
      "Epoch: 172 Batch:   0 Loss: 69.31162 Accuracy: 0.92999\n",
      "Epoch: 173 Batch:   0 Loss: 68.93180 Accuracy: 0.92999\n",
      "Epoch: 174 Batch:   0 Loss: 68.56019 Accuracy: 0.92999\n",
      "Epoch: 175 Batch:   0 Loss: 68.19659 Accuracy: 0.92999\n",
      "Epoch: 176 Batch:   0 Loss: 67.84084 Accuracy: 0.92999\n",
      "Epoch: 177 Batch:   0 Loss: 67.49271 Accuracy: 0.92999\n",
      "Epoch: 178 Batch:   0 Loss: 67.15202 Accuracy: 0.93103\n",
      "Epoch: 179 Batch:   0 Loss: 66.81860 Accuracy: 0.93103\n",
      "Test Loss: 160.12003 Accuracy: 0.76132\n",
      "Epoch: 180 Batch:   0 Loss: 66.49226 Accuracy: 0.93103\n",
      "Epoch: 181 Batch:   0 Loss: 66.17281 Accuracy: 0.93208\n",
      "Epoch: 182 Batch:   0 Loss: 65.86008 Accuracy: 0.93208\n",
      "Epoch: 183 Batch:   0 Loss: 65.55389 Accuracy: 0.93312\n",
      "Epoch: 184 Batch:   0 Loss: 65.25409 Accuracy: 0.93312\n",
      "Epoch: 185 Batch:   0 Loss: 64.96051 Accuracy: 0.93417\n",
      "Epoch: 186 Batch:   0 Loss: 64.67299 Accuracy: 0.93312\n",
      "Epoch: 187 Batch:   0 Loss: 64.39137 Accuracy: 0.93312\n",
      "Epoch: 188 Batch:   0 Loss: 64.11546 Accuracy: 0.93312\n",
      "Epoch: 189 Batch:   0 Loss: 63.84516 Accuracy: 0.93312\n",
      "Test Loss: 160.12887 Accuracy: 0.76346\n",
      "Epoch: 190 Batch:   0 Loss: 63.58031 Accuracy: 0.93312\n",
      "Epoch: 191 Batch:   0 Loss: 63.32075 Accuracy: 0.93208\n",
      "Epoch: 192 Batch:   0 Loss: 63.06633 Accuracy: 0.93208\n",
      "Epoch: 193 Batch:   0 Loss: 62.81697 Accuracy: 0.93208\n",
      "Epoch: 194 Batch:   0 Loss: 62.57244 Accuracy: 0.93417\n",
      "Epoch: 195 Batch:   0 Loss: 62.33270 Accuracy: 0.93417\n",
      "Epoch: 196 Batch:   0 Loss: 62.09757 Accuracy: 0.93417\n",
      "Epoch: 197 Batch:   0 Loss: 61.86694 Accuracy: 0.93521\n",
      "Epoch: 198 Batch:   0 Loss: 61.64067 Accuracy: 0.93521\n",
      "Epoch: 199 Batch:   0 Loss: 61.41867 Accuracy: 0.93521\n",
      "Test Loss: 160.31971 Accuracy: 0.76373\n",
      "Epoch: 200 Batch:   0 Loss: 61.20081 Accuracy: 0.93521\n",
      "Epoch: 201 Batch:   0 Loss: 60.98697 Accuracy: 0.93521\n",
      "Epoch: 202 Batch:   0 Loss: 60.77703 Accuracy: 0.93521\n",
      "Epoch: 203 Batch:   0 Loss: 60.57091 Accuracy: 0.93521\n",
      "Epoch: 204 Batch:   0 Loss: 60.36848 Accuracy: 0.93521\n",
      "Epoch: 205 Batch:   0 Loss: 60.16966 Accuracy: 0.93521\n",
      "Epoch: 206 Batch:   0 Loss: 59.97435 Accuracy: 0.93521\n",
      "Epoch: 207 Batch:   0 Loss: 59.78244 Accuracy: 0.93521\n",
      "Epoch: 208 Batch:   0 Loss: 59.59386 Accuracy: 0.93626\n",
      "Epoch: 209 Batch:   0 Loss: 59.40854 Accuracy: 0.93626\n",
      "Test Loss: 160.73339 Accuracy: 0.76374\n",
      "Epoch: 210 Batch:   0 Loss: 59.22636 Accuracy: 0.93626\n",
      "Epoch: 211 Batch:   0 Loss: 59.04726 Accuracy: 0.93626\n",
      "Epoch: 212 Batch:   0 Loss: 58.87113 Accuracy: 0.93626\n",
      "Epoch: 213 Batch:   0 Loss: 58.69794 Accuracy: 0.93626\n",
      "Epoch: 214 Batch:   0 Loss: 58.52755 Accuracy: 0.93626\n",
      "Epoch: 215 Batch:   0 Loss: 58.35994 Accuracy: 0.93626\n",
      "Epoch: 216 Batch:   0 Loss: 58.19500 Accuracy: 0.93626\n",
      "Epoch: 217 Batch:   0 Loss: 58.03266 Accuracy: 0.93626\n",
      "Epoch: 218 Batch:   0 Loss: 57.87286 Accuracy: 0.93626\n",
      "Epoch: 219 Batch:   0 Loss: 57.71550 Accuracy: 0.93626\n",
      "Test Loss: 161.28097 Accuracy: 0.76347\n",
      "Epoch: 220 Batch:   0 Loss: 57.56055 Accuracy: 0.93730\n",
      "Epoch: 221 Batch:   0 Loss: 57.40789 Accuracy: 0.93730\n",
      "Epoch: 222 Batch:   0 Loss: 57.25750 Accuracy: 0.93730\n",
      "Epoch: 223 Batch:   0 Loss: 57.10890 Accuracy: 0.93730\n",
      "Epoch: 224 Batch:   0 Loss: 56.96321 Accuracy: 0.93730\n",
      "Epoch: 225 Batch:   0 Loss: 56.81962 Accuracy: 0.93730\n",
      "Epoch: 226 Batch:   0 Loss: 56.67814 Accuracy: 0.93730\n",
      "Epoch: 227 Batch:   0 Loss: 56.53865 Accuracy: 0.93730\n",
      "Epoch: 228 Batch:   0 Loss: 56.40118 Accuracy: 0.93730\n",
      "Epoch: 229 Batch:   0 Loss: 56.26565 Accuracy: 0.93730\n",
      "Test Loss: 161.58286 Accuracy: 0.76374\n",
      "Epoch: 230 Batch:   0 Loss: 56.13203 Accuracy: 0.93730\n",
      "Epoch: 231 Batch:   0 Loss: 56.00026 Accuracy: 0.93730\n",
      "Epoch: 232 Batch:   0 Loss: 55.87035 Accuracy: 0.93730\n",
      "Epoch: 233 Batch:   0 Loss: 55.74224 Accuracy: 0.93730\n",
      "Epoch: 234 Batch:   0 Loss: 55.61590 Accuracy: 0.93730\n",
      "Epoch: 235 Batch:   0 Loss: 55.49128 Accuracy: 0.93835\n",
      "Epoch: 236 Batch:   0 Loss: 55.36836 Accuracy: 0.93835\n",
      "Epoch: 237 Batch:   0 Loss: 55.24710 Accuracy: 0.93835\n",
      "Epoch: 238 Batch:   0 Loss: 55.12748 Accuracy: 0.93939\n",
      "Epoch: 239 Batch:   0 Loss: 55.00946 Accuracy: 0.93939\n",
      "Test Loss: 161.77756 Accuracy: 0.76294\n",
      "Epoch: 240 Batch:   0 Loss: 54.89300 Accuracy: 0.93939\n",
      "Epoch: 241 Batch:   0 Loss: 54.77806 Accuracy: 0.93939\n",
      "Epoch: 242 Batch:   0 Loss: 54.66463 Accuracy: 0.93939\n",
      "Epoch: 243 Batch:   0 Loss: 54.55269 Accuracy: 0.94044\n",
      "Epoch: 244 Batch:   0 Loss: 54.44218 Accuracy: 0.94044\n",
      "Epoch: 245 Batch:   0 Loss: 54.33308 Accuracy: 0.94044\n",
      "Epoch: 246 Batch:   0 Loss: 54.22536 Accuracy: 0.94044\n",
      "Epoch: 247 Batch:   0 Loss: 54.11900 Accuracy: 0.94148\n",
      "Epoch: 248 Batch:   0 Loss: 54.01397 Accuracy: 0.94253\n",
      "Epoch: 249 Batch:   0 Loss: 53.91022 Accuracy: 0.94253\n",
      "Test Loss: 162.05757 Accuracy: 0.76508\n",
      "Epoch: 250 Batch:   0 Loss: 53.80776 Accuracy: 0.94253\n",
      "Epoch: 251 Batch:   0 Loss: 53.70656 Accuracy: 0.94253\n",
      "Epoch: 252 Batch:   0 Loss: 53.60656 Accuracy: 0.94253\n",
      "Epoch: 253 Batch:   0 Loss: 53.50775 Accuracy: 0.94357\n",
      "Epoch: 254 Batch:   0 Loss: 53.41014 Accuracy: 0.94357\n",
      "Epoch: 255 Batch:   0 Loss: 53.31368 Accuracy: 0.94462\n",
      "Epoch: 256 Batch:   0 Loss: 53.21835 Accuracy: 0.94462\n",
      "Epoch: 257 Batch:   0 Loss: 53.12411 Accuracy: 0.94462\n",
      "Epoch: 258 Batch:   0 Loss: 53.03098 Accuracy: 0.94566\n",
      "Epoch: 259 Batch:   0 Loss: 52.93890 Accuracy: 0.94671\n",
      "Test Loss: 162.47156 Accuracy: 0.76683\n",
      "Epoch: 260 Batch:   0 Loss: 52.84789 Accuracy: 0.94671\n",
      "Epoch: 261 Batch:   0 Loss: 52.75790 Accuracy: 0.94671\n",
      "Epoch: 262 Batch:   0 Loss: 52.66891 Accuracy: 0.94671\n",
      "Epoch: 263 Batch:   0 Loss: 52.58092 Accuracy: 0.94671\n",
      "Epoch: 264 Batch:   0 Loss: 52.49391 Accuracy: 0.94671\n",
      "Epoch: 265 Batch:   0 Loss: 52.40786 Accuracy: 0.94671\n",
      "Epoch: 266 Batch:   0 Loss: 52.32273 Accuracy: 0.94671\n",
      "Epoch: 267 Batch:   0 Loss: 52.23854 Accuracy: 0.94671\n",
      "Epoch: 268 Batch:   0 Loss: 52.15524 Accuracy: 0.94671\n",
      "Epoch: 269 Batch:   0 Loss: 52.07285 Accuracy: 0.94671\n",
      "Test Loss: 163.01913 Accuracy: 0.76723\n",
      "Epoch: 270 Batch:   0 Loss: 51.99133 Accuracy: 0.94671\n",
      "Epoch: 271 Batch:   0 Loss: 51.91068 Accuracy: 0.94671\n",
      "Epoch: 272 Batch:   0 Loss: 51.83088 Accuracy: 0.94671\n",
      "Epoch: 273 Batch:   0 Loss: 51.75191 Accuracy: 0.94671\n",
      "Epoch: 274 Batch:   0 Loss: 51.67376 Accuracy: 0.94671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 275 Batch:   0 Loss: 51.59643 Accuracy: 0.94671\n",
      "Epoch: 276 Batch:   0 Loss: 51.51987 Accuracy: 0.94671\n",
      "Epoch: 277 Batch:   0 Loss: 51.44411 Accuracy: 0.94671\n",
      "Epoch: 278 Batch:   0 Loss: 51.36912 Accuracy: 0.94671\n",
      "Epoch: 279 Batch:   0 Loss: 51.29488 Accuracy: 0.94671\n",
      "Test Loss: 163.68580 Accuracy: 0.76576\n",
      "Epoch: 280 Batch:   0 Loss: 51.22139 Accuracy: 0.94671\n",
      "Epoch: 281 Batch:   0 Loss: 51.14862 Accuracy: 0.94671\n",
      "Epoch: 282 Batch:   0 Loss: 51.07660 Accuracy: 0.94671\n",
      "Epoch: 283 Batch:   0 Loss: 51.00527 Accuracy: 0.94671\n",
      "Epoch: 284 Batch:   0 Loss: 50.93464 Accuracy: 0.94671\n",
      "Epoch: 285 Batch:   0 Loss: 50.86470 Accuracy: 0.94671\n",
      "Epoch: 286 Batch:   0 Loss: 50.79547 Accuracy: 0.94671\n",
      "Epoch: 287 Batch:   0 Loss: 50.72689 Accuracy: 0.94671\n",
      "Epoch: 288 Batch:   0 Loss: 50.65896 Accuracy: 0.94671\n",
      "Epoch: 289 Batch:   0 Loss: 50.59170 Accuracy: 0.94671\n",
      "Test Loss: 164.45956 Accuracy: 0.76616\n",
      "Epoch: 290 Batch:   0 Loss: 50.52507 Accuracy: 0.94671\n",
      "Epoch: 291 Batch:   0 Loss: 50.45908 Accuracy: 0.94671\n",
      "Epoch: 292 Batch:   0 Loss: 50.39371 Accuracy: 0.94671\n",
      "Epoch: 293 Batch:   0 Loss: 50.32895 Accuracy: 0.94775\n",
      "Epoch: 294 Batch:   0 Loss: 50.26478 Accuracy: 0.94775\n",
      "Epoch: 295 Batch:   0 Loss: 50.20124 Accuracy: 0.94775\n",
      "Epoch: 296 Batch:   0 Loss: 50.13828 Accuracy: 0.94775\n",
      "Epoch: 297 Batch:   0 Loss: 50.07590 Accuracy: 0.94775\n",
      "Epoch: 298 Batch:   0 Loss: 50.01409 Accuracy: 0.94775\n",
      "Epoch: 299 Batch:   0 Loss: 49.95284 Accuracy: 0.94775\n",
      "Test Loss: 165.33182 Accuracy: 0.76616\n",
      "Epoch: 300 Batch:   0 Loss: 49.89216 Accuracy: 0.94775\n",
      "Epoch: 301 Batch:   0 Loss: 49.83203 Accuracy: 0.94775\n",
      "Epoch: 302 Batch:   0 Loss: 49.77243 Accuracy: 0.94775\n",
      "Epoch: 303 Batch:   0 Loss: 49.71339 Accuracy: 0.94775\n",
      "Epoch: 304 Batch:   0 Loss: 49.65485 Accuracy: 0.94775\n",
      "Epoch: 305 Batch:   0 Loss: 49.59684 Accuracy: 0.94775\n",
      "Epoch: 306 Batch:   0 Loss: 49.53935 Accuracy: 0.94775\n",
      "Epoch: 307 Batch:   0 Loss: 49.48235 Accuracy: 0.94775\n",
      "Epoch: 308 Batch:   0 Loss: 49.42586 Accuracy: 0.94775\n",
      "Epoch: 309 Batch:   0 Loss: 49.36985 Accuracy: 0.94775\n",
      "Test Loss: 166.28657 Accuracy: 0.76442\n",
      "Epoch: 310 Batch:   0 Loss: 49.31432 Accuracy: 0.94775\n",
      "Epoch: 311 Batch:   0 Loss: 49.25927 Accuracy: 0.94775\n",
      "Epoch: 312 Batch:   0 Loss: 49.20468 Accuracy: 0.94775\n",
      "Epoch: 313 Batch:   0 Loss: 49.15055 Accuracy: 0.94775\n",
      "Epoch: 314 Batch:   0 Loss: 49.09686 Accuracy: 0.94775\n",
      "Epoch: 315 Batch:   0 Loss: 49.04362 Accuracy: 0.94775\n",
      "Epoch: 316 Batch:   0 Loss: 48.99082 Accuracy: 0.94775\n",
      "Epoch: 317 Batch:   0 Loss: 48.93846 Accuracy: 0.94775\n",
      "Epoch: 318 Batch:   0 Loss: 48.88649 Accuracy: 0.94671\n",
      "Epoch: 319 Batch:   0 Loss: 48.83495 Accuracy: 0.94671\n",
      "Test Loss: 167.30418 Accuracy: 0.76255\n",
      "Epoch: 320 Batch:   0 Loss: 48.78384 Accuracy: 0.94566\n",
      "Epoch: 321 Batch:   0 Loss: 48.73312 Accuracy: 0.94566\n",
      "Epoch: 322 Batch:   0 Loss: 48.68282 Accuracy: 0.94566\n",
      "Epoch: 323 Batch:   0 Loss: 48.63288 Accuracy: 0.94566\n",
      "Epoch: 324 Batch:   0 Loss: 48.58258 Accuracy: 0.94566\n",
      "Epoch: 325 Batch:   0 Loss: 48.53266 Accuracy: 0.94566\n",
      "Epoch: 326 Batch:   0 Loss: 48.48312 Accuracy: 0.94566\n",
      "Epoch: 327 Batch:   0 Loss: 48.43398 Accuracy: 0.94566\n",
      "Epoch: 328 Batch:   0 Loss: 48.38525 Accuracy: 0.94566\n",
      "Epoch: 329 Batch:   0 Loss: 48.33693 Accuracy: 0.94566\n",
      "Test Loss: 168.37159 Accuracy: 0.76202\n",
      "Epoch: 330 Batch:   0 Loss: 48.28897 Accuracy: 0.94566\n",
      "Epoch: 331 Batch:   0 Loss: 48.24141 Accuracy: 0.94566\n",
      "Epoch: 332 Batch:   0 Loss: 48.19423 Accuracy: 0.94671\n",
      "Epoch: 333 Batch:   0 Loss: 48.14741 Accuracy: 0.94671\n",
      "Epoch: 334 Batch:   0 Loss: 48.10096 Accuracy: 0.94671\n",
      "Epoch: 335 Batch:   0 Loss: 48.05486 Accuracy: 0.94671\n",
      "Epoch: 336 Batch:   0 Loss: 48.00912 Accuracy: 0.94671\n",
      "Epoch: 337 Batch:   0 Loss: 47.96373 Accuracy: 0.94671\n",
      "Epoch: 338 Batch:   0 Loss: 47.91867 Accuracy: 0.94671\n",
      "Epoch: 339 Batch:   0 Loss: 47.87395 Accuracy: 0.94671\n",
      "Test Loss: 169.45644 Accuracy: 0.76148\n",
      "Epoch: 340 Batch:   0 Loss: 47.82957 Accuracy: 0.94671\n",
      "Epoch: 341 Batch:   0 Loss: 47.78552 Accuracy: 0.94671\n",
      "Epoch: 342 Batch:   0 Loss: 47.74180 Accuracy: 0.94671\n",
      "Epoch: 343 Batch:   0 Loss: 47.69838 Accuracy: 0.94671\n",
      "Epoch: 344 Batch:   0 Loss: 47.65528 Accuracy: 0.94671\n",
      "Epoch: 345 Batch:   0 Loss: 47.61248 Accuracy: 0.94671\n",
      "Epoch: 346 Batch:   0 Loss: 47.56999 Accuracy: 0.94671\n",
      "Epoch: 347 Batch:   0 Loss: 47.52782 Accuracy: 0.94671\n",
      "Epoch: 348 Batch:   0 Loss: 47.48594 Accuracy: 0.94671\n",
      "Epoch: 349 Batch:   0 Loss: 47.44434 Accuracy: 0.94671\n",
      "Test Loss: 170.54077 Accuracy: 0.76202\n",
      "Epoch: 350 Batch:   0 Loss: 47.40304 Accuracy: 0.94671\n",
      "Epoch: 351 Batch:   0 Loss: 47.36202 Accuracy: 0.94671\n",
      "Epoch: 352 Batch:   0 Loss: 47.32127 Accuracy: 0.94671\n",
      "Epoch: 353 Batch:   0 Loss: 47.28083 Accuracy: 0.94671\n",
      "Epoch: 354 Batch:   0 Loss: 47.24063 Accuracy: 0.94671\n",
      "Epoch: 355 Batch:   0 Loss: 47.20072 Accuracy: 0.94671\n",
      "Epoch: 356 Batch:   0 Loss: 47.16108 Accuracy: 0.94671\n",
      "Epoch: 357 Batch:   0 Loss: 47.12170 Accuracy: 0.94775\n",
      "Epoch: 358 Batch:   0 Loss: 47.08257 Accuracy: 0.94671\n",
      "Epoch: 359 Batch:   0 Loss: 47.04372 Accuracy: 0.94775\n",
      "Test Loss: 171.61496 Accuracy: 0.76028\n",
      "Epoch: 360 Batch:   0 Loss: 47.00510 Accuracy: 0.94775\n",
      "Epoch: 361 Batch:   0 Loss: 46.96676 Accuracy: 0.94775\n",
      "Epoch: 362 Batch:   0 Loss: 46.92866 Accuracy: 0.94775\n",
      "Epoch: 363 Batch:   0 Loss: 46.89080 Accuracy: 0.94775\n",
      "Epoch: 364 Batch:   0 Loss: 46.85319 Accuracy: 0.94775\n",
      "Epoch: 365 Batch:   0 Loss: 46.81581 Accuracy: 0.94775\n",
      "Epoch: 366 Batch:   0 Loss: 46.77866 Accuracy: 0.94775\n",
      "Epoch: 367 Batch:   0 Loss: 46.74176 Accuracy: 0.94775\n",
      "Epoch: 368 Batch:   0 Loss: 46.70511 Accuracy: 0.94775\n",
      "Epoch: 369 Batch:   0 Loss: 46.66866 Accuracy: 0.94880\n",
      "Test Loss: 172.67042 Accuracy: 0.75907\n",
      "Epoch: 370 Batch:   0 Loss: 46.63243 Accuracy: 0.94880\n",
      "Epoch: 371 Batch:   0 Loss: 46.59643 Accuracy: 0.94984\n",
      "Epoch: 372 Batch:   0 Loss: 46.56068 Accuracy: 0.94984\n",
      "Epoch: 373 Batch:   0 Loss: 46.52512 Accuracy: 0.94984\n",
      "Epoch: 374 Batch:   0 Loss: 46.48979 Accuracy: 0.94984\n",
      "Epoch: 375 Batch:   0 Loss: 46.45467 Accuracy: 0.94984\n",
      "Epoch: 376 Batch:   0 Loss: 46.41976 Accuracy: 0.94984\n",
      "Epoch: 377 Batch:   0 Loss: 46.38506 Accuracy: 0.94984\n",
      "Epoch: 378 Batch:   0 Loss: 46.35056 Accuracy: 0.94984\n",
      "Epoch: 379 Batch:   0 Loss: 46.31629 Accuracy: 0.94984\n",
      "Test Loss: 173.69976 Accuracy: 0.75720\n",
      "Epoch: 380 Batch:   0 Loss: 46.28218 Accuracy: 0.94984\n",
      "Epoch: 381 Batch:   0 Loss: 46.24829 Accuracy: 0.94984\n",
      "Epoch: 382 Batch:   0 Loss: 46.21462 Accuracy: 0.94984\n",
      "Epoch: 383 Batch:   0 Loss: 46.18113 Accuracy: 0.94984\n",
      "Epoch: 384 Batch:   0 Loss: 46.14782 Accuracy: 0.94984\n",
      "Epoch: 385 Batch:   0 Loss: 46.11472 Accuracy: 0.94984\n",
      "Epoch: 386 Batch:   0 Loss: 46.08178 Accuracy: 0.94984\n",
      "Epoch: 387 Batch:   0 Loss: 46.04906 Accuracy: 0.94984\n",
      "Epoch: 388 Batch:   0 Loss: 46.01651 Accuracy: 0.94984\n",
      "Epoch: 389 Batch:   0 Loss: 45.98414 Accuracy: 0.94984\n",
      "Test Loss: 174.69694 Accuracy: 0.75600\n",
      "Epoch: 390 Batch:   0 Loss: 45.95195 Accuracy: 0.94984\n",
      "Epoch: 391 Batch:   0 Loss: 45.91994 Accuracy: 0.94984\n",
      "Epoch: 392 Batch:   0 Loss: 45.88812 Accuracy: 0.95089\n",
      "Epoch: 393 Batch:   0 Loss: 45.85645 Accuracy: 0.95089\n",
      "Epoch: 394 Batch:   0 Loss: 45.82499 Accuracy: 0.95089\n",
      "Epoch: 395 Batch:   0 Loss: 45.79367 Accuracy: 0.95089\n",
      "Epoch: 396 Batch:   0 Loss: 45.76253 Accuracy: 0.95089\n",
      "Epoch: 397 Batch:   0 Loss: 45.73154 Accuracy: 0.95089\n",
      "Epoch: 398 Batch:   0 Loss: 45.70073 Accuracy: 0.95089\n",
      "Epoch: 399 Batch:   0 Loss: 45.67008 Accuracy: 0.95089\n",
      "Test Loss: 175.65704 Accuracy: 0.75586\n",
      "Epoch: 400 Batch:   0 Loss: 45.63961 Accuracy: 0.95089\n",
      "Epoch: 401 Batch:   0 Loss: 45.60928 Accuracy: 0.95089\n",
      "Epoch: 402 Batch:   0 Loss: 45.57912 Accuracy: 0.95089\n",
      "Epoch: 403 Batch:   0 Loss: 45.54911 Accuracy: 0.95089\n",
      "Epoch: 404 Batch:   0 Loss: 45.51925 Accuracy: 0.95089\n",
      "Epoch: 405 Batch:   0 Loss: 45.48955 Accuracy: 0.95089\n",
      "Epoch: 406 Batch:   0 Loss: 45.46000 Accuracy: 0.95089\n",
      "Epoch: 407 Batch:   0 Loss: 45.43060 Accuracy: 0.95089\n",
      "Epoch: 408 Batch:   0 Loss: 45.40137 Accuracy: 0.95089\n",
      "Epoch: 409 Batch:   0 Loss: 45.37226 Accuracy: 0.95089\n",
      "Test Loss: 176.57638 Accuracy: 0.75506\n",
      "Epoch: 410 Batch:   0 Loss: 45.34329 Accuracy: 0.95089\n",
      "Epoch: 411 Batch:   0 Loss: 45.31448 Accuracy: 0.95089\n",
      "Epoch: 412 Batch:   0 Loss: 45.28581 Accuracy: 0.95089\n",
      "Epoch: 413 Batch:   0 Loss: 45.25728 Accuracy: 0.95089\n",
      "Epoch: 414 Batch:   0 Loss: 45.22888 Accuracy: 0.95089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 415 Batch:   0 Loss: 45.20063 Accuracy: 0.95089\n",
      "Epoch: 416 Batch:   0 Loss: 45.17252 Accuracy: 0.95089\n",
      "Epoch: 417 Batch:   0 Loss: 45.14454 Accuracy: 0.95089\n",
      "Epoch: 418 Batch:   0 Loss: 45.11669 Accuracy: 0.95089\n",
      "Epoch: 419 Batch:   0 Loss: 45.08898 Accuracy: 0.95089\n",
      "Test Loss: 177.45249 Accuracy: 0.75305\n",
      "Epoch: 420 Batch:   0 Loss: 45.06140 Accuracy: 0.95089\n",
      "Epoch: 421 Batch:   0 Loss: 45.03394 Accuracy: 0.95089\n",
      "Epoch: 422 Batch:   0 Loss: 45.00661 Accuracy: 0.95089\n",
      "Epoch: 423 Batch:   0 Loss: 44.97942 Accuracy: 0.95089\n",
      "Epoch: 424 Batch:   0 Loss: 44.95235 Accuracy: 0.95089\n",
      "Epoch: 425 Batch:   0 Loss: 44.92541 Accuracy: 0.95089\n",
      "Epoch: 426 Batch:   0 Loss: 44.89859 Accuracy: 0.95089\n",
      "Epoch: 427 Batch:   0 Loss: 44.87186 Accuracy: 0.95089\n",
      "Epoch: 428 Batch:   0 Loss: 44.84529 Accuracy: 0.95089\n",
      "Epoch: 429 Batch:   0 Loss: 44.81883 Accuracy: 0.95089\n",
      "Test Loss: 178.28396 Accuracy: 0.75252\n",
      "Epoch: 430 Batch:   0 Loss: 44.79248 Accuracy: 0.95089\n",
      "Epoch: 431 Batch:   0 Loss: 44.76625 Accuracy: 0.95089\n",
      "Epoch: 432 Batch:   0 Loss: 44.74014 Accuracy: 0.95089\n",
      "Epoch: 433 Batch:   0 Loss: 44.71414 Accuracy: 0.95089\n",
      "Epoch: 434 Batch:   0 Loss: 44.68825 Accuracy: 0.95089\n",
      "Epoch: 435 Batch:   0 Loss: 44.66247 Accuracy: 0.95089\n",
      "Epoch: 436 Batch:   0 Loss: 44.63681 Accuracy: 0.95089\n",
      "Epoch: 437 Batch:   0 Loss: 44.61125 Accuracy: 0.95089\n",
      "Epoch: 438 Batch:   0 Loss: 44.58581 Accuracy: 0.95089\n",
      "Epoch: 439 Batch:   0 Loss: 44.56047 Accuracy: 0.95089\n",
      "Test Loss: 179.07016 Accuracy: 0.75225\n",
      "Epoch: 440 Batch:   0 Loss: 44.53524 Accuracy: 0.95089\n",
      "Epoch: 441 Batch:   0 Loss: 44.51011 Accuracy: 0.95193\n",
      "Epoch: 442 Batch:   0 Loss: 44.48509 Accuracy: 0.95193\n",
      "Epoch: 443 Batch:   0 Loss: 44.46017 Accuracy: 0.95193\n",
      "Epoch: 444 Batch:   0 Loss: 44.43534 Accuracy: 0.95298\n",
      "Epoch: 445 Batch:   0 Loss: 44.41063 Accuracy: 0.95298\n",
      "Epoch: 446 Batch:   0 Loss: 44.38602 Accuracy: 0.95298\n",
      "Epoch: 447 Batch:   0 Loss: 44.36150 Accuracy: 0.95298\n",
      "Epoch: 448 Batch:   0 Loss: 44.33708 Accuracy: 0.95298\n",
      "Epoch: 449 Batch:   0 Loss: 44.31274 Accuracy: 0.95298\n",
      "Test Loss: 179.81080 Accuracy: 0.75118\n",
      "Epoch: 450 Batch:   0 Loss: 44.28851 Accuracy: 0.95298\n",
      "Epoch: 451 Batch:   0 Loss: 44.26437 Accuracy: 0.95298\n",
      "Epoch: 452 Batch:   0 Loss: 44.24033 Accuracy: 0.95298\n",
      "Epoch: 453 Batch:   0 Loss: 44.21637 Accuracy: 0.95298\n",
      "Epoch: 454 Batch:   0 Loss: 44.19252 Accuracy: 0.95298\n",
      "Epoch: 455 Batch:   0 Loss: 44.16875 Accuracy: 0.95298\n",
      "Epoch: 456 Batch:   0 Loss: 44.14507 Accuracy: 0.95298\n",
      "Epoch: 457 Batch:   0 Loss: 44.12148 Accuracy: 0.95298\n",
      "Epoch: 458 Batch:   0 Loss: 44.09798 Accuracy: 0.95298\n",
      "Epoch: 459 Batch:   0 Loss: 44.07457 Accuracy: 0.95298\n",
      "Test Loss: 180.50587 Accuracy: 0.75011\n",
      "Epoch: 460 Batch:   0 Loss: 44.05122 Accuracy: 0.95298\n",
      "Epoch: 461 Batch:   0 Loss: 44.02798 Accuracy: 0.95298\n",
      "Epoch: 462 Batch:   0 Loss: 44.00481 Accuracy: 0.95298\n",
      "Epoch: 463 Batch:   0 Loss: 43.98175 Accuracy: 0.95298\n",
      "Epoch: 464 Batch:   0 Loss: 43.95874 Accuracy: 0.95298\n",
      "Epoch: 465 Batch:   0 Loss: 43.93582 Accuracy: 0.95298\n",
      "Epoch: 466 Batch:   0 Loss: 43.91299 Accuracy: 0.95298\n",
      "Epoch: 467 Batch:   0 Loss: 43.89022 Accuracy: 0.95298\n",
      "Epoch: 468 Batch:   0 Loss: 43.86754 Accuracy: 0.95298\n",
      "Epoch: 469 Batch:   0 Loss: 43.84494 Accuracy: 0.95298\n",
      "Test Loss: 181.15519 Accuracy: 0.75011\n",
      "Epoch: 470 Batch:   0 Loss: 43.82243 Accuracy: 0.95298\n",
      "Epoch: 471 Batch:   0 Loss: 43.79998 Accuracy: 0.95298\n",
      "Epoch: 472 Batch:   0 Loss: 43.77761 Accuracy: 0.95298\n",
      "Epoch: 473 Batch:   0 Loss: 43.75531 Accuracy: 0.95298\n",
      "Epoch: 474 Batch:   0 Loss: 43.73310 Accuracy: 0.95298\n",
      "Epoch: 475 Batch:   0 Loss: 43.71095 Accuracy: 0.95402\n",
      "Epoch: 476 Batch:   0 Loss: 43.68887 Accuracy: 0.95402\n",
      "Epoch: 477 Batch:   0 Loss: 43.66687 Accuracy: 0.95402\n",
      "Epoch: 478 Batch:   0 Loss: 43.64494 Accuracy: 0.95402\n",
      "Epoch: 479 Batch:   0 Loss: 43.62307 Accuracy: 0.95402\n",
      "Test Loss: 181.75926 Accuracy: 0.74957\n",
      "Epoch: 480 Batch:   0 Loss: 43.60128 Accuracy: 0.95402\n",
      "Epoch: 481 Batch:   0 Loss: 43.57957 Accuracy: 0.95402\n",
      "Epoch: 482 Batch:   0 Loss: 43.55791 Accuracy: 0.95402\n",
      "Epoch: 483 Batch:   0 Loss: 43.53634 Accuracy: 0.95402\n",
      "Epoch: 484 Batch:   0 Loss: 43.51482 Accuracy: 0.95402\n",
      "Epoch: 485 Batch:   0 Loss: 43.49337 Accuracy: 0.95402\n",
      "Epoch: 486 Batch:   0 Loss: 43.47198 Accuracy: 0.95402\n",
      "Epoch: 487 Batch:   0 Loss: 43.45068 Accuracy: 0.95402\n",
      "Epoch: 488 Batch:   0 Loss: 43.42943 Accuracy: 0.95402\n",
      "Epoch: 489 Batch:   0 Loss: 43.40824 Accuracy: 0.95402\n",
      "Test Loss: 182.32003 Accuracy: 0.74877\n",
      "Epoch: 490 Batch:   0 Loss: 43.38712 Accuracy: 0.95402\n",
      "Epoch: 491 Batch:   0 Loss: 43.36608 Accuracy: 0.95402\n",
      "Epoch: 492 Batch:   0 Loss: 43.34509 Accuracy: 0.95402\n",
      "Epoch: 493 Batch:   0 Loss: 43.32415 Accuracy: 0.95402\n",
      "Epoch: 494 Batch:   0 Loss: 43.30330 Accuracy: 0.95402\n",
      "Epoch: 495 Batch:   0 Loss: 43.28251 Accuracy: 0.95402\n",
      "Epoch: 496 Batch:   0 Loss: 43.26176 Accuracy: 0.95402\n",
      "Epoch: 497 Batch:   0 Loss: 43.24109 Accuracy: 0.95402\n",
      "Epoch: 498 Batch:   0 Loss: 43.22049 Accuracy: 0.95402\n",
      "Epoch: 499 Batch:   0 Loss: 43.19992 Accuracy: 0.95402\n",
      "Test Loss: 182.84385 Accuracy: 0.74877\n",
      "Epoch: 500 Batch:   0 Loss: 43.17943 Accuracy: 0.95402\n",
      "Epoch: 501 Batch:   0 Loss: 43.15901 Accuracy: 0.95402\n",
      "Epoch: 502 Batch:   0 Loss: 43.13863 Accuracy: 0.95402\n",
      "Epoch: 503 Batch:   0 Loss: 43.11831 Accuracy: 0.95402\n",
      "Epoch: 504 Batch:   0 Loss: 43.09805 Accuracy: 0.95402\n",
      "Epoch: 505 Batch:   0 Loss: 43.07785 Accuracy: 0.95402\n",
      "Epoch: 506 Batch:   0 Loss: 43.05771 Accuracy: 0.95402\n",
      "Epoch: 507 Batch:   0 Loss: 43.03764 Accuracy: 0.95402\n",
      "Epoch: 508 Batch:   0 Loss: 43.01760 Accuracy: 0.95402\n",
      "Epoch: 509 Batch:   0 Loss: 42.99763 Accuracy: 0.95402\n",
      "Test Loss: 183.34300 Accuracy: 0.74877\n",
      "Epoch: 510 Batch:   0 Loss: 42.97772 Accuracy: 0.95507\n",
      "Epoch: 511 Batch:   0 Loss: 42.95786 Accuracy: 0.95507\n",
      "Epoch: 512 Batch:   0 Loss: 42.93806 Accuracy: 0.95507\n",
      "Epoch: 513 Batch:   0 Loss: 42.91831 Accuracy: 0.95611\n",
      "Epoch: 514 Batch:   0 Loss: 42.89861 Accuracy: 0.95611\n",
      "Epoch: 515 Batch:   0 Loss: 42.87896 Accuracy: 0.95611\n",
      "Epoch: 516 Batch:   0 Loss: 42.85938 Accuracy: 0.95611\n",
      "Epoch: 517 Batch:   0 Loss: 42.83984 Accuracy: 0.95611\n",
      "Epoch: 518 Batch:   0 Loss: 42.82035 Accuracy: 0.95611\n",
      "Epoch: 519 Batch:   0 Loss: 42.80091 Accuracy: 0.95611\n",
      "Test Loss: 183.83552 Accuracy: 0.74877\n",
      "Epoch: 520 Batch:   0 Loss: 42.78152 Accuracy: 0.95611\n",
      "Epoch: 521 Batch:   0 Loss: 42.76219 Accuracy: 0.95611\n",
      "Epoch: 522 Batch:   0 Loss: 42.74290 Accuracy: 0.95611\n",
      "Epoch: 523 Batch:   0 Loss: 42.72366 Accuracy: 0.95611\n",
      "Epoch: 524 Batch:   0 Loss: 42.70448 Accuracy: 0.95611\n",
      "Epoch: 525 Batch:   0 Loss: 42.68534 Accuracy: 0.95611\n",
      "Epoch: 526 Batch:   0 Loss: 42.66626 Accuracy: 0.95611\n",
      "Epoch: 527 Batch:   0 Loss: 42.64720 Accuracy: 0.95611\n",
      "Epoch: 528 Batch:   0 Loss: 42.62822 Accuracy: 0.95611\n",
      "Epoch: 529 Batch:   0 Loss: 42.60926 Accuracy: 0.95611\n",
      "Test Loss: 184.33850 Accuracy: 0.74837\n",
      "Epoch: 530 Batch:   0 Loss: 42.59035 Accuracy: 0.95611\n",
      "Epoch: 531 Batch:   0 Loss: 42.57150 Accuracy: 0.95611\n",
      "Epoch: 532 Batch:   0 Loss: 42.55270 Accuracy: 0.95611\n",
      "Epoch: 533 Batch:   0 Loss: 42.53392 Accuracy: 0.95611\n",
      "Epoch: 534 Batch:   0 Loss: 42.51521 Accuracy: 0.95611\n",
      "Epoch: 535 Batch:   0 Loss: 42.49654 Accuracy: 0.95611\n",
      "Epoch: 536 Batch:   0 Loss: 42.47790 Accuracy: 0.95611\n",
      "Epoch: 537 Batch:   0 Loss: 42.45933 Accuracy: 0.95611\n",
      "Epoch: 538 Batch:   0 Loss: 42.44078 Accuracy: 0.95611\n",
      "Epoch: 539 Batch:   0 Loss: 42.42228 Accuracy: 0.95611\n",
      "Test Loss: 184.86094 Accuracy: 0.74890\n",
      "Epoch: 540 Batch:   0 Loss: 42.40382 Accuracy: 0.95507\n",
      "Epoch: 541 Batch:   0 Loss: 42.38541 Accuracy: 0.95507\n",
      "Epoch: 542 Batch:   0 Loss: 42.36703 Accuracy: 0.95507\n",
      "Epoch: 543 Batch:   0 Loss: 42.34872 Accuracy: 0.95507\n",
      "Epoch: 544 Batch:   0 Loss: 42.33042 Accuracy: 0.95507\n",
      "Epoch: 545 Batch:   0 Loss: 42.31219 Accuracy: 0.95507\n",
      "Epoch: 546 Batch:   0 Loss: 42.29398 Accuracy: 0.95507\n",
      "Epoch: 547 Batch:   0 Loss: 42.27583 Accuracy: 0.95507\n",
      "Epoch: 548 Batch:   0 Loss: 42.25771 Accuracy: 0.95507\n",
      "Epoch: 549 Batch:   0 Loss: 42.23963 Accuracy: 0.95507\n",
      "Test Loss: 185.40219 Accuracy: 0.74797\n",
      "Epoch: 550 Batch:   0 Loss: 42.22159 Accuracy: 0.95507\n",
      "Epoch: 551 Batch:   0 Loss: 42.20360 Accuracy: 0.95507\n",
      "Epoch: 552 Batch:   0 Loss: 42.18564 Accuracy: 0.95507\n",
      "Epoch: 553 Batch:   0 Loss: 42.16771 Accuracy: 0.95507\n",
      "Epoch: 554 Batch:   0 Loss: 42.14985 Accuracy: 0.95507\n",
      "Epoch: 555 Batch:   0 Loss: 42.13201 Accuracy: 0.95507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 556 Batch:   0 Loss: 42.11421 Accuracy: 0.95507\n",
      "Epoch: 557 Batch:   0 Loss: 42.09645 Accuracy: 0.95507\n",
      "Epoch: 558 Batch:   0 Loss: 42.07874 Accuracy: 0.95507\n",
      "Epoch: 559 Batch:   0 Loss: 42.06104 Accuracy: 0.95507\n",
      "Test Loss: 185.95644 Accuracy: 0.74743\n",
      "Epoch: 560 Batch:   0 Loss: 42.04341 Accuracy: 0.95507\n",
      "Epoch: 561 Batch:   0 Loss: 42.02581 Accuracy: 0.95611\n",
      "Epoch: 562 Batch:   0 Loss: 42.00824 Accuracy: 0.95611\n",
      "Epoch: 563 Batch:   0 Loss: 41.99072 Accuracy: 0.95611\n",
      "Epoch: 564 Batch:   0 Loss: 41.97322 Accuracy: 0.95611\n",
      "Epoch: 565 Batch:   0 Loss: 41.95575 Accuracy: 0.95611\n",
      "Epoch: 566 Batch:   0 Loss: 41.93835 Accuracy: 0.95611\n",
      "Epoch: 567 Batch:   0 Loss: 41.92096 Accuracy: 0.95611\n",
      "Epoch: 568 Batch:   0 Loss: 41.90361 Accuracy: 0.95611\n",
      "Epoch: 569 Batch:   0 Loss: 41.88631 Accuracy: 0.95611\n",
      "Test Loss: 186.51765 Accuracy: 0.74770\n",
      "Epoch: 570 Batch:   0 Loss: 41.86903 Accuracy: 0.95611\n",
      "Epoch: 571 Batch:   0 Loss: 41.85180 Accuracy: 0.95611\n",
      "Epoch: 572 Batch:   0 Loss: 41.83459 Accuracy: 0.95611\n",
      "Epoch: 573 Batch:   0 Loss: 41.81745 Accuracy: 0.95611\n",
      "Epoch: 574 Batch:   0 Loss: 41.80032 Accuracy: 0.95611\n",
      "Epoch: 575 Batch:   0 Loss: 41.78323 Accuracy: 0.95611\n",
      "Epoch: 576 Batch:   0 Loss: 41.76616 Accuracy: 0.95611\n",
      "Epoch: 577 Batch:   0 Loss: 41.74915 Accuracy: 0.95611\n",
      "Epoch: 578 Batch:   0 Loss: 41.73215 Accuracy: 0.95611\n",
      "Epoch: 579 Batch:   0 Loss: 41.71518 Accuracy: 0.95611\n",
      "Test Loss: 187.08156 Accuracy: 0.74716\n",
      "Epoch: 580 Batch:   0 Loss: 41.69826 Accuracy: 0.95611\n",
      "Epoch: 581 Batch:   0 Loss: 41.68137 Accuracy: 0.95611\n",
      "Epoch: 582 Batch:   0 Loss: 41.66454 Accuracy: 0.95611\n",
      "Epoch: 583 Batch:   0 Loss: 41.64771 Accuracy: 0.95611\n",
      "Epoch: 584 Batch:   0 Loss: 41.63091 Accuracy: 0.95611\n",
      "Epoch: 585 Batch:   0 Loss: 41.61415 Accuracy: 0.95611\n",
      "Epoch: 586 Batch:   0 Loss: 41.59744 Accuracy: 0.95611\n",
      "Epoch: 587 Batch:   0 Loss: 41.58074 Accuracy: 0.95611\n",
      "Epoch: 588 Batch:   0 Loss: 41.56408 Accuracy: 0.95611\n",
      "Epoch: 589 Batch:   0 Loss: 41.54745 Accuracy: 0.95611\n",
      "Test Loss: 187.64547 Accuracy: 0.74743\n",
      "Epoch: 590 Batch:   0 Loss: 41.53092 Accuracy: 0.95611\n",
      "Epoch: 591 Batch:   0 Loss: 41.51427 Accuracy: 0.95611\n",
      "Epoch: 592 Batch:   0 Loss: 41.49763 Accuracy: 0.95611\n",
      "Epoch: 593 Batch:   0 Loss: 41.48103 Accuracy: 0.95611\n",
      "Epoch: 594 Batch:   0 Loss: 41.46445 Accuracy: 0.95611\n",
      "Epoch: 595 Batch:   0 Loss: 41.44791 Accuracy: 0.95611\n",
      "Epoch: 596 Batch:   0 Loss: 41.43142 Accuracy: 0.95611\n",
      "Epoch: 597 Batch:   0 Loss: 41.41496 Accuracy: 0.95611\n",
      "Epoch: 598 Batch:   0 Loss: 41.39853 Accuracy: 0.95611\n",
      "Epoch: 599 Batch:   0 Loss: 41.38214 Accuracy: 0.95611\n",
      "Test Loss: 188.21254 Accuracy: 0.74770\n",
      "Epoch: 600 Batch:   0 Loss: 41.36579 Accuracy: 0.95611\n",
      "Epoch: 601 Batch:   0 Loss: 41.34945 Accuracy: 0.95611\n",
      "Epoch: 602 Batch:   0 Loss: 41.33316 Accuracy: 0.95611\n",
      "Epoch: 603 Batch:   0 Loss: 41.31688 Accuracy: 0.95611\n",
      "Epoch: 604 Batch:   0 Loss: 41.30064 Accuracy: 0.95611\n",
      "Epoch: 605 Batch:   0 Loss: 41.28444 Accuracy: 0.95611\n",
      "Epoch: 606 Batch:   0 Loss: 41.26826 Accuracy: 0.95611\n",
      "Epoch: 607 Batch:   0 Loss: 41.25208 Accuracy: 0.95611\n",
      "Epoch: 608 Batch:   0 Loss: 41.23597 Accuracy: 0.95611\n",
      "Epoch: 609 Batch:   0 Loss: 41.21987 Accuracy: 0.95611\n",
      "Test Loss: 188.77670 Accuracy: 0.74797\n",
      "Epoch: 610 Batch:   0 Loss: 41.20380 Accuracy: 0.95611\n",
      "Epoch: 611 Batch:   0 Loss: 41.18776 Accuracy: 0.95611\n",
      "Epoch: 612 Batch:   0 Loss: 41.17173 Accuracy: 0.95611\n",
      "Epoch: 613 Batch:   0 Loss: 41.15572 Accuracy: 0.95611\n",
      "Epoch: 614 Batch:   0 Loss: 41.13975 Accuracy: 0.95611\n",
      "Epoch: 615 Batch:   0 Loss: 41.12381 Accuracy: 0.95611\n",
      "Epoch: 616 Batch:   0 Loss: 41.10791 Accuracy: 0.95611\n",
      "Epoch: 617 Batch:   0 Loss: 41.09201 Accuracy: 0.95611\n",
      "Epoch: 618 Batch:   0 Loss: 41.07615 Accuracy: 0.95611\n",
      "Epoch: 619 Batch:   0 Loss: 41.06031 Accuracy: 0.95611\n",
      "Test Loss: 189.34109 Accuracy: 0.74823\n",
      "Epoch: 620 Batch:   0 Loss: 41.04451 Accuracy: 0.95611\n",
      "Epoch: 621 Batch:   0 Loss: 41.02873 Accuracy: 0.95611\n",
      "Epoch: 622 Batch:   0 Loss: 41.01295 Accuracy: 0.95611\n",
      "Epoch: 623 Batch:   0 Loss: 40.99723 Accuracy: 0.95611\n",
      "Epoch: 624 Batch:   0 Loss: 40.98152 Accuracy: 0.95611\n",
      "Epoch: 625 Batch:   0 Loss: 40.96584 Accuracy: 0.95611\n",
      "Epoch: 626 Batch:   0 Loss: 40.95019 Accuracy: 0.95611\n",
      "Epoch: 627 Batch:   0 Loss: 40.93456 Accuracy: 0.95611\n",
      "Epoch: 628 Batch:   0 Loss: 40.91895 Accuracy: 0.95611\n",
      "Epoch: 629 Batch:   0 Loss: 40.90337 Accuracy: 0.95611\n",
      "Test Loss: 189.90571 Accuracy: 0.74783\n",
      "Epoch: 630 Batch:   0 Loss: 40.88780 Accuracy: 0.95611\n",
      "Epoch: 631 Batch:   0 Loss: 40.87227 Accuracy: 0.95611\n",
      "Epoch: 632 Batch:   0 Loss: 40.85675 Accuracy: 0.95611\n",
      "Epoch: 633 Batch:   0 Loss: 40.84127 Accuracy: 0.95611\n",
      "Epoch: 634 Batch:   0 Loss: 40.82580 Accuracy: 0.95716\n",
      "Epoch: 635 Batch:   0 Loss: 40.81037 Accuracy: 0.95716\n",
      "Epoch: 636 Batch:   0 Loss: 40.79494 Accuracy: 0.95716\n",
      "Epoch: 637 Batch:   0 Loss: 40.77954 Accuracy: 0.95716\n",
      "Epoch: 638 Batch:   0 Loss: 40.76418 Accuracy: 0.95716\n",
      "Epoch: 639 Batch:   0 Loss: 40.74882 Accuracy: 0.95716\n",
      "Test Loss: 190.47106 Accuracy: 0.74770\n",
      "Epoch: 640 Batch:   0 Loss: 40.73349 Accuracy: 0.95716\n",
      "Epoch: 641 Batch:   0 Loss: 40.71818 Accuracy: 0.95716\n",
      "Epoch: 642 Batch:   0 Loss: 40.70290 Accuracy: 0.95716\n",
      "Epoch: 643 Batch:   0 Loss: 40.68764 Accuracy: 0.95716\n",
      "Epoch: 644 Batch:   0 Loss: 40.67239 Accuracy: 0.95716\n",
      "Epoch: 645 Batch:   0 Loss: 40.65718 Accuracy: 0.95716\n",
      "Epoch: 646 Batch:   0 Loss: 40.64199 Accuracy: 0.95716\n",
      "Epoch: 647 Batch:   0 Loss: 40.62680 Accuracy: 0.95716\n",
      "Epoch: 648 Batch:   0 Loss: 40.61164 Accuracy: 0.95716\n",
      "Epoch: 649 Batch:   0 Loss: 40.59631 Accuracy: 0.95716\n",
      "Test Loss: 191.04126 Accuracy: 0.74770\n",
      "Epoch: 650 Batch:   0 Loss: 40.58101 Accuracy: 0.95716\n",
      "Epoch: 651 Batch:   0 Loss: 40.56575 Accuracy: 0.95716\n",
      "Epoch: 652 Batch:   0 Loss: 40.55054 Accuracy: 0.95716\n",
      "Epoch: 653 Batch:   0 Loss: 40.53535 Accuracy: 0.95716\n",
      "Epoch: 654 Batch:   0 Loss: 40.52024 Accuracy: 0.95716\n",
      "Epoch: 655 Batch:   0 Loss: 40.50512 Accuracy: 0.95716\n",
      "Epoch: 656 Batch:   0 Loss: 40.49006 Accuracy: 0.95716\n",
      "Epoch: 657 Batch:   0 Loss: 40.47501 Accuracy: 0.95716\n",
      "Epoch: 658 Batch:   0 Loss: 40.46000 Accuracy: 0.95716\n",
      "Epoch: 659 Batch:   0 Loss: 40.44500 Accuracy: 0.95716\n",
      "Test Loss: 191.61601 Accuracy: 0.74850\n",
      "Epoch: 660 Batch:   0 Loss: 40.43004 Accuracy: 0.95716\n",
      "Epoch: 661 Batch:   0 Loss: 40.41510 Accuracy: 0.95716\n",
      "Epoch: 662 Batch:   0 Loss: 40.40017 Accuracy: 0.95716\n",
      "Epoch: 663 Batch:   0 Loss: 40.38528 Accuracy: 0.95716\n",
      "Epoch: 664 Batch:   0 Loss: 40.37041 Accuracy: 0.95716\n",
      "Epoch: 665 Batch:   0 Loss: 40.35556 Accuracy: 0.95716\n",
      "Epoch: 666 Batch:   0 Loss: 40.34072 Accuracy: 0.95716\n",
      "Epoch: 667 Batch:   0 Loss: 40.32591 Accuracy: 0.95716\n",
      "Epoch: 668 Batch:   0 Loss: 40.31112 Accuracy: 0.95716\n",
      "Epoch: 669 Batch:   0 Loss: 40.29634 Accuracy: 0.95716\n",
      "Test Loss: 192.18717 Accuracy: 0.74810\n",
      "Epoch: 670 Batch:   0 Loss: 40.28160 Accuracy: 0.95716\n",
      "Epoch: 671 Batch:   0 Loss: 40.26688 Accuracy: 0.95716\n",
      "Epoch: 672 Batch:   0 Loss: 40.25217 Accuracy: 0.95716\n",
      "Epoch: 673 Batch:   0 Loss: 40.23748 Accuracy: 0.95716\n",
      "Epoch: 674 Batch:   0 Loss: 40.22282 Accuracy: 0.95716\n",
      "Epoch: 675 Batch:   0 Loss: 40.20817 Accuracy: 0.95716\n",
      "Epoch: 676 Batch:   0 Loss: 40.19354 Accuracy: 0.95716\n",
      "Epoch: 677 Batch:   0 Loss: 40.17894 Accuracy: 0.95716\n",
      "Epoch: 678 Batch:   0 Loss: 40.16436 Accuracy: 0.95716\n",
      "Epoch: 679 Batch:   0 Loss: 40.14978 Accuracy: 0.95716\n",
      "Test Loss: 192.76029 Accuracy: 0.74783\n",
      "Epoch: 680 Batch:   0 Loss: 40.13525 Accuracy: 0.95716\n",
      "Epoch: 681 Batch:   0 Loss: 40.12072 Accuracy: 0.95716\n",
      "Epoch: 682 Batch:   0 Loss: 40.10622 Accuracy: 0.95716\n",
      "Epoch: 683 Batch:   0 Loss: 40.09173 Accuracy: 0.95716\n",
      "Epoch: 684 Batch:   0 Loss: 40.07726 Accuracy: 0.95716\n",
      "Epoch: 685 Batch:   0 Loss: 40.06281 Accuracy: 0.95716\n",
      "Epoch: 686 Batch:   0 Loss: 40.04840 Accuracy: 0.95716\n",
      "Epoch: 687 Batch:   0 Loss: 40.03400 Accuracy: 0.95716\n",
      "Epoch: 688 Batch:   0 Loss: 40.01960 Accuracy: 0.95716\n",
      "Epoch: 689 Batch:   0 Loss: 40.00524 Accuracy: 0.95716\n",
      "Test Loss: 193.33680 Accuracy: 0.74716\n",
      "Epoch: 690 Batch:   0 Loss: 39.99087 Accuracy: 0.95716\n",
      "Epoch: 691 Batch:   0 Loss: 39.97655 Accuracy: 0.95716\n",
      "Epoch: 692 Batch:   0 Loss: 39.96225 Accuracy: 0.95716\n",
      "Epoch: 693 Batch:   0 Loss: 39.94796 Accuracy: 0.95716\n",
      "Epoch: 694 Batch:   0 Loss: 39.93369 Accuracy: 0.95716\n",
      "Epoch: 695 Batch:   0 Loss: 39.91945 Accuracy: 0.95716\n",
      "Epoch: 696 Batch:   0 Loss: 39.90522 Accuracy: 0.95716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 697 Batch:   0 Loss: 39.89101 Accuracy: 0.95716\n",
      "Epoch: 698 Batch:   0 Loss: 39.87683 Accuracy: 0.95716\n",
      "Epoch: 699 Batch:   0 Loss: 39.86265 Accuracy: 0.95716\n",
      "Test Loss: 193.91769 Accuracy: 0.74676\n",
      "Epoch: 700 Batch:   0 Loss: 39.84851 Accuracy: 0.95716\n",
      "Epoch: 701 Batch:   0 Loss: 39.83438 Accuracy: 0.95716\n",
      "Epoch: 702 Batch:   0 Loss: 39.82028 Accuracy: 0.95716\n",
      "Epoch: 703 Batch:   0 Loss: 39.80618 Accuracy: 0.95716\n",
      "Epoch: 704 Batch:   0 Loss: 39.79211 Accuracy: 0.95716\n",
      "Epoch: 705 Batch:   0 Loss: 39.77806 Accuracy: 0.95716\n",
      "Epoch: 706 Batch:   0 Loss: 39.76404 Accuracy: 0.95716\n",
      "Epoch: 707 Batch:   0 Loss: 39.75003 Accuracy: 0.95716\n",
      "Epoch: 708 Batch:   0 Loss: 39.73605 Accuracy: 0.95716\n",
      "Epoch: 709 Batch:   0 Loss: 39.72209 Accuracy: 0.95820\n",
      "Test Loss: 194.50344 Accuracy: 0.74663\n",
      "Epoch: 710 Batch:   0 Loss: 39.70816 Accuracy: 0.95820\n",
      "Epoch: 711 Batch:   0 Loss: 39.69424 Accuracy: 0.95820\n",
      "Epoch: 712 Batch:   0 Loss: 39.68032 Accuracy: 0.95820\n",
      "Epoch: 713 Batch:   0 Loss: 39.66644 Accuracy: 0.95820\n",
      "Epoch: 714 Batch:   0 Loss: 39.65260 Accuracy: 0.95820\n",
      "Epoch: 715 Batch:   0 Loss: 39.63876 Accuracy: 0.95820\n",
      "Epoch: 716 Batch:   0 Loss: 39.62495 Accuracy: 0.95820\n",
      "Epoch: 717 Batch:   0 Loss: 39.61116 Accuracy: 0.95820\n",
      "Epoch: 718 Batch:   0 Loss: 39.59739 Accuracy: 0.95820\n",
      "Epoch: 719 Batch:   0 Loss: 39.58363 Accuracy: 0.95820\n",
      "Test Loss: 195.09455 Accuracy: 0.74716\n",
      "Epoch: 720 Batch:   0 Loss: 39.56992 Accuracy: 0.95820\n",
      "Epoch: 721 Batch:   0 Loss: 39.55621 Accuracy: 0.95820\n",
      "Epoch: 722 Batch:   0 Loss: 39.54252 Accuracy: 0.95820\n",
      "Epoch: 723 Batch:   0 Loss: 39.52887 Accuracy: 0.95820\n",
      "Epoch: 724 Batch:   0 Loss: 39.51523 Accuracy: 0.95820\n",
      "Epoch: 725 Batch:   0 Loss: 39.50162 Accuracy: 0.95820\n",
      "Epoch: 726 Batch:   0 Loss: 39.48803 Accuracy: 0.95820\n",
      "Epoch: 727 Batch:   0 Loss: 39.47448 Accuracy: 0.95820\n",
      "Epoch: 728 Batch:   0 Loss: 39.46093 Accuracy: 0.95820\n",
      "Epoch: 729 Batch:   0 Loss: 39.44740 Accuracy: 0.95820\n",
      "Test Loss: 195.69146 Accuracy: 0.74730\n",
      "Epoch: 730 Batch:   0 Loss: 39.43391 Accuracy: 0.95820\n",
      "Epoch: 731 Batch:   0 Loss: 39.42043 Accuracy: 0.95820\n",
      "Epoch: 732 Batch:   0 Loss: 39.40698 Accuracy: 0.95820\n",
      "Epoch: 733 Batch:   0 Loss: 39.39355 Accuracy: 0.95925\n",
      "Epoch: 734 Batch:   0 Loss: 39.38015 Accuracy: 0.95925\n",
      "Epoch: 735 Batch:   0 Loss: 39.36678 Accuracy: 0.95925\n",
      "Epoch: 736 Batch:   0 Loss: 39.35342 Accuracy: 0.95925\n",
      "Epoch: 737 Batch:   0 Loss: 39.34009 Accuracy: 0.95925\n",
      "Epoch: 738 Batch:   0 Loss: 39.32680 Accuracy: 0.95925\n",
      "Epoch: 739 Batch:   0 Loss: 39.31352 Accuracy: 0.95925\n",
      "Test Loss: 196.29456 Accuracy: 0.74783\n",
      "Epoch: 740 Batch:   0 Loss: 39.30026 Accuracy: 0.95925\n",
      "Epoch: 741 Batch:   0 Loss: 39.28704 Accuracy: 0.95925\n",
      "Epoch: 742 Batch:   0 Loss: 39.27382 Accuracy: 0.95925\n",
      "Epoch: 743 Batch:   0 Loss: 39.26064 Accuracy: 0.95925\n",
      "Epoch: 744 Batch:   0 Loss: 39.24747 Accuracy: 0.95925\n",
      "Epoch: 745 Batch:   0 Loss: 39.23431 Accuracy: 0.95925\n",
      "Epoch: 746 Batch:   0 Loss: 39.22116 Accuracy: 0.95925\n",
      "Epoch: 747 Batch:   0 Loss: 39.20803 Accuracy: 0.95925\n",
      "Epoch: 748 Batch:   0 Loss: 39.19493 Accuracy: 0.95925\n",
      "Epoch: 749 Batch:   0 Loss: 39.18185 Accuracy: 0.95925\n",
      "Test Loss: 196.90580 Accuracy: 0.74783\n",
      "Epoch: 750 Batch:   0 Loss: 39.16879 Accuracy: 0.95925\n",
      "Epoch: 751 Batch:   0 Loss: 39.15578 Accuracy: 0.95925\n",
      "Epoch: 752 Batch:   0 Loss: 39.14278 Accuracy: 0.95925\n",
      "Epoch: 753 Batch:   0 Loss: 39.12983 Accuracy: 0.95925\n",
      "Epoch: 754 Batch:   0 Loss: 39.11687 Accuracy: 0.95925\n",
      "Epoch: 755 Batch:   0 Loss: 39.10396 Accuracy: 0.95925\n",
      "Epoch: 756 Batch:   0 Loss: 39.09107 Accuracy: 0.95925\n",
      "Epoch: 757 Batch:   0 Loss: 39.07821 Accuracy: 0.96029\n",
      "Epoch: 758 Batch:   0 Loss: 39.06538 Accuracy: 0.96029\n",
      "Epoch: 759 Batch:   0 Loss: 39.05256 Accuracy: 0.96029\n",
      "Test Loss: 197.52458 Accuracy: 0.74743\n",
      "Epoch: 760 Batch:   0 Loss: 39.03977 Accuracy: 0.96029\n",
      "Epoch: 761 Batch:   0 Loss: 39.02703 Accuracy: 0.96029\n",
      "Epoch: 762 Batch:   0 Loss: 39.01429 Accuracy: 0.96029\n",
      "Epoch: 763 Batch:   0 Loss: 39.00159 Accuracy: 0.96029\n",
      "Epoch: 764 Batch:   0 Loss: 38.98892 Accuracy: 0.96029\n",
      "Epoch: 765 Batch:   0 Loss: 38.97628 Accuracy: 0.96029\n",
      "Epoch: 766 Batch:   0 Loss: 38.96366 Accuracy: 0.96029\n",
      "Epoch: 767 Batch:   0 Loss: 38.95107 Accuracy: 0.96029\n",
      "Epoch: 768 Batch:   0 Loss: 38.93850 Accuracy: 0.96029\n",
      "Epoch: 769 Batch:   0 Loss: 38.92596 Accuracy: 0.96029\n",
      "Test Loss: 198.15106 Accuracy: 0.74676\n",
      "Epoch: 770 Batch:   0 Loss: 38.91346 Accuracy: 0.96029\n",
      "Epoch: 771 Batch:   0 Loss: 38.90096 Accuracy: 0.96029\n",
      "Epoch: 772 Batch:   0 Loss: 38.88851 Accuracy: 0.95925\n",
      "Epoch: 773 Batch:   0 Loss: 38.87608 Accuracy: 0.95925\n",
      "Epoch: 774 Batch:   0 Loss: 38.86367 Accuracy: 0.95925\n",
      "Epoch: 775 Batch:   0 Loss: 38.85130 Accuracy: 0.95925\n",
      "Epoch: 776 Batch:   0 Loss: 38.83896 Accuracy: 0.96029\n",
      "Epoch: 777 Batch:   0 Loss: 38.82663 Accuracy: 0.96029\n",
      "Epoch: 778 Batch:   0 Loss: 38.81434 Accuracy: 0.96029\n",
      "Epoch: 779 Batch:   0 Loss: 38.80208 Accuracy: 0.96029\n",
      "Test Loss: 198.78558 Accuracy: 0.74676\n",
      "Epoch: 780 Batch:   0 Loss: 38.78984 Accuracy: 0.96029\n",
      "Epoch: 781 Batch:   0 Loss: 38.77764 Accuracy: 0.96029\n",
      "Epoch: 782 Batch:   0 Loss: 38.76545 Accuracy: 0.96029\n",
      "Epoch: 783 Batch:   0 Loss: 38.75330 Accuracy: 0.96029\n",
      "Epoch: 784 Batch:   0 Loss: 38.74119 Accuracy: 0.96029\n",
      "Epoch: 785 Batch:   0 Loss: 38.72908 Accuracy: 0.96029\n",
      "Epoch: 786 Batch:   0 Loss: 38.71701 Accuracy: 0.96029\n",
      "Epoch: 787 Batch:   0 Loss: 38.70496 Accuracy: 0.96029\n",
      "Epoch: 788 Batch:   0 Loss: 38.69295 Accuracy: 0.96029\n",
      "Epoch: 789 Batch:   0 Loss: 38.68097 Accuracy: 0.96029\n",
      "Test Loss: 199.42839 Accuracy: 0.74649\n",
      "Epoch: 790 Batch:   0 Loss: 38.66901 Accuracy: 0.96029\n",
      "Epoch: 791 Batch:   0 Loss: 38.65708 Accuracy: 0.96029\n",
      "Epoch: 792 Batch:   0 Loss: 38.64518 Accuracy: 0.96029\n",
      "Epoch: 793 Batch:   0 Loss: 38.63330 Accuracy: 0.96029\n",
      "Epoch: 794 Batch:   0 Loss: 38.62146 Accuracy: 0.96029\n",
      "Epoch: 795 Batch:   0 Loss: 38.60963 Accuracy: 0.96029\n",
      "Epoch: 796 Batch:   0 Loss: 38.59784 Accuracy: 0.96029\n",
      "Epoch: 797 Batch:   0 Loss: 38.58607 Accuracy: 0.96029\n",
      "Epoch: 798 Batch:   0 Loss: 38.57433 Accuracy: 0.96029\n",
      "Epoch: 799 Batch:   0 Loss: 38.56263 Accuracy: 0.96134\n",
      "Test Loss: 200.07973 Accuracy: 0.74649\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, dataDimension], name='X')\n",
    "Y = tf.placeholder(tf.float32, [None, numClasses], name='Y')\n",
    "\n",
    "protoNN = ProtoNN(dataDimension, PROJECTION_DIM,\n",
    "                  NUM_PROTOTYPES, numClasses,\n",
    "                  gamma, W=W, B=B)\n",
    "\n",
    "trainer = ProtoNNTrainer(protoNN,  REG_W, REG_B, REG_Z,\n",
    "                         SPAR_W, SPAR_B, SPAR_Z,\n",
    "                        LEARNING_RATE, X, Y, lossType='l2')\n",
    "sess = tf.Session()\n",
    "\n",
    "trainer.train(2048, 800, sess, x_train, x_test, y_train, y_test,printStep=600, valStep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy 0.7464506\n",
      "Model size constraint (Bytes):  9580\n",
      "Number of non-zeros:  2395\n"
     ]
    }
   ],
   "source": [
    "acc = sess.run(protoNN.accuracy, feed_dict={X: x_test, Y: y_test})\n",
    "pred = sess.run(protoNN.predictions, feed_dict={X: x_test, Y: y_test})\n",
    "# W, B, Z are tensorflow graph nodes\n",
    "W, B, Z, _ = protoNN.getModelMatrices()\n",
    "matrixList = sess.run([W, B, Z])\n",
    "sparcityList = [SPAR_W, SPAR_B, SPAR_Z]                       \n",
    "nnz, size, sparse = getModelSize(matrixList, sparcityList)\n",
    "print(\"Final test accuracy\", acc)\n",
    "print(\"Model size constraint (Bytes): \", size)\n",
    "print(\"Number of non-zeros: \", nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2386 1347]\n",
      " [ 546 3187]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.81378   0.63916   0.71598      3733\n",
      "           1    0.70291   0.85374   0.77102      3733\n",
      "\n",
      "    accuracy                        0.74645      7466\n",
      "   macro avg    0.75835   0.74645   0.74350      7466\n",
      "weighted avg    0.75835   0.74645   0.74350      7466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "y_test = np.argmax(y_test,axis=1)\n",
    "print (confusion_matrix(y_test,pred))\n",
    "print (classification_report(y_test,pred,digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8537369407982855"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivity = confusion_matrix(y_test,pred)[1][1]/(confusion_matrix(y_test,pred)[1][1] + confusion_matrix(y_test,pred)[1][0])\n",
    "sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6391642110902759"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specificity = confusion_matrix(y_test,pred)[0][0]/(confusion_matrix(y_test,pred)[0][0] + confusion_matrix(y_test,pred)[0][1])\n",
    "specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WINDOW 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT license.\n",
    "\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#sys.path.insert(0, '../../')\n",
    "# from edgeml.trainer.protoNNTrainer import ProtoNNTrainer\n",
    "# from edgeml.graph.protoNN import ProtoNN\n",
    "# import edgeml.utils as utils\n",
    "# import helpermethods as helper\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "sys.path.append(r\"E:\\programming\\practice\\research\\optimized code\\EdgeML\\examples\\tf\\ProtoNN\")\n",
    "import helpermethods as helper\n",
    "\n",
    "#helper methods\n",
    "sys.path.insert(0, '../')\n",
    "import argparse\n",
    "\n",
    "\n",
    "def getModelSize(matrixList, sparcityList, expected=True, bytesPerVar=4):\n",
    "    '''\n",
    "    expected: Expected size according to the parameters set. The number of\n",
    "        zeros could actually be more than that is required to satisfy the\n",
    "        sparsity constraint.\n",
    "    '''\n",
    "    nnzList, sizeList, isSparseList = [], [], []\n",
    "    hasSparse = False\n",
    "    for i in range(len(matrixList)):\n",
    "        A, s = matrixList[i], sparcityList[i]\n",
    "        assert A.ndim == 2\n",
    "        assert s >= 0\n",
    "        assert s <= 1\n",
    "        nnz, size, sparse = countnnZ(A, s, bytesPerVar=bytesPerVar)\n",
    "        nnzList.append(nnz)\n",
    "        sizeList.append(size)\n",
    "        hasSparse = (hasSparse or sparse)\n",
    "\n",
    "    totalnnZ = np.sum(nnzList)\n",
    "    totalSize = np.sum(sizeList)\n",
    "    if expected:\n",
    "        return totalnnZ, totalSize, hasSparse\n",
    "    numNonZero = 0\n",
    "    totalSize = 0\n",
    "    hasSparse = False\n",
    "    for i in range(len(matrixList)):\n",
    "        A, s = matrixList[i], sparcityList[i]\n",
    "        numNonZero_ = np.count_nonzero(A)\n",
    "        numNonZero += numNonZero_\n",
    "        hasSparse = (hasSparse or (s < 0.5))\n",
    "        if s <= 0.5:\n",
    "            totalSize += numNonZero_ * 2 * bytesPerVar\n",
    "        else:\n",
    "            totalSize += A.size * bytesPerVar\n",
    "    return numNonZero, totalSize, hasSparse\n",
    "\n",
    "\n",
    "def getGamma(gammaInit, projectionDim, dataDim, numPrototypes, x_train):\n",
    "    if gammaInit is None:\n",
    "        print(\"Using median heuristic to estimate gamma.\")\n",
    "        gamma, W, B = medianHeuristic(x_train, projectionDim,\n",
    "                                            numPrototypes)\n",
    "        print(\"Gamma estimate is: %f\" % gamma)\n",
    "        return W, B, gamma\n",
    "    return None, None, gammaInit\n",
    "\n",
    "\n",
    "def preprocessData(dataDir,w):\n",
    "    '''\n",
    "    Loads data from the dataDir and does some initial preprocessing\n",
    "    steps. Data is assumed to be contained in two files,\n",
    "    train.npy and test.npy. Each containing a 2D numpy array of dimension\n",
    "    [numberOfExamples, numberOfFeatures + 1]. The first column of each\n",
    "    matrix is assumed to contain label information.\n",
    "\n",
    "    For an N-Class problem, we assume the labels are integers from 0 through\n",
    "    N-1.\n",
    "    '''\n",
    "    # Uncomment for usual training data\n",
    "    # train = np.load(dataDir + '/train_'+str(w)+'.npy')\n",
    "    # test = np.load(dataDir + '/test_'+str(w)+'.npy')\n",
    "    # Uncomment for time domain training data\n",
    "    train = np.load(dataDir + '/ttrain_'+str(w)+'.npy')\n",
    "    test = np.load(dataDir + '/ttest_'+str(w)+'.npy')\n",
    "    # Uncomment for 1 sensordrop training data\n",
    "    # train = np.load(dataDir + '/train_'+str(w)+'.npy')\n",
    "    # test = np.load(dataDir + '/test_'+str(w)+'.npy')\n",
    "\n",
    "    dataDimension = int(train.shape[1]) - 1\n",
    "    x_train = train[:, 1:dataDimension + 1]\n",
    "    y_train_ = train[:, 0]\n",
    "    x_test = test[:, 1:dataDimension + 1]\n",
    "    y_test_ = test[:, 0]\n",
    "\n",
    "    numClasses = max(y_train_) - min(y_train_) + 1\n",
    "    numClasses = max(numClasses, max(y_test_) - min(y_test_) + 1)\n",
    "    numClasses = int(numClasses)\n",
    "\n",
    "    # mean-var\n",
    "    mean = np.mean(x_train, 0)\n",
    "    std = np.std(x_train, 0)\n",
    "    std[std[:] < 0.000001] = 1\n",
    "    x_train = (x_train - mean) / std\n",
    "    x_test = (x_test - mean) / std\n",
    "\n",
    "    # one hot y-train\n",
    "    lab = y_train_.astype('uint8')\n",
    "    lab = np.array(lab) - min(lab)\n",
    "    lab_ = np.zeros((x_train.shape[0], numClasses))\n",
    "    lab_[np.arange(x_train.shape[0]), lab] = 1\n",
    "    y_train = lab_\n",
    "\n",
    "    # one hot y-test\n",
    "    lab = y_test_.astype('uint8')\n",
    "    lab = np.array(lab) - min(lab)\n",
    "    lab_ = np.zeros((x_test.shape[0], numClasses))\n",
    "    lab_[np.arange(x_test.shape[0]), lab] = 1\n",
    "    y_test = lab_\n",
    "\n",
    "    return dataDimension, numClasses, x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "def getProtoNNArgs():\n",
    "    def checkIntPos(value):\n",
    "        ivalue = int(value)\n",
    "        if ivalue <= 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid positive int value\" % value)\n",
    "        return ivalue\n",
    "\n",
    "    def checkIntNneg(value):\n",
    "        ivalue = int(value)\n",
    "        if ivalue < 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid non-neg int value\" % value)\n",
    "        return ivalue\n",
    "\n",
    "    def checkFloatNneg(value):\n",
    "        fvalue = float(value)\n",
    "        if fvalue < 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid non-neg float value\" % value)\n",
    "        return fvalue\n",
    "\n",
    "    def checkFloatPos(value):\n",
    "        fvalue = float(value)\n",
    "        if fvalue <= 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid positive float value\" % value)\n",
    "        return fvalue\n",
    "\n",
    "    '''\n",
    "    Parse protoNN commandline arguments\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Hyperparameters for ProtoNN Algorithm')\n",
    "\n",
    "    msg = 'Data directory containing train and test data. The '\n",
    "    msg += 'data is assumed to be saved as 2-D numpy matrices with '\n",
    "    msg += 'names `train.npy` and `test.npy`, of dimensions\\n'\n",
    "    msg += '\\t[numberOfInstances, numberOfFeatures + 1].\\n'\n",
    "    msg += 'The first column of each file is assumed to contain label information.'\n",
    "    msg += ' For a N-class problem, labels are assumed to be integers from 0 to'\n",
    "    msg += ' N-1 (inclusive).'\n",
    "    parser.add_argument('-d', '--data-dir', required=True, help=msg)\n",
    "    parser.add_argument('-l', '--projection-dim', type=checkIntPos, default=10,\n",
    "                        help='Projection Dimension.')\n",
    "    parser.add_argument('-p', '--num-prototypes', type=checkIntPos, default=20,\n",
    "                        help='Number of prototypes.')\n",
    "    parser.add_argument('-g', '--gamma', type=checkFloatPos, default=None,\n",
    "                        help='Gamma for Gaussian kernel. If not provided, ' +\n",
    "                        'median heuristic will be used to estimate gamma.')\n",
    "\n",
    "    parser.add_argument('-e', '--epochs', type=checkIntPos, default=100,\n",
    "                        help='Total training epochs.')\n",
    "    parser.add_argument('-b', '--batch-size', type=checkIntPos, default=32,\n",
    "                        help='Batch size for each pass.')\n",
    "    parser.add_argument('-r', '--learning-rate', type=checkFloatPos,\n",
    "                        default=0.001,\n",
    "                        help='Initial Learning rate for ADAM Optimizer.')\n",
    "\n",
    "    parser.add_argument('-rW', type=float, default=0.000,\n",
    "                        help='Coefficient for l2 regularizer for predictor' +\n",
    "                        ' parameter W ' + '(default = 0.0).')\n",
    "    parser.add_argument('-rB', type=float, default=0.00,\n",
    "                        help='Coefficient for l2 regularizer for predictor' +\n",
    "                        ' parameter B ' + '(default = 0.0).')\n",
    "    parser.add_argument('-rZ', type=float, default=0.00,\n",
    "                        help='Coefficient for l2 regularizer for predictor' +\n",
    "                        'parameter Z ' +\n",
    "                        '(default = 0.0).')\n",
    "\n",
    "    parser.add_argument('-sW', type=float, default=1.000,\n",
    "                        help='Sparsity constraint for predictor parameter W ' +\n",
    "                        '(default = 1.0, i.e. dense matrix).')\n",
    "    parser.add_argument('-sB', type=float, default=1.00,\n",
    "                        help='Sparsity constraint for predictor parameter B ' +\n",
    "                        '(default = 1.0, i.e. dense matrix).')\n",
    "    parser.add_argument('-sZ', type=float, default=1.00,\n",
    "                        help='Sparsity constraint for predictor parameter Z ' +\n",
    "                        '(default = 1.0, i.e. dense matrix).')\n",
    "    parser.add_argument('-pS', '--print-step', type=int, default=200,\n",
    "                        help='The number of update steps between print ' +\n",
    "                        'calls to console.')\n",
    "    parser.add_argument('-vS', '--val-step', type=int, default=3,\n",
    "                        help='The number of epochs between validation' +\n",
    "                        'performance evaluation')\n",
    "    return parser.parse_args()\n",
    "\n",
    "#utils\n",
    "import scipy.cluster\n",
    "import scipy.spatial\n",
    "import os\n",
    "\n",
    "\n",
    "def medianHeuristic(data, projectionDimension, numPrototypes, W_init=None):\n",
    "    '''\n",
    "    This method can be used to estimate gamma for ProtoNN. An approximation to\n",
    "    median heuristic is used here.\n",
    "    1. First the data is collapsed into the projectionDimension by W_init. If\n",
    "    W_init is not provided, it is initialized from a random normal(0, 1). Hence\n",
    "    data normalization is essential.\n",
    "    2. Prototype are computed by running a  k-means clustering on the projected\n",
    "    data.\n",
    "    3. The median distance is then estimated by calculating median distance\n",
    "    between prototypes and projected data points.\n",
    "\n",
    "    data needs to be [-1, numFeats]\n",
    "    If using this method to initialize gamma, please use the W and B as well.\n",
    "\n",
    "    TODO: Return estimate of Z (prototype labels) based on cluster centroids\n",
    "    andand labels\n",
    "\n",
    "    TODO: Clustering fails due to singularity error if projecting upwards\n",
    "\n",
    "    W [dxd_cap]\n",
    "    B [d_cap, m]\n",
    "    returns gamma, W, B\n",
    "    '''\n",
    "    assert data.ndim == 2\n",
    "    X = data\n",
    "    featDim = data.shape[1]\n",
    "    if projectionDimension > featDim:\n",
    "        print(\"Warning: Projection dimension > feature dimension. Gamma\")\n",
    "        print(\"\\t estimation due to median heuristic could fail.\")\n",
    "        print(\"\\tTo retain the projection dataDimension, provide\")\n",
    "        print(\"\\ta value for gamma.\")\n",
    "\n",
    "    if W_init is None:\n",
    "        W_init = np.random.normal(size=[featDim, projectionDimension])\n",
    "    W = W_init\n",
    "    XW = np.matmul(X, W)\n",
    "    assert XW.shape[1] == projectionDimension\n",
    "    assert XW.shape[0] == len(X)\n",
    "    # Requires [N x d_cap] data matrix of N observations of d_cap-dimension and\n",
    "    # the number of centroids m. Returns, [n x d_cap] centroids and\n",
    "    # elementwise center information.\n",
    "    B, centers = scipy.cluster.vq.kmeans2(XW, numPrototypes)\n",
    "    # Requires two matrices. Number of observations x dimension of observation\n",
    "    # space. Distances[i,j] is the distance between XW[i] and B[j]\n",
    "    distances = scipy.spatial.distance.cdist(XW, B, metric='euclidean')\n",
    "    distances = np.reshape(distances, [-1])\n",
    "    gamma = np.median(distances)\n",
    "    gamma = 1 / (2.5 * gamma)\n",
    "    return gamma.astype('float32'), W.astype('float32'), B.T.astype('float32')\n",
    "\n",
    "\n",
    "def multiClassHingeLoss(logits, label, batch_th):\n",
    "    '''\n",
    "    MultiClassHingeLoss to match C++ Version - No TF internal version\n",
    "    '''\n",
    "    flatLogits = tf.reshape(logits, [-1, ])\n",
    "    label_ = tf.argmax(label, 1)\n",
    "\n",
    "    correctId = tf.range(0, batch_th) * label.shape[1] + label_\n",
    "    correctLogit = tf.gather(flatLogits, correctId)\n",
    "\n",
    "    maxLabel = tf.argmax(logits, 1)\n",
    "    top2, _ = tf.nn.top_k(logits, k=2, sorted=True)\n",
    "\n",
    "    wrongMaxLogit = tf.where(\n",
    "        tf.equal(maxLabel, label_), top2[:, 1], top2[:, 0])\n",
    "\n",
    "    return tf.reduce_mean(tf.nn.relu(1. + wrongMaxLogit - correctLogit))\n",
    "\n",
    "\n",
    "def crossEntropyLoss(logits, label):\n",
    "    '''\n",
    "    Cross Entropy loss for MultiClass case in joint training for\n",
    "    faster convergence\n",
    "    '''\n",
    "    return tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                   labels=tf.stop_gradient(label)))\n",
    "\n",
    "\n",
    "def mean_absolute_error(logits, label):\n",
    "    '''\n",
    "    Function to compute the mean absolute error.\n",
    "    '''\n",
    "    return tf.reduce_mean(tf.abs(tf.subtract(logits, label)))\n",
    "\n",
    "\n",
    "def hardThreshold(A, s):\n",
    "    '''\n",
    "    Hard thresholding function on Tensor A with sparsity s\n",
    "    '''\n",
    "    A_ = np.copy(A)\n",
    "    A_ = A_.ravel()\n",
    "    if len(A_) > 0:\n",
    "        th = np.percentile(np.abs(A_), (1 - s) * 100.0, interpolation='higher')\n",
    "        A_[np.abs(A_) < th] = 0.0\n",
    "    A_ = A_.reshape(A.shape)\n",
    "    return A_\n",
    "\n",
    "\n",
    "def copySupport(src, dest):\n",
    "    '''\n",
    "    copy support of src tensor to dest tensor\n",
    "    '''\n",
    "    support = np.nonzero(src)\n",
    "    dest_ = dest\n",
    "    dest = np.zeros(dest_.shape)\n",
    "    dest[support] = dest_[support]\n",
    "    return dest\n",
    "\n",
    "\n",
    "def countnnZ(A, s, bytesPerVar=4):\n",
    "    '''\n",
    "    Returns # of non-zeros and representative size of the tensor\n",
    "    Uses dense for s >= 0.5 - 4 byte\n",
    "    Else uses sparse - 8 byte\n",
    "    '''\n",
    "    params = 1\n",
    "    hasSparse = False\n",
    "    for i in range(0, len(A.shape)):\n",
    "        params *= int(A.shape[i])\n",
    "    if s < 0.5:\n",
    "        nnZ = np.ceil(params * s)\n",
    "        hasSparse = True\n",
    "        return nnZ, nnZ * 2 * bytesPerVar, hasSparse\n",
    "    else:\n",
    "        nnZ = params\n",
    "        return nnZ, nnZ * bytesPerVar, hasSparse\n",
    "\n",
    "\n",
    "def getConfusionMatrix(predicted, target, numClasses):\n",
    "    '''\n",
    "    Returns a confusion matrix for a multiclass classification\n",
    "    problem. `predicted` is a 1-D array of integers representing\n",
    "    the predicted classes and `target` is the target classes.\n",
    "\n",
    "    confusion[i][j]: Number of elements of class j\n",
    "        predicted as class i\n",
    "    Labels are assumed to be in range(0, numClasses)\n",
    "    Use`printFormattedConfusionMatrix` to echo the confusion matrix\n",
    "    in a user friendly form.\n",
    "    '''\n",
    "    assert(predicted.ndim == 1)\n",
    "    assert(target.ndim == 1)\n",
    "    arr = np.zeros([numClasses, numClasses])\n",
    "\n",
    "    for i in range(len(predicted)):\n",
    "        arr[predicted[i]][target[i]] += 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "def printFormattedConfusionMatrix(matrix):\n",
    "    '''\n",
    "    Given a 2D confusion matrix, prints it in a human readable way.\n",
    "    The confusion matrix is expected to be a 2D numpy array with\n",
    "    square dimensions\n",
    "    '''\n",
    "    assert(matrix.ndim == 2)\n",
    "    assert(matrix.shape[0] == matrix.shape[1])\n",
    "    RECALL = 'Recall'\n",
    "    PRECISION = 'PRECISION'\n",
    "    print(\"|%s|\" % ('True->'), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%7d|\" % i, end='')\n",
    "    print(\"%s|\" % 'Precision')\n",
    "\n",
    "    print(\"|%s|\" % ('-' * len(RECALL)), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%s|\" % ('-' * 7), end='')\n",
    "    print(\"%s|\" % ('-' * len(PRECISION)))\n",
    "\n",
    "    precisionlist = np.sum(matrix, axis=1)\n",
    "    recalllist = np.sum(matrix, axis=0)\n",
    "    precisionlist = [matrix[i][i] / x if x !=\n",
    "                     0 else -1 for i, x in enumerate(precisionlist)]\n",
    "    recalllist = [matrix[i][i] / x if x !=\n",
    "                  0 else -1 for i, x in enumerate(recalllist)]\n",
    "    for i in range(matrix.shape[0]):\n",
    "        # len recall = 6\n",
    "        print(\"|%6d|\" % (i), end='')\n",
    "        for j in range(matrix.shape[0]):\n",
    "            print(\"%7d|\" % (matrix[i][j]), end='')\n",
    "        print(\"%s\" % (\" \" * (len(PRECISION) - 7)), end='')\n",
    "        if precisionlist[i] != -1:\n",
    "            print(\"%1.5f|\" % precisionlist[i])\n",
    "        else:\n",
    "            print(\"%7s|\" % \"nan\")\n",
    "\n",
    "    print(\"|%s|\" % ('-' * len(RECALL)), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%s|\" % ('-' * 7), end='')\n",
    "    print(\"%s|\" % ('-' * len(PRECISION)))\n",
    "    print(\"|%s|\" % ('Recall'), end='')\n",
    "\n",
    "    for i in range(matrix.shape[0]):\n",
    "        if recalllist[i] != -1:\n",
    "            print(\"%1.5f|\" % (recalllist[i]), end='')\n",
    "        else:\n",
    "            print(\"%7s|\" % \"nan\", end='')\n",
    "\n",
    "    print('%s|' % (' ' * len(PRECISION)))\n",
    "\n",
    "\n",
    "def getPrecisionRecall(cmatrix, label=1):\n",
    "    trueP = cmatrix[label][label]\n",
    "    denom = np.sum(cmatrix, axis=0)[label]\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    recall = trueP / denom\n",
    "    denom = np.sum(cmatrix, axis=1)[label]\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    precision = trueP / denom\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMacroPrecisionRecall(cmatrix):\n",
    "    # TP + FP\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    # TP + FN\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    precisionlist__ = [cmatrix[i][i] / x if x !=\n",
    "                       0 else 0 for i, x in enumerate(precisionlist)]\n",
    "    recalllist__ = [cmatrix[i][i] / x if x !=\n",
    "                    0 else 0 for i, x in enumerate(recalllist)]\n",
    "    precision = np.sum(precisionlist__)\n",
    "    precision /= len(precisionlist__)\n",
    "    recall = np.sum(recalllist__)\n",
    "    recall /= len(recalllist__)\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMicroPrecisionRecall(cmatrix):\n",
    "    # TP + FP\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    # TP + FN\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    num = 0.0\n",
    "    for i in range(len(cmatrix)):\n",
    "        num += cmatrix[i][i]\n",
    "\n",
    "    precision = num / np.sum(precisionlist)\n",
    "    recall = num / np.sum(recalllist)\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMacroMicroFScore(cmatrix):\n",
    "    '''\n",
    "    Returns macro and micro f-scores.\n",
    "    Refer: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.104.8244&rep=rep1&type=pdf\n",
    "    '''\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    precisionlist__ = [cmatrix[i][i] / x if x !=\n",
    "                       0 else 0 for i, x in enumerate(precisionlist)]\n",
    "    recalllist__ = [cmatrix[i][i] / x if x !=\n",
    "                    0 else 0 for i, x in enumerate(recalllist)]\n",
    "    macro = 0.0\n",
    "    for i in range(len(precisionlist)):\n",
    "        denom = precisionlist__[i] + recalllist__[i]\n",
    "        numer = precisionlist__[i] * recalllist__[i] * 2\n",
    "        if denom == 0:\n",
    "            denom = 1\n",
    "        macro += numer / denom\n",
    "    macro /= len(precisionlist)\n",
    "\n",
    "    num = 0.0\n",
    "    for i in range(len(precisionlist)):\n",
    "        num += cmatrix[i][i]\n",
    "\n",
    "    denom1 = np.sum(precisionlist)\n",
    "    denom2 = np.sum(recalllist)\n",
    "    pi = num / denom1\n",
    "    rho = num / denom2\n",
    "    denom = pi + rho\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    micro = 2 * pi * rho / denom\n",
    "    return macro, micro\n",
    "\n",
    "\n",
    "class GraphManager:\n",
    "    '''\n",
    "    Manages saving and restoring graphs. Designed to be used with EMI-RNN\n",
    "    though is general enough to be useful otherwise as well.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def checkpointModel(self, saver, sess, modelPrefix,\n",
    "                        globalStep=1000, redirFile=None):\n",
    "        saver.save(sess, modelPrefix, global_step=globalStep)\n",
    "        print('Model saved to %s, global_step %d' % (modelPrefix, globalStep),\n",
    "              file=redirFile)\n",
    "\n",
    "    def loadCheckpoint(self, sess, modelPrefix, globalStep,\n",
    "                       redirFile=None):\n",
    "        metaname = modelPrefix + '-%d.meta' % globalStep\n",
    "        basename = os.path.basename(metaname)\n",
    "        fileList = os.listdir(os.path.dirname(modelPrefix))\n",
    "        fileList = [x for x in fileList if x.startswith(basename)]\n",
    "        assert len(fileList) > 0, 'Checkpoint file not found'\n",
    "        msg = 'Too many or too few checkpoint files for globalStep: %d' % globalStep\n",
    "        assert len(fileList) is 1, msg\n",
    "        chkpt = basename + '/' + fileList[0]\n",
    "        saver = tf.train.import_meta_graph(metaname)\n",
    "        metaname = metaname[:-5]\n",
    "        saver.restore(sess, metaname)\n",
    "        graph = tf.get_default_graph()\n",
    "        return graph\n",
    "\n",
    "#Trainer\n",
    "class ProtoNNTrainer:\n",
    "    def __init__(self, protoNNObj, regW, regB, regZ,\n",
    "                 sparcityW, sparcityB, sparcityZ,\n",
    "                 learningRate, X, Y, lossType='l2'):\n",
    "        '''\n",
    "        A wrapper for the various techniques used for training ProtoNN. This\n",
    "        subsumes both the responsibility of loss graph construction and\n",
    "        performing training. The original training routine that is part of the\n",
    "        C++ implementation of EdgeML used iterative hard thresholding (IHT),\n",
    "        gamma estimation through median heuristic and other tricks for\n",
    "        training ProtoNN. This module implements the same in Tensorflow\n",
    "        and python.\n",
    "\n",
    "        protoNNObj: An instance of ProtoNN class defining the forward\n",
    "            computation graph. The loss functions and training routines will be\n",
    "            attached to this instance.\n",
    "        regW, regB, regZ: Regularization constants for W, B, and\n",
    "            Z matrices of protoNN.\n",
    "        sparcityW, sparcityB, sparcityZ: Sparsity constraints\n",
    "            for W, B and Z matrices. A value between 0 (exclusive) and 1\n",
    "            (inclusive) is expected. A value of 1 indicates dense training.\n",
    "        learningRate: Initial learning rate for ADAM optimizer.\n",
    "        X, Y : Placeholders for data and labels.\n",
    "            X [-1, featureDimension]\n",
    "            Y [-1, num Labels]\n",
    "        lossType: ['l2', 'xentropy']\n",
    "        '''\n",
    "        self.protoNNObj = protoNNObj\n",
    "        self.__regW = regW\n",
    "        self.__regB = regB\n",
    "        self.__regZ = regZ\n",
    "        self.__sW = sparcityW\n",
    "        self.__sB = sparcityB\n",
    "        self.__sZ = sparcityZ\n",
    "        self.__lR = learningRate\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.sparseTraining = True\n",
    "        if (sparcityW == 1.0) and (sparcityB == 1.0) and (sparcityZ == 1.0):\n",
    "            self.sparseTraining = False\n",
    "            print(\"Sparse training disabled.\", file=sys.stderr)\n",
    "        # Define placeholders for sparse training\n",
    "        self.W_th = None\n",
    "        self.B_th = None\n",
    "        self.Z_th = None\n",
    "        self.__lossType = lossType\n",
    "        self.__validInit = False\n",
    "        self.__validInit = self.__validateInit()\n",
    "        self.__protoNNOut = protoNNObj(X, Y)\n",
    "        self.loss = self.__lossGraph()\n",
    "        self.trainStep = self.__trainGraph()\n",
    "        self.__hthOp = self.__getHardThresholdOp()\n",
    "        self.accuracy = protoNNObj.getAccuracyOp()\n",
    "\n",
    "    def __validateInit(self):\n",
    "        self.__validInit = False\n",
    "        msg = \"Sparsity value should be between\"\n",
    "        msg += \" 0 and 1 (both inclusive).\"\n",
    "        assert self.__sW >= 0. and self.__sW <= 1., 'W:' + msg\n",
    "        assert self.__sB >= 0. and self.__sB <= 1., 'B:' + msg\n",
    "        assert self.__sZ >= 0. and self.__sZ <= 1., 'Z:' + msg\n",
    "        d, dcap, m, L, _ = self.protoNNObj.getHyperParams()\n",
    "        msg = 'Y should be of dimension [-1, num labels/classes]'\n",
    "        msg += ' specified as part of ProtoNN object.'\n",
    "        assert (len(self.Y.shape)) == 2, msg\n",
    "        assert (self.Y.shape[1] == L), msg\n",
    "        msg = 'X should be of dimension [-1, featureDimension]'\n",
    "        msg += ' specified as part of ProtoNN object.'\n",
    "        assert (len(self.X.shape) == 2), msg\n",
    "        assert (self.X.shape[1] == d), msg\n",
    "        self.__validInit = True\n",
    "        msg = 'Values can be \\'l2\\', or \\'xentropy\\''\n",
    "        if self.__lossType not in ['l2', 'xentropy']:\n",
    "            raise ValueError(msg)\n",
    "        return True\n",
    "\n",
    "    def __lossGraph(self):\n",
    "        pnnOut = self.__protoNNOut\n",
    "        l1, l2, l3 = self.__regW, self.__regB, self.__regZ\n",
    "        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n",
    "        if self.__lossType == 'l2':\n",
    "            with tf.name_scope('protonn-l2-loss'):\n",
    "                loss_0 = tf.nn.l2_loss(self.Y - pnnOut)\n",
    "                reg = l1 * tf.nn.l2_loss(W) + l2 * tf.nn.l2_loss(B)\n",
    "                reg += l3 * tf.nn.l2_loss(Z)\n",
    "                loss = loss_0 + reg\n",
    "        elif self.__lossType == 'xentropy':\n",
    "            with tf.name_scope('protonn-xentropy-loss'):\n",
    "                loss_0 = tf.nn.softmax_cross_entropy_with_logits_v2(logits=pnnOut,\n",
    "                                                         labels=tf.stop_gradient(self.Y))\n",
    "                loss_0 = tf.reduce_mean(loss_0)\n",
    "                reg = l1 * tf.nn.l2_loss(W) + l2 * tf.nn.l2_loss(B)\n",
    "                reg += l3 * tf.nn.l2_loss(Z)\n",
    "                loss = loss_0 + reg\n",
    "        return loss\n",
    "\n",
    "    def __trainGraph(self):\n",
    "        with tf.name_scope('protonn-gradient-adam'):\n",
    "            trainStep = tf.train.AdamOptimizer(self.__lR)\n",
    "            trainStep = trainStep.minimize(self.loss)\n",
    "        return trainStep\n",
    "\n",
    "    def __getHardThresholdOp(self):\n",
    "        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n",
    "        self.W_th = tf.placeholder(tf.float32, name='W_th')\n",
    "        self.B_th = tf.placeholder(tf.float32, name='B_th')\n",
    "        self.Z_th = tf.placeholder(tf.float32, name='Z_th')\n",
    "        with tf.name_scope('hard-threshold-assignments'):\n",
    "            hard_thrsd_W = W.assign(self.W_th)\n",
    "            hard_thrsd_B = B.assign(self.B_th)\n",
    "            hard_thrsd_Z = Z.assign(self.Z_th)\n",
    "            hard_thrsd_op = tf.group(hard_thrsd_W, hard_thrsd_B, hard_thrsd_Z)\n",
    "        return hard_thrsd_op\n",
    "\n",
    "    def train(self, batchSize, totalEpochs, sess,\n",
    "              x_train, x_val, y_train, y_val, noInit=False,\n",
    "              redirFile=None, printStep=10, valStep=3):\n",
    "        '''\n",
    "        Performs dense training of ProtoNN followed by iterative hard\n",
    "        thresholding to enforce sparsity constraints.\n",
    "\n",
    "        batchSize: Batch size per update\n",
    "        totalEpochs: The number of epochs to run training for. One epoch is\n",
    "            defined as one pass over the entire training data.\n",
    "        sess: The Tensorflow session to use for running various graph\n",
    "            operators.\n",
    "        x_train, x_val, y_train, y_val: The numpy array containing train and\n",
    "            validation data. x data is assumed to in of shape [-1,\n",
    "            featureDimension] while y should have shape [-1, numberLabels].\n",
    "        noInit: By default, all the tensors of the computation graph are\n",
    "        initialized at the start of the training session. Set noInit=False to\n",
    "        disable this behaviour.\n",
    "        printStep: Number of batches between echoing of loss and train accuracy.\n",
    "        valStep: Number of epochs between evolutions on validation set.\n",
    "        '''\n",
    "        d, d_cap, m, L, gamma = self.protoNNObj.getHyperParams()\n",
    "        assert batchSize >= 1, 'Batch size should be positive integer'\n",
    "        assert totalEpochs >= 1, 'Total epochs should be positive integer'\n",
    "        assert x_train.ndim == 2, 'Expected training data to be of rank 2'\n",
    "        assert x_train.shape[1] == d, 'Expected x_train to be [-1, %d]' % d\n",
    "        assert x_val.ndim == 2, 'Expected validation data to be of rank 2'\n",
    "        assert x_val.shape[1] == d, 'Expected x_val to be [-1, %d]' % d\n",
    "        assert y_train.ndim == 2, 'Expected training labels to be of rank 2'\n",
    "        assert y_train.shape[1] == L, 'Expected y_train to be [-1, %d]' % L\n",
    "        assert y_val.ndim == 2, 'Expected validation labels to be of rank 2'\n",
    "        assert y_val.shape[1] == L, 'Expected y_val to be [-1, %d]' % L\n",
    "\n",
    "        # Numpy will throw asserts for arrays\n",
    "        if sess is None:\n",
    "            raise ValueError('sess must be valid Tensorflow session.')\n",
    "\n",
    "        trainNumBatches = int(np.ceil(len(x_train) / batchSize))\n",
    "        valNumBatches = int(np.ceil(len(x_val) / batchSize))\n",
    "        x_train_batches = np.array_split(x_train, trainNumBatches)\n",
    "        y_train_batches = np.array_split(y_train, trainNumBatches)\n",
    "        x_val_batches = np.array_split(x_val, valNumBatches)\n",
    "        y_val_batches = np.array_split(y_val, valNumBatches)\n",
    "        if not noInit:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        X, Y = self.X, self.Y\n",
    "        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n",
    "        for epoch in range(totalEpochs):\n",
    "            for i in range(len(x_train_batches)):\n",
    "                batch_x = x_train_batches[i]\n",
    "                batch_y = y_train_batches[i]\n",
    "                feed_dict = {\n",
    "                    X: batch_x,\n",
    "                    Y: batch_y\n",
    "                }\n",
    "                sess.run(self.trainStep, feed_dict=feed_dict)\n",
    "                if i % printStep == 0:\n",
    "                    loss, acc = sess.run([self.loss, self.accuracy],\n",
    "                                         feed_dict=feed_dict)\n",
    "                    msg = \"Epoch: %3d Batch: %3d\" % (epoch, i)\n",
    "                    msg += \" Loss: %3.5f Accuracy: %2.5f\" % (loss, acc)\n",
    "                    print(msg, file=redirFile)\n",
    "\n",
    "            # Perform Hard thresholding\n",
    "            if self.sparseTraining:\n",
    "                W_, B_, Z_ = sess.run([W, B, Z])\n",
    "                fd_thrsd = {\n",
    "                    self.W_th: hardThreshold(W_, self.__sW),\n",
    "                    self.B_th: hardThreshold(B_, self.__sB),\n",
    "                    self.Z_th: hardThreshold(Z_, self.__sZ)\n",
    "                }\n",
    "                sess.run(self.__hthOp, feed_dict=fd_thrsd)\n",
    "\n",
    "            if (epoch + 1) % valStep  == 0:\n",
    "                acc = 0.0\n",
    "                loss = 0.0\n",
    "                for j in range(len(x_val_batches)):\n",
    "                    batch_x = x_val_batches[j]\n",
    "                    batch_y = y_val_batches[j]\n",
    "                    feed_dict = {\n",
    "                        X: batch_x,\n",
    "                        Y: batch_y\n",
    "                    }\n",
    "                    acc_, loss_ = sess.run([self.accuracy, self.loss],\n",
    "                                           feed_dict=feed_dict)\n",
    "                    acc += acc_\n",
    "                    loss += loss_\n",
    "                acc /= len(y_val_batches)\n",
    "                loss /= len(y_val_batches)\n",
    "                print(\"Test Loss: %2.5f Accuracy: %2.5f\" % (loss, acc))\n",
    "\n",
    "\n",
    "\n",
    "class ProtoNN:\n",
    "    def __init__(self, inputDimension, projectionDimension, numPrototypes,\n",
    "                 numOutputLabels, gamma,\n",
    "                 W = None, B = None, Z = None):\n",
    "        '''\n",
    "        Forward computation graph for ProtoNN.\n",
    "\n",
    "        inputDimension: Input data dimension or feature dimension.\n",
    "        projectionDimension: hyperparameter\n",
    "        numPrototypes: hyperparameter\n",
    "        numOutputLabels: The number of output labels or classes\n",
    "        W, B, Z: Numpy matrices that can be used to initialize\n",
    "            projection matrix(W), prototype matrix (B) and prototype labels\n",
    "            matrix (B).\n",
    "            Expected Dimensions:\n",
    "                W   inputDimension (d) x projectionDimension (d_cap)\n",
    "                B   projectionDimension (d_cap) x numPrototypes (m)\n",
    "                Z   numOutputLabels (L) x numPrototypes (m)\n",
    "        '''\n",
    "        with tf.name_scope('protoNN') as ns:\n",
    "            self.__nscope = ns\n",
    "        self.__d = inputDimension\n",
    "        self.__d_cap = projectionDimension\n",
    "        self.__m = numPrototypes\n",
    "        self.__L = numOutputLabels\n",
    "\n",
    "        self.__inW = W\n",
    "        self.__inB = B\n",
    "        self.__inZ = Z\n",
    "        self.__inGamma = gamma\n",
    "        self.W, self.B, self.Z = None, None, None\n",
    "        self.gamma = None\n",
    "\n",
    "        self.__validInit = False\n",
    "        self.__initWBZ()\n",
    "        self.__initGamma()\n",
    "        self.__validateInit()\n",
    "        self.protoNNOut = None\n",
    "        self.predictions = None\n",
    "        self.accuracy = None\n",
    "\n",
    "    def __validateInit(self):\n",
    "        self.__validInit = False\n",
    "        errmsg = \"Dimensions mismatch! Should be W[d, d_cap]\"\n",
    "        errmsg += \", B[d_cap, m] and Z[L, m]\"\n",
    "        d, d_cap, m, L, _ = self.getHyperParams()\n",
    "        assert self.W.shape[0] == d, errmsg\n",
    "        assert self.W.shape[1] == d_cap, errmsg\n",
    "        assert self.B.shape[0] == d_cap, errmsg\n",
    "        assert self.B.shape[1] == m, errmsg\n",
    "        assert self.Z.shape[0] == L, errmsg\n",
    "        assert self.Z.shape[1] == m, errmsg\n",
    "        self.__validInit = True\n",
    "\n",
    "    def __initWBZ(self):\n",
    "        with tf.name_scope(self.__nscope):\n",
    "            W = self.__inW\n",
    "            if W is None:\n",
    "                W = tf.random_normal_initializer()\n",
    "                W = W([self.__d, self.__d_cap])\n",
    "            self.W = tf.Variable(W, name='W', dtype=tf.float32)\n",
    "\n",
    "            B = self.__inB\n",
    "            if B is None:\n",
    "                B = tf.random_uniform_initializer()\n",
    "                B = B([self.__d_cap, self.__m])\n",
    "            self.B = tf.Variable(B, name='B', dtype=tf.float32)\n",
    "\n",
    "            Z = self.__inZ\n",
    "            if Z is None:\n",
    "                Z = tf.random_normal_initializer()\n",
    "                Z = Z([self.__L, self.__m])\n",
    "            Z = tf.Variable(Z, name='Z', dtype=tf.float32)\n",
    "            self.Z = Z\n",
    "        return self.W, self.B, self.Z\n",
    "\n",
    "    def __initGamma(self):\n",
    "        with tf.name_scope(self.__nscope):\n",
    "            gamma = self.__inGamma\n",
    "            self.gamma = tf.constant(gamma, name='gamma')\n",
    "\n",
    "    def getHyperParams(self):\n",
    "        '''\n",
    "        Returns the model hyperparameters:\n",
    "            [inputDimension, projectionDimension,\n",
    "            numPrototypes, numOutputLabels, gamma]\n",
    "        '''\n",
    "        d = self.__d\n",
    "        dcap = self.__d_cap\n",
    "        m = self.__m\n",
    "        L = self.__L\n",
    "        return d, dcap, m, L, self.gamma\n",
    "\n",
    "    def getModelMatrices(self):\n",
    "        '''\n",
    "        Returns Tensorflow tensors of the model matrices, which\n",
    "        can then be evaluated to obtain corresponding numpy arrays.\n",
    "\n",
    "        These can then be exported as part of other implementations of\n",
    "        ProtonNN, for instance a C++ implementation or pure python\n",
    "        implementation.\n",
    "        Returns\n",
    "            [ProjectionMatrix (W), prototypeMatrix (B),\n",
    "             prototypeLabelsMatrix (Z), gamma]\n",
    "        '''\n",
    "        return self.W, self.B, self.Z, self.gamma\n",
    "\n",
    "    def __call__(self, X, Y=None):\n",
    "        '''\n",
    "        This method is responsible for construction of the forward computation\n",
    "        graph. The end point of the computation graph, or in other words the\n",
    "        output operator for the forward computation is returned. Additionally,\n",
    "        if the argument Y is provided, a classification accuracy operator with\n",
    "        Y as target will also be created. For this, Y is assumed to in one-hot\n",
    "        encoded format and the class with the maximum prediction score is\n",
    "        compared to the encoded class in Y.  This accuracy operator is returned\n",
    "        by getAccuracyOp() method. If a different accuracyOp is required, it\n",
    "        can be defined by overriding the createAccOp(protoNNScoresOut, Y)\n",
    "        method.\n",
    "\n",
    "        X: Input tensor or placeholder of shape [-1, inputDimension]\n",
    "        Y: Optional tensor or placeholder for targets (labels or classes).\n",
    "            Expected shape is [-1, numOutputLabels].\n",
    "        returns: The forward computation outputs, self.protoNNOut\n",
    "        '''\n",
    "        # This should never execute\n",
    "        assert self.__validInit is True, \"Initialization failed!\"\n",
    "        if self.protoNNOut is not None:\n",
    "            return self.protoNNOut\n",
    "\n",
    "        W, B, Z, gamma = self.W, self.B, self.Z, self.gamma\n",
    "        with tf.name_scope(self.__nscope):\n",
    "            WX = tf.matmul(X, W)\n",
    "            # Convert WX to tensor so that broadcasting can work\n",
    "            dim = [-1, WX.shape.as_list()[1], 1]\n",
    "            WX = tf.reshape(WX, dim)\n",
    "            dim = [1, B.shape.as_list()[0], -1]\n",
    "            B = tf.reshape(B, dim)\n",
    "            l2sim = B - WX\n",
    "            l2sim = tf.pow(l2sim, 2)\n",
    "            l2sim = tf.reduce_sum(l2sim, 1, keepdims=True)\n",
    "            self.l2sim = l2sim\n",
    "            gammal2sim = (-1 * gamma * gamma) * l2sim\n",
    "            M = tf.exp(gammal2sim)\n",
    "            dim = [1] + Z.shape.as_list()\n",
    "            Z = tf.reshape(Z, dim)\n",
    "            y = tf.multiply(Z, M)\n",
    "            y = tf.reduce_sum(y, 2, name='protoNNScoreOut')\n",
    "            self.protoNNOut = y\n",
    "            self.predictions = tf.argmax(y, 1, name='protoNNPredictions')\n",
    "            if Y is not None:\n",
    "                self.createAccOp(self.protoNNOut, Y)\n",
    "        return y\n",
    "\n",
    "    def createAccOp(self, outputs, target):\n",
    "        '''\n",
    "        Define an accuracy operation on ProtoNN's output scores and targets.\n",
    "        Here a simple classification accuracy operator is defined. More\n",
    "        complicated operators (for multiple label problems and so forth) can be\n",
    "        defined by overriding this method\n",
    "        '''\n",
    "        assert self.predictions is not None\n",
    "        target = tf.argmax(target, 1)\n",
    "        correctPrediction = tf.equal(self.predictions, target)\n",
    "        acc = tf.reduce_mean(tf.cast(correctPrediction, tf.float32),\n",
    "                             name='protoNNAccuracy')\n",
    "        self.accuracy = acc\n",
    "\n",
    "    def getPredictionsOp(self):\n",
    "        '''\n",
    "        The predictions operator is defined as argmax(protoNNScores) for each\n",
    "        prediction.\n",
    "        '''\n",
    "        return self.predictions\n",
    "\n",
    "    def getAccuracyOp(self):\n",
    "        '''\n",
    "        returns accuracyOp as defined by createAccOp. It defaults to\n",
    "        multi-class classification accuracy.\n",
    "        '''\n",
    "        msg = \"Accuracy operator not defined in graph. Did you provide Y as an\"\n",
    "        msg += \" argument to _call_?\"\n",
    "        assert self.accuracy is not None, msg\n",
    "        return self.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Dimension:  423\n",
      "Num classes:  2\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = r\"./experiments\"\n",
    "windowLen = 'data_w4'\n",
    "out = preprocessData(DATA_DIR,windowLen)\n",
    "dataDimension = out[0]\n",
    "numClasses = out[1]\n",
    "x_train, y_train = out[2], out[3]\n",
    "x_test, y_test = out[4], out[5]\n",
    "print(\"Feature Dimension: \", dataDimension)\n",
    "print(\"Num classes: \", numClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_DIR = r\"./experiments\"\n",
    "train, test = np.load(DATA_DIR + '/ttrain_data_w4.npy'), np.load(DATA_DIR + '/ttest_data_w4.npy')\n",
    "x_train, y_train = train[:, 1:], train[:, 0]\n",
    "x_test, y_test = test[:, 1:], test[:, 0]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "numClasses = max(y_train) - min(y_train) + 1\n",
    "numClasses = max(numClasses, max(y_test) - min(y_test) + 1)\n",
    "numClasses = int(numClasses)\n",
    "\n",
    "y_train = helper.to_onehot(y_train, numClasses)\n",
    "y_test = helper.to_onehot(y_test, numClasses)\n",
    "y_val = helper.to_onehot(y_val, numClasses)\n",
    "\n",
    "dataDimension = x_train.shape[1]\n",
    "numClasses = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nadit\\anaconda3\\envs\\ProtoNN\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py:2825: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "Epoch:   0 Batch:   0 Loss: 32765.28516 Accuracy: 0.49447\n",
      "Epoch:   1 Batch:   0 Loss: 32096.01172 Accuracy: 0.49447\n",
      "Epoch:   2 Batch:   0 Loss: 29436.13086 Accuracy: 0.49447\n",
      "Epoch:   3 Batch:   0 Loss: 26945.33594 Accuracy: 0.49447\n",
      "Epoch:   4 Batch:   0 Loss: 24635.99805 Accuracy: 0.49447\n",
      "Epoch:   5 Batch:   0 Loss: 22509.29883 Accuracy: 0.49447\n",
      "Epoch:   6 Batch:   0 Loss: 20560.07227 Accuracy: 0.49447\n",
      "Epoch:   7 Batch:   0 Loss: 18779.18945 Accuracy: 0.49447\n",
      "Epoch:   8 Batch:   0 Loss: 17155.16406 Accuracy: 0.49447\n",
      "Epoch:   9 Batch:   0 Loss: 15675.44434 Accuracy: 0.49447\n",
      "Test Loss: 13436.28101 Accuracy: 0.50000\n",
      "Epoch:  10 Batch:   0 Loss: 14327.37305 Accuracy: 0.49447\n",
      "Epoch:  11 Batch:   0 Loss: 13098.72559 Accuracy: 0.49447\n",
      "Epoch:  12 Batch:   0 Loss: 11978.18359 Accuracy: 0.49447\n",
      "Epoch:  13 Batch:   0 Loss: 10955.44922 Accuracy: 0.49447\n",
      "Epoch:  14 Batch:   0 Loss: 10021.30566 Accuracy: 0.49447\n",
      "Epoch:  15 Batch:   0 Loss: 9167.54590 Accuracy: 0.49447\n",
      "Epoch:  16 Batch:   0 Loss: 8386.91699 Accuracy: 0.49447\n",
      "Epoch:  17 Batch:   0 Loss: 7672.95703 Accuracy: 0.49447\n",
      "Epoch:  18 Batch:   0 Loss: 7019.91211 Accuracy: 0.49447\n",
      "Epoch:  19 Batch:   0 Loss: 6422.61279 Accuracy: 0.49447\n",
      "Test Loss: 5414.53705 Accuracy: 0.50000\n",
      "Epoch:  20 Batch:   0 Loss: 5876.39111 Accuracy: 0.49447\n",
      "Epoch:  21 Batch:   0 Loss: 5377.01367 Accuracy: 0.49447\n",
      "Epoch:  22 Batch:   0 Loss: 4920.61621 Accuracy: 0.49447\n",
      "Epoch:  23 Batch:   0 Loss: 4503.67529 Accuracy: 0.49447\n",
      "Epoch:  24 Batch:   0 Loss: 4122.95264 Accuracy: 0.49447\n",
      "Epoch:  25 Batch:   0 Loss: 3775.47559 Accuracy: 0.49447\n",
      "Epoch:  26 Batch:   0 Loss: 3458.51440 Accuracy: 0.49447\n",
      "Epoch:  27 Batch:   0 Loss: 3169.55981 Accuracy: 0.49447\n",
      "Epoch:  28 Batch:   0 Loss: 2906.31030 Accuracy: 0.49447\n",
      "Epoch:  29 Batch:   0 Loss: 2666.64160 Accuracy: 0.49447\n",
      "Test Loss: 2252.31848 Accuracy: 0.50000\n",
      "Epoch:  30 Batch:   0 Loss: 2448.59888 Accuracy: 0.49447\n",
      "Epoch:  31 Batch:   0 Loss: 2250.38770 Accuracy: 0.49447\n",
      "Epoch:  32 Batch:   0 Loss: 2070.35376 Accuracy: 0.49447\n",
      "Epoch:  33 Batch:   0 Loss: 1906.97534 Accuracy: 0.49447\n",
      "Epoch:  34 Batch:   0 Loss: 1758.83435 Accuracy: 0.49447\n",
      "Epoch:  35 Batch:   0 Loss: 1624.66455 Accuracy: 0.49447\n",
      "Epoch:  36 Batch:   0 Loss: 1503.25659 Accuracy: 0.49447\n",
      "Epoch:  37 Batch:   0 Loss: 1393.53125 Accuracy: 0.49447\n",
      "Epoch:  38 Batch:   0 Loss: 1294.46643 Accuracy: 0.49447\n",
      "Epoch:  39 Batch:   0 Loss: 1205.12366 Accuracy: 0.49447\n",
      "Test Loss: 1053.58384 Accuracy: 0.50000\n",
      "Epoch:  40 Batch:   0 Loss: 1124.64954 Accuracy: 0.49447\n",
      "Epoch:  41 Batch:   0 Loss: 1052.25049 Accuracy: 0.49447\n",
      "Epoch:  42 Batch:   0 Loss: 987.21082 Accuracy: 0.49447\n",
      "Epoch:  43 Batch:   0 Loss: 928.84698 Accuracy: 0.49447\n",
      "Epoch:  44 Batch:   0 Loss: 876.54242 Accuracy: 0.49447\n",
      "Epoch:  45 Batch:   0 Loss: 829.72949 Accuracy: 0.49447\n",
      "Epoch:  46 Batch:   0 Loss: 787.88666 Accuracy: 0.49447\n",
      "Epoch:  47 Batch:   0 Loss: 750.52588 Accuracy: 0.49447\n",
      "Epoch:  48 Batch:   0 Loss: 717.21722 Accuracy: 0.49447\n",
      "Epoch:  49 Batch:   0 Loss: 687.55682 Accuracy: 0.49447\n",
      "Test Loss: 636.14946 Accuracy: 0.50000\n",
      "Epoch:  50 Batch:   0 Loss: 661.18872 Accuracy: 0.49447\n",
      "Epoch:  51 Batch:   0 Loss: 637.77112 Accuracy: 0.49447\n",
      "Epoch:  52 Batch:   0 Loss: 616.99939 Accuracy: 0.49447\n",
      "Epoch:  53 Batch:   0 Loss: 598.57904 Accuracy: 0.49447\n",
      "Epoch:  54 Batch:   0 Loss: 582.28308 Accuracy: 0.49447\n",
      "Epoch:  55 Batch:   0 Loss: 567.88000 Accuracy: 0.49447\n",
      "Epoch:  56 Batch:   0 Loss: 555.15417 Accuracy: 0.49447\n",
      "Epoch:  57 Batch:   0 Loss: 543.92584 Accuracy: 0.49447\n",
      "Epoch:  58 Batch:   0 Loss: 534.02319 Accuracy: 0.49447\n",
      "Epoch:  59 Batch:   0 Loss: 525.29480 Accuracy: 0.49447\n",
      "Test Loss: 506.85323 Accuracy: 0.50244\n",
      "Epoch:  60 Batch:   0 Loss: 517.60431 Accuracy: 0.49605\n",
      "Epoch:  61 Batch:   0 Loss: 510.83014 Accuracy: 0.50342\n",
      "Epoch:  62 Batch:   0 Loss: 504.86282 Accuracy: 0.51290\n",
      "Epoch:  63 Batch:   0 Loss: 499.60547 Accuracy: 0.53081\n",
      "Epoch:  64 Batch:   0 Loss: 494.96817 Accuracy: 0.55134\n",
      "Epoch:  65 Batch:   0 Loss: 490.87851 Accuracy: 0.56977\n",
      "Epoch:  66 Batch:   0 Loss: 487.26627 Accuracy: 0.58452\n",
      "Epoch:  67 Batch:   0 Loss: 484.07092 Accuracy: 0.59979\n",
      "Epoch:  68 Batch:   0 Loss: 481.23901 Accuracy: 0.61032\n",
      "Epoch:  69 Batch:   0 Loss: 478.71362 Accuracy: 0.61611\n",
      "Test Loss: 470.25954 Accuracy: 0.64228\n",
      "Epoch:  70 Batch:   0 Loss: 476.46237 Accuracy: 0.62138\n",
      "Epoch:  71 Batch:   0 Loss: 474.44901 Accuracy: 0.63402\n",
      "Epoch:  72 Batch:   0 Loss: 472.64209 Accuracy: 0.64876\n",
      "Epoch:  73 Batch:   0 Loss: 471.01566 Accuracy: 0.65824\n",
      "Epoch:  74 Batch:   0 Loss: 469.53763 Accuracy: 0.66825\n",
      "Epoch:  75 Batch:   0 Loss: 468.18762 Accuracy: 0.67983\n",
      "Epoch:  76 Batch:   0 Loss: 466.96255 Accuracy: 0.69352\n",
      "Epoch:  77 Batch:   0 Loss: 465.82925 Accuracy: 0.69668\n",
      "Epoch:  78 Batch:   0 Loss: 464.78223 Accuracy: 0.69826\n",
      "Epoch:  79 Batch:   0 Loss: 463.80557 Accuracy: 0.70248\n",
      "Test Loss: 459.03217 Accuracy: 0.65623\n",
      "Epoch:  80 Batch:   0 Loss: 462.88608 Accuracy: 0.70511\n",
      "Epoch:  81 Batch:   0 Loss: 462.02640 Accuracy: 0.71037\n",
      "Epoch:  82 Batch:   0 Loss: 461.21506 Accuracy: 0.71353\n",
      "Epoch:  83 Batch:   0 Loss: 460.44254 Accuracy: 0.71617\n",
      "Epoch:  84 Batch:   0 Loss: 459.70334 Accuracy: 0.71775\n",
      "Epoch:  85 Batch:   0 Loss: 458.99258 Accuracy: 0.71669\n",
      "Epoch:  86 Batch:   0 Loss: 458.30609 Accuracy: 0.71722\n",
      "Epoch:  87 Batch:   0 Loss: 457.64023 Accuracy: 0.71722\n",
      "Epoch:  88 Batch:   0 Loss: 456.98813 Accuracy: 0.71722\n",
      "Epoch:  89 Batch:   0 Loss: 456.34451 Accuracy: 0.71775\n",
      "Test Loss: 453.89658 Accuracy: 0.65678\n",
      "Epoch:  90 Batch:   0 Loss: 455.72885 Accuracy: 0.71827\n",
      "Epoch:  91 Batch:   0 Loss: 455.12442 Accuracy: 0.71827\n",
      "Epoch:  92 Batch:   0 Loss: 454.52942 Accuracy: 0.71775\n",
      "Epoch:  93 Batch:   0 Loss: 453.94250 Accuracy: 0.71827\n",
      "Epoch:  94 Batch:   0 Loss: 453.35287 Accuracy: 0.71985\n",
      "Epoch:  95 Batch:   0 Loss: 452.77576 Accuracy: 0.71880\n",
      "Epoch:  96 Batch:   0 Loss: 452.20087 Accuracy: 0.71933\n",
      "Epoch:  97 Batch:   0 Loss: 451.63242 Accuracy: 0.71933\n",
      "Epoch:  98 Batch:   0 Loss: 451.06442 Accuracy: 0.71933\n",
      "Epoch:  99 Batch:   0 Loss: 450.50320 Accuracy: 0.71775\n",
      "Test Loss: 450.13657 Accuracy: 0.65569\n",
      "Epoch: 100 Batch:   0 Loss: 449.94479 Accuracy: 0.71722\n",
      "Epoch: 101 Batch:   0 Loss: 449.38892 Accuracy: 0.71775\n",
      "Epoch: 102 Batch:   0 Loss: 448.83505 Accuracy: 0.71669\n",
      "Epoch: 103 Batch:   0 Loss: 448.28326 Accuracy: 0.71617\n",
      "Epoch: 104 Batch:   0 Loss: 447.73291 Accuracy: 0.71617\n",
      "Epoch: 105 Batch:   0 Loss: 447.18399 Accuracy: 0.71459\n",
      "Epoch: 106 Batch:   0 Loss: 446.63626 Accuracy: 0.71301\n",
      "Epoch: 107 Batch:   0 Loss: 446.08942 Accuracy: 0.71195\n",
      "Epoch: 108 Batch:   0 Loss: 445.54330 Accuracy: 0.71195\n",
      "Epoch: 109 Batch:   0 Loss: 444.99808 Accuracy: 0.71195\n",
      "Test Loss: 446.72890 Accuracy: 0.65379\n",
      "Epoch: 110 Batch:   0 Loss: 444.43915 Accuracy: 0.71090\n",
      "Epoch: 111 Batch:   0 Loss: 443.89032 Accuracy: 0.71090\n",
      "Epoch: 112 Batch:   0 Loss: 443.33594 Accuracy: 0.71195\n",
      "Epoch: 113 Batch:   0 Loss: 442.78070 Accuracy: 0.71195\n",
      "Epoch: 114 Batch:   0 Loss: 442.22473 Accuracy: 0.71248\n",
      "Epoch: 115 Batch:   0 Loss: 441.67160 Accuracy: 0.71353\n",
      "Epoch: 116 Batch:   0 Loss: 441.12790 Accuracy: 0.71195\n",
      "Epoch: 117 Batch:   0 Loss: 440.57388 Accuracy: 0.71248\n",
      "Epoch: 118 Batch:   0 Loss: 440.02032 Accuracy: 0.71143\n",
      "Epoch: 119 Batch:   0 Loss: 439.46335 Accuracy: 0.71090\n",
      "Test Loss: 443.24573 Accuracy: 0.65163\n",
      "Epoch: 120 Batch:   0 Loss: 438.91397 Accuracy: 0.71090\n",
      "Epoch: 121 Batch:   0 Loss: 438.36377 Accuracy: 0.71037\n",
      "Epoch: 122 Batch:   0 Loss: 437.81290 Accuracy: 0.71090\n",
      "Epoch: 123 Batch:   0 Loss: 437.26120 Accuracy: 0.71195\n",
      "Epoch: 124 Batch:   0 Loss: 436.70377 Accuracy: 0.71195\n",
      "Epoch: 125 Batch:   0 Loss: 436.14862 Accuracy: 0.71143\n",
      "Epoch: 126 Batch:   0 Loss: 435.59235 Accuracy: 0.71143\n",
      "Epoch: 127 Batch:   0 Loss: 435.03510 Accuracy: 0.71248\n",
      "Epoch: 128 Batch:   0 Loss: 434.47092 Accuracy: 0.71301\n",
      "Epoch: 129 Batch:   0 Loss: 433.90884 Accuracy: 0.71353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 439.69151 Accuracy: 0.65014\n",
      "Epoch: 130 Batch:   0 Loss: 433.34543 Accuracy: 0.71248\n",
      "Epoch: 131 Batch:   0 Loss: 432.78049 Accuracy: 0.71301\n",
      "Epoch: 132 Batch:   0 Loss: 432.21405 Accuracy: 0.71143\n",
      "Epoch: 133 Batch:   0 Loss: 431.64600 Accuracy: 0.71090\n",
      "Epoch: 134 Batch:   0 Loss: 431.06546 Accuracy: 0.71143\n",
      "Epoch: 135 Batch:   0 Loss: 430.50153 Accuracy: 0.71195\n",
      "Epoch: 136 Batch:   0 Loss: 429.93594 Accuracy: 0.71143\n",
      "Epoch: 137 Batch:   0 Loss: 429.35843 Accuracy: 0.71195\n",
      "Epoch: 138 Batch:   0 Loss: 428.77936 Accuracy: 0.71353\n",
      "Epoch: 139 Batch:   0 Loss: 428.20099 Accuracy: 0.71353\n",
      "Test Loss: 436.00726 Accuracy: 0.65027\n",
      "Epoch: 140 Batch:   0 Loss: 427.61618 Accuracy: 0.71301\n",
      "Epoch: 141 Batch:   0 Loss: 427.02643 Accuracy: 0.71301\n",
      "Epoch: 142 Batch:   0 Loss: 426.44025 Accuracy: 0.71301\n",
      "Epoch: 143 Batch:   0 Loss: 425.85196 Accuracy: 0.71301\n",
      "Epoch: 144 Batch:   0 Loss: 425.26089 Accuracy: 0.71248\n",
      "Epoch: 145 Batch:   0 Loss: 424.66754 Accuracy: 0.71248\n",
      "Epoch: 146 Batch:   0 Loss: 424.07162 Accuracy: 0.71248\n",
      "Epoch: 147 Batch:   0 Loss: 423.47299 Accuracy: 0.71248\n",
      "Epoch: 148 Batch:   0 Loss: 422.86923 Accuracy: 0.71195\n",
      "Epoch: 149 Batch:   0 Loss: 422.26602 Accuracy: 0.71195\n",
      "Test Loss: 432.15549 Accuracy: 0.65149\n",
      "Epoch: 150 Batch:   0 Loss: 421.65878 Accuracy: 0.71195\n",
      "Epoch: 151 Batch:   0 Loss: 421.05032 Accuracy: 0.71195\n",
      "Epoch: 152 Batch:   0 Loss: 420.43903 Accuracy: 0.71195\n",
      "Epoch: 153 Batch:   0 Loss: 419.82474 Accuracy: 0.71195\n",
      "Epoch: 154 Batch:   0 Loss: 419.20712 Accuracy: 0.71248\n",
      "Epoch: 155 Batch:   0 Loss: 418.58636 Accuracy: 0.71353\n",
      "Epoch: 156 Batch:   0 Loss: 417.96222 Accuracy: 0.71459\n",
      "Epoch: 157 Batch:   0 Loss: 417.33496 Accuracy: 0.71459\n",
      "Epoch: 158 Batch:   0 Loss: 416.70349 Accuracy: 0.71459\n",
      "Epoch: 159 Batch:   0 Loss: 416.06885 Accuracy: 0.71511\n",
      "Test Loss: 428.09917 Accuracy: 0.65244\n",
      "Epoch: 160 Batch:   0 Loss: 415.43091 Accuracy: 0.71459\n",
      "Epoch: 161 Batch:   0 Loss: 414.78766 Accuracy: 0.71406\n",
      "Epoch: 162 Batch:   0 Loss: 414.14484 Accuracy: 0.71459\n",
      "Epoch: 163 Batch:   0 Loss: 413.49802 Accuracy: 0.71459\n",
      "Epoch: 164 Batch:   0 Loss: 412.84564 Accuracy: 0.71511\n",
      "Epoch: 165 Batch:   0 Loss: 412.19012 Accuracy: 0.71564\n",
      "Epoch: 166 Batch:   0 Loss: 411.52689 Accuracy: 0.71564\n",
      "Epoch: 167 Batch:   0 Loss: 410.86038 Accuracy: 0.71617\n",
      "Epoch: 168 Batch:   0 Loss: 410.18408 Accuracy: 0.71617\n",
      "Epoch: 169 Batch:   0 Loss: 409.50848 Accuracy: 0.71617\n",
      "Test Loss: 423.77330 Accuracy: 0.65352\n",
      "Epoch: 170 Batch:   0 Loss: 408.82883 Accuracy: 0.71617\n",
      "Epoch: 171 Batch:   0 Loss: 408.14471 Accuracy: 0.71617\n",
      "Epoch: 172 Batch:   0 Loss: 407.45291 Accuracy: 0.71722\n",
      "Epoch: 173 Batch:   0 Loss: 406.75711 Accuracy: 0.71775\n",
      "Epoch: 174 Batch:   0 Loss: 406.06079 Accuracy: 0.71775\n",
      "Epoch: 175 Batch:   0 Loss: 405.35983 Accuracy: 0.71827\n",
      "Epoch: 176 Batch:   0 Loss: 404.65424 Accuracy: 0.71827\n",
      "Epoch: 177 Batch:   0 Loss: 403.94406 Accuracy: 0.71827\n",
      "Epoch: 178 Batch:   0 Loss: 403.22891 Accuracy: 0.71827\n",
      "Epoch: 179 Batch:   0 Loss: 402.50906 Accuracy: 0.71827\n",
      "Test Loss: 419.10249 Accuracy: 0.65542\n",
      "Epoch: 180 Batch:   0 Loss: 401.78433 Accuracy: 0.71827\n",
      "Epoch: 181 Batch:   0 Loss: 401.05472 Accuracy: 0.71827\n",
      "Epoch: 182 Batch:   0 Loss: 400.31656 Accuracy: 0.71827\n",
      "Epoch: 183 Batch:   0 Loss: 399.57526 Accuracy: 0.72038\n",
      "Epoch: 184 Batch:   0 Loss: 398.82880 Accuracy: 0.72091\n",
      "Epoch: 185 Batch:   0 Loss: 398.07715 Accuracy: 0.72091\n",
      "Epoch: 186 Batch:   0 Loss: 397.32025 Accuracy: 0.72143\n",
      "Epoch: 187 Batch:   0 Loss: 396.55804 Accuracy: 0.72143\n",
      "Epoch: 188 Batch:   0 Loss: 395.79041 Accuracy: 0.72196\n",
      "Epoch: 189 Batch:   0 Loss: 395.01740 Accuracy: 0.72301\n",
      "Test Loss: 414.04841 Accuracy: 0.65759\n",
      "Epoch: 190 Batch:   0 Loss: 394.23895 Accuracy: 0.72301\n",
      "Epoch: 191 Batch:   0 Loss: 393.45486 Accuracy: 0.72354\n",
      "Epoch: 192 Batch:   0 Loss: 392.66537 Accuracy: 0.72354\n",
      "Epoch: 193 Batch:   0 Loss: 391.87012 Accuracy: 0.72354\n",
      "Epoch: 194 Batch:   0 Loss: 391.06924 Accuracy: 0.72512\n",
      "Epoch: 195 Batch:   0 Loss: 390.26270 Accuracy: 0.72565\n",
      "Epoch: 196 Batch:   0 Loss: 389.45038 Accuracy: 0.72670\n",
      "Epoch: 197 Batch:   0 Loss: 388.62845 Accuracy: 0.72670\n",
      "Epoch: 198 Batch:   0 Loss: 387.80164 Accuracy: 0.72722\n",
      "Epoch: 199 Batch:   0 Loss: 386.96887 Accuracy: 0.72722\n",
      "Test Loss: 408.54330 Accuracy: 0.66003\n",
      "Epoch: 200 Batch:   0 Loss: 386.13031 Accuracy: 0.72775\n",
      "Epoch: 201 Batch:   0 Loss: 385.28561 Accuracy: 0.72775\n",
      "Epoch: 202 Batch:   0 Loss: 384.43478 Accuracy: 0.72828\n",
      "Epoch: 203 Batch:   0 Loss: 383.57806 Accuracy: 0.72933\n",
      "Epoch: 204 Batch:   0 Loss: 382.71533 Accuracy: 0.73091\n",
      "Epoch: 205 Batch:   0 Loss: 381.84628 Accuracy: 0.73196\n",
      "Epoch: 206 Batch:   0 Loss: 380.97110 Accuracy: 0.73302\n",
      "Epoch: 207 Batch:   0 Loss: 380.08575 Accuracy: 0.73354\n",
      "Epoch: 208 Batch:   0 Loss: 379.19617 Accuracy: 0.73460\n",
      "Epoch: 209 Batch:   0 Loss: 378.30032 Accuracy: 0.73512\n",
      "Test Loss: 402.53535 Accuracy: 0.66098\n",
      "Epoch: 210 Batch:   0 Loss: 377.39828 Accuracy: 0.73618\n",
      "Epoch: 211 Batch:   0 Loss: 376.49005 Accuracy: 0.73723\n",
      "Epoch: 212 Batch:   0 Loss: 375.57538 Accuracy: 0.73723\n",
      "Epoch: 213 Batch:   0 Loss: 374.65466 Accuracy: 0.73776\n",
      "Epoch: 214 Batch:   0 Loss: 373.72763 Accuracy: 0.73881\n",
      "Epoch: 215 Batch:   0 Loss: 372.79413 Accuracy: 0.73986\n",
      "Epoch: 216 Batch:   0 Loss: 371.85464 Accuracy: 0.74197\n",
      "Epoch: 217 Batch:   0 Loss: 370.90875 Accuracy: 0.74250\n",
      "Epoch: 218 Batch:   0 Loss: 369.95657 Accuracy: 0.74408\n",
      "Epoch: 219 Batch:   0 Loss: 368.99814 Accuracy: 0.74460\n",
      "Test Loss: 396.02651 Accuracy: 0.66355\n",
      "Epoch: 220 Batch:   0 Loss: 368.03354 Accuracy: 0.74513\n",
      "Epoch: 221 Batch:   0 Loss: 367.06256 Accuracy: 0.74618\n",
      "Epoch: 222 Batch:   0 Loss: 366.08545 Accuracy: 0.74776\n",
      "Epoch: 223 Batch:   0 Loss: 365.10223 Accuracy: 0.74934\n",
      "Epoch: 224 Batch:   0 Loss: 364.11029 Accuracy: 0.75092\n",
      "Epoch: 225 Batch:   0 Loss: 363.11401 Accuracy: 0.75303\n",
      "Epoch: 226 Batch:   0 Loss: 362.11160 Accuracy: 0.75355\n",
      "Epoch: 227 Batch:   0 Loss: 361.10300 Accuracy: 0.75566\n",
      "Epoch: 228 Batch:   0 Loss: 360.08838 Accuracy: 0.75566\n",
      "Epoch: 229 Batch:   0 Loss: 359.06778 Accuracy: 0.75461\n",
      "Test Loss: 389.02393 Accuracy: 0.66640\n",
      "Epoch: 230 Batch:   0 Loss: 358.04117 Accuracy: 0.75513\n",
      "Epoch: 231 Batch:   0 Loss: 357.00861 Accuracy: 0.75566\n",
      "Epoch: 232 Batch:   0 Loss: 355.97009 Accuracy: 0.75619\n",
      "Epoch: 233 Batch:   0 Loss: 354.92590 Accuracy: 0.75619\n",
      "Epoch: 234 Batch:   0 Loss: 353.87570 Accuracy: 0.75566\n",
      "Epoch: 235 Batch:   0 Loss: 352.81979 Accuracy: 0.75566\n",
      "Epoch: 236 Batch:   0 Loss: 351.75839 Accuracy: 0.75619\n",
      "Epoch: 237 Batch:   0 Loss: 350.69122 Accuracy: 0.75829\n",
      "Epoch: 238 Batch:   0 Loss: 349.61859 Accuracy: 0.75935\n",
      "Epoch: 239 Batch:   0 Loss: 348.54047 Accuracy: 0.76040\n",
      "Test Loss: 381.55228 Accuracy: 0.67046\n",
      "Epoch: 240 Batch:   0 Loss: 347.45685 Accuracy: 0.76461\n",
      "Epoch: 241 Batch:   0 Loss: 346.36792 Accuracy: 0.76619\n",
      "Epoch: 242 Batch:   0 Loss: 345.27393 Accuracy: 0.76830\n",
      "Epoch: 243 Batch:   0 Loss: 344.17450 Accuracy: 0.77041\n",
      "Epoch: 244 Batch:   0 Loss: 343.07004 Accuracy: 0.77304\n",
      "Epoch: 245 Batch:   0 Loss: 341.96057 Accuracy: 0.77514\n",
      "Epoch: 246 Batch:   0 Loss: 340.84631 Accuracy: 0.77672\n",
      "Epoch: 247 Batch:   0 Loss: 339.72714 Accuracy: 0.77778\n",
      "Epoch: 248 Batch:   0 Loss: 338.60321 Accuracy: 0.77778\n",
      "Epoch: 249 Batch:   0 Loss: 337.47458 Accuracy: 0.77988\n",
      "Test Loss: 373.66788 Accuracy: 0.67547\n",
      "Epoch: 250 Batch:   0 Loss: 336.34146 Accuracy: 0.78146\n",
      "Epoch: 251 Batch:   0 Loss: 335.20389 Accuracy: 0.78357\n",
      "Epoch: 252 Batch:   0 Loss: 334.06198 Accuracy: 0.78620\n",
      "Epoch: 253 Batch:   0 Loss: 332.91574 Accuracy: 0.78620\n",
      "Epoch: 254 Batch:   0 Loss: 331.76550 Accuracy: 0.78831\n",
      "Epoch: 255 Batch:   0 Loss: 330.61111 Accuracy: 0.78884\n",
      "Epoch: 256 Batch:   0 Loss: 329.45279 Accuracy: 0.79094\n",
      "Epoch: 257 Batch:   0 Loss: 328.29077 Accuracy: 0.79358\n",
      "Epoch: 258 Batch:   0 Loss: 327.12497 Accuracy: 0.79463\n",
      "Epoch: 259 Batch:   0 Loss: 325.95572 Accuracy: 0.79568\n",
      "Test Loss: 365.47092 Accuracy: 0.68279\n",
      "Epoch: 260 Batch:   0 Loss: 324.78293 Accuracy: 0.79621\n",
      "Epoch: 261 Batch:   0 Loss: 323.60672 Accuracy: 0.79779\n",
      "Epoch: 262 Batch:   0 Loss: 322.42749 Accuracy: 0.79937\n",
      "Epoch: 263 Batch:   0 Loss: 321.24493 Accuracy: 0.79989\n",
      "Epoch: 264 Batch:   0 Loss: 320.05951 Accuracy: 0.80200\n",
      "Epoch: 265 Batch:   0 Loss: 318.87128 Accuracy: 0.80463\n",
      "Epoch: 266 Batch:   0 Loss: 317.68027 Accuracy: 0.80621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 267 Batch:   0 Loss: 316.48669 Accuracy: 0.80621\n",
      "Epoch: 268 Batch:   0 Loss: 315.29065 Accuracy: 0.80779\n",
      "Epoch: 269 Batch:   0 Loss: 314.08969 Accuracy: 0.80832\n",
      "Test Loss: 357.09574 Accuracy: 0.69661\n",
      "Epoch: 270 Batch:   0 Loss: 312.88812 Accuracy: 0.80937\n",
      "Epoch: 271 Batch:   0 Loss: 311.68439 Accuracy: 0.81148\n",
      "Epoch: 272 Batch:   0 Loss: 310.47885 Accuracy: 0.81253\n",
      "Epoch: 273 Batch:   0 Loss: 309.27136 Accuracy: 0.81464\n",
      "Epoch: 274 Batch:   0 Loss: 308.06223 Accuracy: 0.81464\n",
      "Epoch: 275 Batch:   0 Loss: 306.84875 Accuracy: 0.81569\n",
      "Epoch: 276 Batch:   0 Loss: 305.63635 Accuracy: 0.81833\n",
      "Epoch: 277 Batch:   0 Loss: 304.42267 Accuracy: 0.81885\n",
      "Epoch: 278 Batch:   0 Loss: 303.20776 Accuracy: 0.82043\n",
      "Epoch: 279 Batch:   0 Loss: 301.99179 Accuracy: 0.82148\n",
      "Test Loss: 348.70241 Accuracy: 0.71138\n",
      "Epoch: 280 Batch:   0 Loss: 300.77136 Accuracy: 0.82201\n",
      "Epoch: 281 Batch:   0 Loss: 299.55084 Accuracy: 0.82359\n",
      "Epoch: 282 Batch:   0 Loss: 298.32980 Accuracy: 0.82675\n",
      "Epoch: 283 Batch:   0 Loss: 297.10828 Accuracy: 0.82886\n",
      "Epoch: 284 Batch:   0 Loss: 295.88620 Accuracy: 0.83149\n",
      "Epoch: 285 Batch:   0 Loss: 294.66400 Accuracy: 0.83254\n",
      "Epoch: 286 Batch:   0 Loss: 293.44165 Accuracy: 0.83623\n",
      "Epoch: 287 Batch:   0 Loss: 292.21933 Accuracy: 0.83834\n",
      "Epoch: 288 Batch:   0 Loss: 290.99713 Accuracy: 0.84150\n",
      "Epoch: 289 Batch:   0 Loss: 289.77527 Accuracy: 0.84150\n",
      "Test Loss: 340.43481 Accuracy: 0.72954\n",
      "Epoch: 290 Batch:   0 Loss: 288.55386 Accuracy: 0.84360\n",
      "Epoch: 291 Batch:   0 Loss: 287.33295 Accuracy: 0.84518\n",
      "Epoch: 292 Batch:   0 Loss: 286.11276 Accuracy: 0.84887\n",
      "Epoch: 293 Batch:   0 Loss: 284.89340 Accuracy: 0.84939\n",
      "Epoch: 294 Batch:   0 Loss: 283.67499 Accuracy: 0.85150\n",
      "Epoch: 295 Batch:   0 Loss: 282.45764 Accuracy: 0.85413\n",
      "Epoch: 296 Batch:   0 Loss: 281.24152 Accuracy: 0.85624\n",
      "Epoch: 297 Batch:   0 Loss: 280.02673 Accuracy: 0.85835\n",
      "Epoch: 298 Batch:   0 Loss: 278.81345 Accuracy: 0.85835\n",
      "Epoch: 299 Batch:   0 Loss: 277.60181 Accuracy: 0.86045\n",
      "Test Loss: 332.48799 Accuracy: 0.74661\n",
      "Epoch: 300 Batch:   0 Loss: 276.39191 Accuracy: 0.86151\n",
      "Epoch: 301 Batch:   0 Loss: 275.18387 Accuracy: 0.86256\n",
      "Epoch: 302 Batch:   0 Loss: 273.97772 Accuracy: 0.86361\n",
      "Epoch: 303 Batch:   0 Loss: 272.77368 Accuracy: 0.86519\n",
      "Epoch: 304 Batch:   0 Loss: 271.57193 Accuracy: 0.86572\n",
      "Epoch: 305 Batch:   0 Loss: 270.37244 Accuracy: 0.86783\n",
      "Epoch: 306 Batch:   0 Loss: 269.17545 Accuracy: 0.86888\n",
      "Epoch: 307 Batch:   0 Loss: 267.98108 Accuracy: 0.87151\n",
      "Epoch: 308 Batch:   0 Loss: 266.78934 Accuracy: 0.87309\n",
      "Epoch: 309 Batch:   0 Loss: 265.60046 Accuracy: 0.87414\n",
      "Test Loss: 325.01380 Accuracy: 0.75908\n",
      "Epoch: 310 Batch:   0 Loss: 264.41443 Accuracy: 0.87520\n",
      "Epoch: 311 Batch:   0 Loss: 263.23141 Accuracy: 0.87520\n",
      "Epoch: 312 Batch:   0 Loss: 262.05164 Accuracy: 0.87783\n",
      "Epoch: 313 Batch:   0 Loss: 260.87509 Accuracy: 0.87783\n",
      "Epoch: 314 Batch:   0 Loss: 259.70181 Accuracy: 0.87888\n",
      "Epoch: 315 Batch:   0 Loss: 258.53204 Accuracy: 0.88152\n",
      "Epoch: 316 Batch:   0 Loss: 257.36581 Accuracy: 0.88257\n",
      "Epoch: 317 Batch:   0 Loss: 256.20325 Accuracy: 0.88257\n",
      "Epoch: 318 Batch:   0 Loss: 255.04437 Accuracy: 0.88310\n",
      "Epoch: 319 Batch:   0 Loss: 253.88947 Accuracy: 0.88415\n",
      "Test Loss: 318.13638 Accuracy: 0.76870\n",
      "Epoch: 320 Batch:   0 Loss: 252.73849 Accuracy: 0.88415\n",
      "Epoch: 321 Batch:   0 Loss: 251.59148 Accuracy: 0.88573\n",
      "Epoch: 322 Batch:   0 Loss: 250.44862 Accuracy: 0.88731\n",
      "Epoch: 323 Batch:   0 Loss: 249.31006 Accuracy: 0.88678\n",
      "Epoch: 324 Batch:   0 Loss: 248.17279 Accuracy: 0.88889\n",
      "Epoch: 325 Batch:   0 Loss: 247.04089 Accuracy: 0.88994\n",
      "Epoch: 326 Batch:   0 Loss: 245.91339 Accuracy: 0.89047\n",
      "Epoch: 327 Batch:   0 Loss: 244.79057 Accuracy: 0.89205\n",
      "Epoch: 328 Batch:   0 Loss: 243.67238 Accuracy: 0.89310\n",
      "Epoch: 329 Batch:   0 Loss: 242.55888 Accuracy: 0.89258\n",
      "Test Loss: 311.94548 Accuracy: 0.77656\n",
      "Epoch: 330 Batch:   0 Loss: 241.45030 Accuracy: 0.89310\n",
      "Epoch: 331 Batch:   0 Loss: 240.34651 Accuracy: 0.89363\n",
      "Epoch: 332 Batch:   0 Loss: 239.24763 Accuracy: 0.89415\n",
      "Epoch: 333 Batch:   0 Loss: 238.15388 Accuracy: 0.89468\n",
      "Epoch: 334 Batch:   0 Loss: 237.06509 Accuracy: 0.89679\n",
      "Epoch: 335 Batch:   0 Loss: 235.98152 Accuracy: 0.89837\n",
      "Epoch: 336 Batch:   0 Loss: 234.90317 Accuracy: 0.89837\n",
      "Epoch: 337 Batch:   0 Loss: 233.83011 Accuracy: 0.89942\n",
      "Epoch: 338 Batch:   0 Loss: 232.76247 Accuracy: 0.89995\n",
      "Epoch: 339 Batch:   0 Loss: 231.70006 Accuracy: 0.90100\n",
      "Test Loss: 306.49853 Accuracy: 0.78320\n",
      "Epoch: 340 Batch:   0 Loss: 230.64328 Accuracy: 0.90205\n",
      "Epoch: 341 Batch:   0 Loss: 229.59206 Accuracy: 0.90258\n",
      "Epoch: 342 Batch:   0 Loss: 228.54628 Accuracy: 0.90258\n",
      "Epoch: 343 Batch:   0 Loss: 227.50621 Accuracy: 0.90258\n",
      "Epoch: 344 Batch:   0 Loss: 226.47186 Accuracy: 0.90416\n",
      "Epoch: 345 Batch:   0 Loss: 225.44318 Accuracy: 0.90521\n",
      "Epoch: 346 Batch:   0 Loss: 224.42029 Accuracy: 0.90521\n",
      "Epoch: 347 Batch:   0 Loss: 223.40326 Accuracy: 0.90574\n",
      "Epoch: 348 Batch:   0 Loss: 222.39204 Accuracy: 0.90627\n",
      "Epoch: 349 Batch:   0 Loss: 221.38676 Accuracy: 0.90627\n",
      "Test Loss: 301.81576 Accuracy: 0.78523\n",
      "Epoch: 350 Batch:   0 Loss: 220.38737 Accuracy: 0.90732\n",
      "Epoch: 351 Batch:   0 Loss: 219.39403 Accuracy: 0.90732\n",
      "Epoch: 352 Batch:   0 Loss: 218.40669 Accuracy: 0.90890\n",
      "Epoch: 353 Batch:   0 Loss: 217.42543 Accuracy: 0.90943\n",
      "Epoch: 354 Batch:   0 Loss: 216.45016 Accuracy: 0.91048\n",
      "Epoch: 355 Batch:   0 Loss: 215.48103 Accuracy: 0.91101\n",
      "Epoch: 356 Batch:   0 Loss: 214.51810 Accuracy: 0.91153\n",
      "Epoch: 357 Batch:   0 Loss: 213.56125 Accuracy: 0.91311\n",
      "Epoch: 358 Batch:   0 Loss: 212.61072 Accuracy: 0.91364\n",
      "Epoch: 359 Batch:   0 Loss: 211.66631 Accuracy: 0.91417\n",
      "Test Loss: 297.89138 Accuracy: 0.78699\n",
      "Epoch: 360 Batch:   0 Loss: 210.72815 Accuracy: 0.91469\n",
      "Epoch: 361 Batch:   0 Loss: 209.79625 Accuracy: 0.91522\n",
      "Epoch: 362 Batch:   0 Loss: 208.87057 Accuracy: 0.91522\n",
      "Epoch: 363 Batch:   0 Loss: 207.95128 Accuracy: 0.91522\n",
      "Epoch: 364 Batch:   0 Loss: 207.03825 Accuracy: 0.91627\n",
      "Epoch: 365 Batch:   0 Loss: 206.13156 Accuracy: 0.91680\n",
      "Epoch: 366 Batch:   0 Loss: 205.23125 Accuracy: 0.91838\n",
      "Epoch: 367 Batch:   0 Loss: 204.33714 Accuracy: 0.91890\n",
      "Epoch: 368 Batch:   0 Loss: 203.44948 Accuracy: 0.91838\n",
      "Epoch: 369 Batch:   0 Loss: 202.56816 Accuracy: 0.91838\n",
      "Test Loss: 294.69569 Accuracy: 0.78794\n",
      "Epoch: 370 Batch:   0 Loss: 201.69324 Accuracy: 0.91785\n",
      "Epoch: 371 Batch:   0 Loss: 200.82463 Accuracy: 0.91785\n",
      "Epoch: 372 Batch:   0 Loss: 199.96245 Accuracy: 0.91890\n",
      "Epoch: 373 Batch:   0 Loss: 199.10661 Accuracy: 0.91943\n",
      "Epoch: 374 Batch:   0 Loss: 198.25664 Accuracy: 0.91943\n",
      "Epoch: 375 Batch:   0 Loss: 197.41341 Accuracy: 0.91996\n",
      "Epoch: 376 Batch:   0 Loss: 196.57660 Accuracy: 0.92101\n",
      "Epoch: 377 Batch:   0 Loss: 195.74611 Accuracy: 0.92154\n",
      "Epoch: 378 Batch:   0 Loss: 194.91937 Accuracy: 0.92154\n",
      "Epoch: 379 Batch:   0 Loss: 194.09920 Accuracy: 0.92206\n",
      "Test Loss: 292.16943 Accuracy: 0.78794\n",
      "Epoch: 380 Batch:   0 Loss: 193.28552 Accuracy: 0.92259\n",
      "Epoch: 381 Batch:   0 Loss: 192.47812 Accuracy: 0.92206\n",
      "Epoch: 382 Batch:   0 Loss: 191.67711 Accuracy: 0.92206\n",
      "Epoch: 383 Batch:   0 Loss: 190.88240 Accuracy: 0.92312\n",
      "Epoch: 384 Batch:   0 Loss: 190.09416 Accuracy: 0.92312\n",
      "Epoch: 385 Batch:   0 Loss: 189.31218 Accuracy: 0.92312\n",
      "Epoch: 386 Batch:   0 Loss: 188.53653 Accuracy: 0.92364\n",
      "Epoch: 387 Batch:   0 Loss: 187.76718 Accuracy: 0.92364\n",
      "Epoch: 388 Batch:   0 Loss: 187.00417 Accuracy: 0.92364\n",
      "Epoch: 389 Batch:   0 Loss: 186.24745 Accuracy: 0.92470\n",
      "Test Loss: 290.25387 Accuracy: 0.78916\n",
      "Epoch: 390 Batch:   0 Loss: 185.49696 Accuracy: 0.92470\n",
      "Epoch: 391 Batch:   0 Loss: 184.75276 Accuracy: 0.92522\n",
      "Epoch: 392 Batch:   0 Loss: 184.01471 Accuracy: 0.92522\n",
      "Epoch: 393 Batch:   0 Loss: 183.28297 Accuracy: 0.92522\n",
      "Epoch: 394 Batch:   0 Loss: 182.55736 Accuracy: 0.92575\n",
      "Epoch: 395 Batch:   0 Loss: 181.83794 Accuracy: 0.92575\n",
      "Epoch: 396 Batch:   0 Loss: 181.12459 Accuracy: 0.92575\n",
      "Epoch: 397 Batch:   0 Loss: 180.41745 Accuracy: 0.92575\n",
      "Epoch: 398 Batch:   0 Loss: 179.71640 Accuracy: 0.92575\n",
      "Epoch: 399 Batch:   0 Loss: 179.02147 Accuracy: 0.92628\n",
      "Test Loss: 288.89815 Accuracy: 0.79079\n",
      "Epoch: 400 Batch:   0 Loss: 178.33253 Accuracy: 0.92680\n",
      "Epoch: 401 Batch:   0 Loss: 177.64969 Accuracy: 0.92733\n",
      "Epoch: 402 Batch:   0 Loss: 176.97273 Accuracy: 0.92733\n",
      "Epoch: 403 Batch:   0 Loss: 176.30183 Accuracy: 0.92838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 404 Batch:   0 Loss: 175.63692 Accuracy: 0.92786\n",
      "Epoch: 405 Batch:   0 Loss: 174.97783 Accuracy: 0.92786\n",
      "Epoch: 406 Batch:   0 Loss: 174.32475 Accuracy: 0.92786\n",
      "Epoch: 407 Batch:   0 Loss: 173.67746 Accuracy: 0.92786\n",
      "Epoch: 408 Batch:   0 Loss: 173.03603 Accuracy: 0.92786\n",
      "Epoch: 409 Batch:   0 Loss: 172.40036 Accuracy: 0.92786\n",
      "Test Loss: 288.02987 Accuracy: 0.78902\n",
      "Epoch: 410 Batch:   0 Loss: 171.77049 Accuracy: 0.92891\n",
      "Epoch: 411 Batch:   0 Loss: 171.14638 Accuracy: 0.92944\n",
      "Epoch: 412 Batch:   0 Loss: 170.52792 Accuracy: 0.92944\n",
      "Epoch: 413 Batch:   0 Loss: 169.91516 Accuracy: 0.92944\n",
      "Epoch: 414 Batch:   0 Loss: 169.30811 Accuracy: 0.92944\n",
      "Epoch: 415 Batch:   0 Loss: 168.70665 Accuracy: 0.92944\n",
      "Epoch: 416 Batch:   0 Loss: 168.11072 Accuracy: 0.92944\n",
      "Epoch: 417 Batch:   0 Loss: 167.52039 Accuracy: 0.92944\n",
      "Epoch: 418 Batch:   0 Loss: 166.93558 Accuracy: 0.92944\n",
      "Epoch: 419 Batch:   0 Loss: 166.35619 Accuracy: 0.92996\n",
      "Test Loss: 287.57638 Accuracy: 0.78767\n",
      "Epoch: 420 Batch:   0 Loss: 165.78227 Accuracy: 0.92996\n",
      "Epoch: 421 Batch:   0 Loss: 165.21376 Accuracy: 0.92996\n",
      "Epoch: 422 Batch:   0 Loss: 164.65063 Accuracy: 0.92996\n",
      "Epoch: 423 Batch:   0 Loss: 164.09285 Accuracy: 0.93049\n",
      "Epoch: 424 Batch:   0 Loss: 163.54033 Accuracy: 0.93049\n",
      "Epoch: 425 Batch:   0 Loss: 162.99309 Accuracy: 0.93049\n",
      "Epoch: 426 Batch:   0 Loss: 162.45116 Accuracy: 0.93049\n",
      "Epoch: 427 Batch:   0 Loss: 161.91431 Accuracy: 0.93049\n",
      "Epoch: 428 Batch:   0 Loss: 161.38263 Accuracy: 0.93102\n",
      "Epoch: 429 Batch:   0 Loss: 160.85608 Accuracy: 0.93102\n",
      "Test Loss: 287.46682 Accuracy: 0.78767\n",
      "Epoch: 430 Batch:   0 Loss: 160.33466 Accuracy: 0.93102\n",
      "Epoch: 431 Batch:   0 Loss: 159.81821 Accuracy: 0.93102\n",
      "Epoch: 432 Batch:   0 Loss: 159.30681 Accuracy: 0.93154\n",
      "Epoch: 433 Batch:   0 Loss: 158.80037 Accuracy: 0.93154\n",
      "Epoch: 434 Batch:   0 Loss: 158.29886 Accuracy: 0.93102\n",
      "Epoch: 435 Batch:   0 Loss: 157.80223 Accuracy: 0.93154\n",
      "Epoch: 436 Batch:   0 Loss: 157.31047 Accuracy: 0.93154\n",
      "Epoch: 437 Batch:   0 Loss: 156.82349 Accuracy: 0.93154\n",
      "Epoch: 438 Batch:   0 Loss: 156.34134 Accuracy: 0.93207\n",
      "Epoch: 439 Batch:   0 Loss: 155.86388 Accuracy: 0.93260\n",
      "Test Loss: 287.63401 Accuracy: 0.78509\n",
      "Epoch: 440 Batch:   0 Loss: 155.39114 Accuracy: 0.93260\n",
      "Epoch: 441 Batch:   0 Loss: 154.92305 Accuracy: 0.93260\n",
      "Epoch: 442 Batch:   0 Loss: 154.45914 Accuracy: 0.93260\n",
      "Epoch: 443 Batch:   0 Loss: 153.99994 Accuracy: 0.93260\n",
      "Epoch: 444 Batch:   0 Loss: 153.54526 Accuracy: 0.93207\n",
      "Epoch: 445 Batch:   0 Loss: 153.09511 Accuracy: 0.93365\n",
      "Epoch: 446 Batch:   0 Loss: 152.64946 Accuracy: 0.93365\n",
      "Epoch: 447 Batch:   0 Loss: 152.20824 Accuracy: 0.93365\n",
      "Epoch: 448 Batch:   0 Loss: 151.77135 Accuracy: 0.93418\n",
      "Epoch: 449 Batch:   0 Loss: 151.33887 Accuracy: 0.93418\n",
      "Test Loss: 288.01263 Accuracy: 0.78252\n",
      "Epoch: 450 Batch:   0 Loss: 150.91068 Accuracy: 0.93418\n",
      "Epoch: 451 Batch:   0 Loss: 150.48680 Accuracy: 0.93418\n",
      "Epoch: 452 Batch:   0 Loss: 150.06715 Accuracy: 0.93418\n",
      "Epoch: 453 Batch:   0 Loss: 149.65167 Accuracy: 0.93418\n",
      "Epoch: 454 Batch:   0 Loss: 149.24043 Accuracy: 0.93418\n",
      "Epoch: 455 Batch:   0 Loss: 148.83325 Accuracy: 0.93418\n",
      "Epoch: 456 Batch:   0 Loss: 148.43016 Accuracy: 0.93418\n",
      "Epoch: 457 Batch:   0 Loss: 148.03113 Accuracy: 0.93418\n",
      "Epoch: 458 Batch:   0 Loss: 147.63617 Accuracy: 0.93418\n",
      "Epoch: 459 Batch:   0 Loss: 147.24510 Accuracy: 0.93418\n",
      "Test Loss: 288.54842 Accuracy: 0.78103\n",
      "Epoch: 460 Batch:   0 Loss: 146.85803 Accuracy: 0.93576\n",
      "Epoch: 461 Batch:   0 Loss: 146.47482 Accuracy: 0.93628\n",
      "Epoch: 462 Batch:   0 Loss: 146.09547 Accuracy: 0.93681\n",
      "Epoch: 463 Batch:   0 Loss: 145.71994 Accuracy: 0.93681\n",
      "Epoch: 464 Batch:   0 Loss: 145.34822 Accuracy: 0.93681\n",
      "Epoch: 465 Batch:   0 Loss: 144.98026 Accuracy: 0.93681\n",
      "Epoch: 466 Batch:   0 Loss: 144.61595 Accuracy: 0.93681\n",
      "Epoch: 467 Batch:   0 Loss: 144.25537 Accuracy: 0.93681\n",
      "Epoch: 468 Batch:   0 Loss: 143.89842 Accuracy: 0.93681\n",
      "Epoch: 469 Batch:   0 Loss: 143.54504 Accuracy: 0.93681\n",
      "Test Loss: 289.18851 Accuracy: 0.78008\n",
      "Epoch: 470 Batch:   0 Loss: 143.19522 Accuracy: 0.93734\n",
      "Epoch: 471 Batch:   0 Loss: 142.84900 Accuracy: 0.93734\n",
      "Epoch: 472 Batch:   0 Loss: 142.50626 Accuracy: 0.93734\n",
      "Epoch: 473 Batch:   0 Loss: 142.16687 Accuracy: 0.93734\n",
      "Epoch: 474 Batch:   0 Loss: 141.83095 Accuracy: 0.93734\n",
      "Epoch: 475 Batch:   0 Loss: 141.49846 Accuracy: 0.93734\n",
      "Epoch: 476 Batch:   0 Loss: 141.16927 Accuracy: 0.93734\n",
      "Epoch: 477 Batch:   0 Loss: 140.84343 Accuracy: 0.93786\n",
      "Epoch: 478 Batch:   0 Loss: 140.52084 Accuracy: 0.93786\n",
      "Epoch: 479 Batch:   0 Loss: 140.20155 Accuracy: 0.93786\n",
      "Test Loss: 289.88474 Accuracy: 0.78049\n",
      "Epoch: 480 Batch:   0 Loss: 139.88538 Accuracy: 0.93839\n",
      "Epoch: 481 Batch:   0 Loss: 139.57243 Accuracy: 0.93839\n",
      "Epoch: 482 Batch:   0 Loss: 139.26262 Accuracy: 0.93892\n",
      "Epoch: 483 Batch:   0 Loss: 138.95590 Accuracy: 0.93892\n",
      "Epoch: 484 Batch:   0 Loss: 138.65230 Accuracy: 0.93892\n",
      "Epoch: 485 Batch:   0 Loss: 138.35167 Accuracy: 0.93997\n",
      "Epoch: 486 Batch:   0 Loss: 138.05409 Accuracy: 0.93997\n",
      "Epoch: 487 Batch:   0 Loss: 137.75952 Accuracy: 0.93997\n",
      "Epoch: 488 Batch:   0 Loss: 137.46782 Accuracy: 0.93997\n",
      "Epoch: 489 Batch:   0 Loss: 137.17908 Accuracy: 0.93997\n",
      "Test Loss: 290.59377 Accuracy: 0.77954\n",
      "Epoch: 490 Batch:   0 Loss: 136.89316 Accuracy: 0.93997\n",
      "Epoch: 491 Batch:   0 Loss: 136.61009 Accuracy: 0.93997\n",
      "Epoch: 492 Batch:   0 Loss: 136.32983 Accuracy: 0.93997\n",
      "Epoch: 493 Batch:   0 Loss: 136.05235 Accuracy: 0.94050\n",
      "Epoch: 494 Batch:   0 Loss: 135.77762 Accuracy: 0.94050\n",
      "Epoch: 495 Batch:   0 Loss: 135.50563 Accuracy: 0.94050\n",
      "Epoch: 496 Batch:   0 Loss: 135.23630 Accuracy: 0.94050\n",
      "Epoch: 497 Batch:   0 Loss: 134.96957 Accuracy: 0.94050\n",
      "Epoch: 498 Batch:   0 Loss: 134.70552 Accuracy: 0.94102\n",
      "Epoch: 499 Batch:   0 Loss: 134.44406 Accuracy: 0.94102\n",
      "Test Loss: 291.27739 Accuracy: 0.77873\n",
      "Epoch: 500 Batch:   0 Loss: 134.18513 Accuracy: 0.94102\n",
      "Epoch: 501 Batch:   0 Loss: 133.92876 Accuracy: 0.94102\n",
      "Epoch: 502 Batch:   0 Loss: 133.67491 Accuracy: 0.94155\n",
      "Epoch: 503 Batch:   0 Loss: 133.42349 Accuracy: 0.94155\n",
      "Epoch: 504 Batch:   0 Loss: 133.17450 Accuracy: 0.94155\n",
      "Epoch: 505 Batch:   0 Loss: 132.92796 Accuracy: 0.94207\n",
      "Epoch: 506 Batch:   0 Loss: 132.68381 Accuracy: 0.94207\n",
      "Epoch: 507 Batch:   0 Loss: 132.44194 Accuracy: 0.94207\n",
      "Epoch: 508 Batch:   0 Loss: 132.20245 Accuracy: 0.94260\n",
      "Epoch: 509 Batch:   0 Loss: 131.96529 Accuracy: 0.94260\n",
      "Test Loss: 291.90216 Accuracy: 0.77846\n",
      "Epoch: 510 Batch:   0 Loss: 131.73039 Accuracy: 0.94260\n",
      "Epoch: 511 Batch:   0 Loss: 131.49770 Accuracy: 0.94260\n",
      "Epoch: 512 Batch:   0 Loss: 131.26726 Accuracy: 0.94313\n",
      "Epoch: 513 Batch:   0 Loss: 131.03900 Accuracy: 0.94313\n",
      "Epoch: 514 Batch:   0 Loss: 130.81291 Accuracy: 0.94313\n",
      "Epoch: 515 Batch:   0 Loss: 130.58893 Accuracy: 0.94313\n",
      "Epoch: 516 Batch:   0 Loss: 130.36707 Accuracy: 0.94313\n",
      "Epoch: 517 Batch:   0 Loss: 130.14732 Accuracy: 0.94313\n",
      "Epoch: 518 Batch:   0 Loss: 129.92964 Accuracy: 0.94365\n",
      "Epoch: 519 Batch:   0 Loss: 129.71397 Accuracy: 0.94365\n",
      "Test Loss: 292.44048 Accuracy: 0.77764\n",
      "Epoch: 520 Batch:   0 Loss: 129.50032 Accuracy: 0.94365\n",
      "Epoch: 521 Batch:   0 Loss: 129.28868 Accuracy: 0.94365\n",
      "Epoch: 522 Batch:   0 Loss: 129.07893 Accuracy: 0.94365\n",
      "Epoch: 523 Batch:   0 Loss: 128.87122 Accuracy: 0.94365\n",
      "Epoch: 524 Batch:   0 Loss: 128.66533 Accuracy: 0.94365\n",
      "Epoch: 525 Batch:   0 Loss: 128.46140 Accuracy: 0.94365\n",
      "Epoch: 526 Batch:   0 Loss: 128.25934 Accuracy: 0.94365\n",
      "Epoch: 527 Batch:   0 Loss: 128.05910 Accuracy: 0.94365\n",
      "Epoch: 528 Batch:   0 Loss: 127.86067 Accuracy: 0.94365\n",
      "Epoch: 529 Batch:   0 Loss: 127.66405 Accuracy: 0.94365\n",
      "Test Loss: 292.87044 Accuracy: 0.77818\n",
      "Epoch: 530 Batch:   0 Loss: 127.46920 Accuracy: 0.94365\n",
      "Epoch: 531 Batch:   0 Loss: 127.27613 Accuracy: 0.94418\n",
      "Epoch: 532 Batch:   0 Loss: 127.08479 Accuracy: 0.94418\n",
      "Epoch: 533 Batch:   0 Loss: 126.89517 Accuracy: 0.94418\n",
      "Epoch: 534 Batch:   0 Loss: 126.70724 Accuracy: 0.94418\n",
      "Epoch: 535 Batch:   0 Loss: 126.52092 Accuracy: 0.94418\n",
      "Epoch: 536 Batch:   0 Loss: 126.33630 Accuracy: 0.94418\n",
      "Epoch: 537 Batch:   0 Loss: 126.15332 Accuracy: 0.94471\n",
      "Epoch: 538 Batch:   0 Loss: 125.97198 Accuracy: 0.94471\n",
      "Epoch: 539 Batch:   0 Loss: 125.79222 Accuracy: 0.94471\n",
      "Test Loss: 293.17667 Accuracy: 0.77805\n",
      "Epoch: 540 Batch:   0 Loss: 125.61397 Accuracy: 0.94471\n",
      "Epoch: 541 Batch:   0 Loss: 125.43730 Accuracy: 0.94471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 542 Batch:   0 Loss: 125.26221 Accuracy: 0.94523\n",
      "Epoch: 543 Batch:   0 Loss: 125.08860 Accuracy: 0.94576\n",
      "Epoch: 544 Batch:   0 Loss: 124.91650 Accuracy: 0.94576\n",
      "Epoch: 545 Batch:   0 Loss: 124.74584 Accuracy: 0.94629\n",
      "Epoch: 546 Batch:   0 Loss: 124.57666 Accuracy: 0.94629\n",
      "Epoch: 547 Batch:   0 Loss: 124.40889 Accuracy: 0.94629\n",
      "Epoch: 548 Batch:   0 Loss: 124.24261 Accuracy: 0.94629\n",
      "Epoch: 549 Batch:   0 Loss: 124.07771 Accuracy: 0.94629\n",
      "Test Loss: 293.34890 Accuracy: 0.77737\n",
      "Epoch: 550 Batch:   0 Loss: 123.91420 Accuracy: 0.94629\n",
      "Epoch: 551 Batch:   0 Loss: 123.75206 Accuracy: 0.94629\n",
      "Epoch: 552 Batch:   0 Loss: 123.59124 Accuracy: 0.94629\n",
      "Epoch: 553 Batch:   0 Loss: 123.43182 Accuracy: 0.94576\n",
      "Epoch: 554 Batch:   0 Loss: 123.27369 Accuracy: 0.94576\n",
      "Epoch: 555 Batch:   0 Loss: 123.11685 Accuracy: 0.94576\n",
      "Epoch: 556 Batch:   0 Loss: 122.96133 Accuracy: 0.94576\n",
      "Epoch: 557 Batch:   0 Loss: 122.80708 Accuracy: 0.94576\n",
      "Epoch: 558 Batch:   0 Loss: 122.65409 Accuracy: 0.94576\n",
      "Epoch: 559 Batch:   0 Loss: 122.50235 Accuracy: 0.94576\n",
      "Test Loss: 293.38221 Accuracy: 0.77683\n",
      "Epoch: 560 Batch:   0 Loss: 122.35181 Accuracy: 0.94576\n",
      "Epoch: 561 Batch:   0 Loss: 122.20253 Accuracy: 0.94576\n",
      "Epoch: 562 Batch:   0 Loss: 122.05444 Accuracy: 0.94576\n",
      "Epoch: 563 Batch:   0 Loss: 121.90751 Accuracy: 0.94576\n",
      "Epoch: 564 Batch:   0 Loss: 121.76172 Accuracy: 0.94576\n",
      "Epoch: 565 Batch:   0 Loss: 121.61713 Accuracy: 0.94576\n",
      "Epoch: 566 Batch:   0 Loss: 121.47367 Accuracy: 0.94576\n",
      "Epoch: 567 Batch:   0 Loss: 121.33140 Accuracy: 0.94576\n",
      "Epoch: 568 Batch:   0 Loss: 121.19016 Accuracy: 0.94629\n",
      "Epoch: 569 Batch:   0 Loss: 121.05009 Accuracy: 0.94629\n",
      "Test Loss: 293.27525 Accuracy: 0.77642\n",
      "Epoch: 570 Batch:   0 Loss: 120.91102 Accuracy: 0.94681\n",
      "Epoch: 571 Batch:   0 Loss: 120.77312 Accuracy: 0.94681\n",
      "Epoch: 572 Batch:   0 Loss: 120.63618 Accuracy: 0.94681\n",
      "Epoch: 573 Batch:   0 Loss: 120.50035 Accuracy: 0.94681\n",
      "Epoch: 574 Batch:   0 Loss: 120.36559 Accuracy: 0.94681\n",
      "Epoch: 575 Batch:   0 Loss: 120.23178 Accuracy: 0.94681\n",
      "Epoch: 576 Batch:   0 Loss: 120.09903 Accuracy: 0.94681\n",
      "Epoch: 577 Batch:   0 Loss: 119.96725 Accuracy: 0.94681\n",
      "Epoch: 578 Batch:   0 Loss: 119.83645 Accuracy: 0.94681\n",
      "Epoch: 579 Batch:   0 Loss: 119.70666 Accuracy: 0.94681\n",
      "Test Loss: 293.02991 Accuracy: 0.77588\n",
      "Epoch: 580 Batch:   0 Loss: 119.57784 Accuracy: 0.94734\n",
      "Epoch: 581 Batch:   0 Loss: 119.44992 Accuracy: 0.94734\n",
      "Epoch: 582 Batch:   0 Loss: 119.32297 Accuracy: 0.94734\n",
      "Epoch: 583 Batch:   0 Loss: 119.19692 Accuracy: 0.94734\n",
      "Epoch: 584 Batch:   0 Loss: 119.07183 Accuracy: 0.94734\n",
      "Epoch: 585 Batch:   0 Loss: 118.94761 Accuracy: 0.94681\n",
      "Epoch: 586 Batch:   0 Loss: 118.82430 Accuracy: 0.94681\n",
      "Epoch: 587 Batch:   0 Loss: 118.70187 Accuracy: 0.94681\n",
      "Epoch: 588 Batch:   0 Loss: 118.58034 Accuracy: 0.94681\n",
      "Epoch: 589 Batch:   0 Loss: 118.45962 Accuracy: 0.94681\n",
      "Test Loss: 292.65018 Accuracy: 0.77575\n",
      "Epoch: 590 Batch:   0 Loss: 118.33980 Accuracy: 0.94681\n",
      "Epoch: 591 Batch:   0 Loss: 118.22078 Accuracy: 0.94734\n",
      "Epoch: 592 Batch:   0 Loss: 118.10262 Accuracy: 0.94734\n",
      "Epoch: 593 Batch:   0 Loss: 117.98521 Accuracy: 0.94734\n",
      "Epoch: 594 Batch:   0 Loss: 117.86871 Accuracy: 0.94734\n",
      "Epoch: 595 Batch:   0 Loss: 117.75295 Accuracy: 0.94734\n",
      "Epoch: 596 Batch:   0 Loss: 117.63803 Accuracy: 0.94734\n",
      "Epoch: 597 Batch:   0 Loss: 117.52382 Accuracy: 0.94734\n",
      "Epoch: 598 Batch:   0 Loss: 117.41043 Accuracy: 0.94734\n",
      "Epoch: 599 Batch:   0 Loss: 117.29781 Accuracy: 0.94734\n",
      "Test Loss: 292.14251 Accuracy: 0.77588\n",
      "Epoch: 600 Batch:   0 Loss: 117.18591 Accuracy: 0.94734\n",
      "Epoch: 601 Batch:   0 Loss: 117.07476 Accuracy: 0.94734\n",
      "Epoch: 602 Batch:   0 Loss: 116.96438 Accuracy: 0.94734\n",
      "Epoch: 603 Batch:   0 Loss: 116.85470 Accuracy: 0.94734\n",
      "Epoch: 604 Batch:   0 Loss: 116.74574 Accuracy: 0.94734\n",
      "Epoch: 605 Batch:   0 Loss: 116.63750 Accuracy: 0.94734\n",
      "Epoch: 606 Batch:   0 Loss: 116.52992 Accuracy: 0.94734\n",
      "Epoch: 607 Batch:   0 Loss: 116.42308 Accuracy: 0.94734\n",
      "Epoch: 608 Batch:   0 Loss: 116.31689 Accuracy: 0.94734\n",
      "Epoch: 609 Batch:   0 Loss: 116.21140 Accuracy: 0.94734\n",
      "Test Loss: 291.51456 Accuracy: 0.77629\n",
      "Epoch: 610 Batch:   0 Loss: 116.10655 Accuracy: 0.94734\n",
      "Epoch: 611 Batch:   0 Loss: 116.00238 Accuracy: 0.94734\n",
      "Epoch: 612 Batch:   0 Loss: 115.89886 Accuracy: 0.94734\n",
      "Epoch: 613 Batch:   0 Loss: 115.79594 Accuracy: 0.94734\n",
      "Epoch: 614 Batch:   0 Loss: 115.69368 Accuracy: 0.94734\n",
      "Epoch: 615 Batch:   0 Loss: 115.59206 Accuracy: 0.94734\n",
      "Epoch: 616 Batch:   0 Loss: 115.49104 Accuracy: 0.94734\n",
      "Epoch: 617 Batch:   0 Loss: 115.39062 Accuracy: 0.94734\n",
      "Epoch: 618 Batch:   0 Loss: 115.29086 Accuracy: 0.94734\n",
      "Epoch: 619 Batch:   0 Loss: 115.19164 Accuracy: 0.94681\n",
      "Test Loss: 290.77554 Accuracy: 0.77602\n",
      "Epoch: 620 Batch:   0 Loss: 115.09306 Accuracy: 0.94734\n",
      "Epoch: 621 Batch:   0 Loss: 114.99501 Accuracy: 0.94734\n",
      "Epoch: 622 Batch:   0 Loss: 114.89756 Accuracy: 0.94787\n",
      "Epoch: 623 Batch:   0 Loss: 114.80067 Accuracy: 0.94787\n",
      "Epoch: 624 Batch:   0 Loss: 114.70436 Accuracy: 0.94787\n",
      "Epoch: 625 Batch:   0 Loss: 114.60857 Accuracy: 0.94839\n",
      "Epoch: 626 Batch:   0 Loss: 114.51334 Accuracy: 0.94892\n",
      "Epoch: 627 Batch:   0 Loss: 114.41866 Accuracy: 0.94892\n",
      "Epoch: 628 Batch:   0 Loss: 114.32453 Accuracy: 0.94892\n",
      "Epoch: 629 Batch:   0 Loss: 114.23091 Accuracy: 0.94892\n",
      "Test Loss: 289.93595 Accuracy: 0.77629\n",
      "Epoch: 630 Batch:   0 Loss: 114.13782 Accuracy: 0.94892\n",
      "Epoch: 631 Batch:   0 Loss: 114.04523 Accuracy: 0.94892\n",
      "Epoch: 632 Batch:   0 Loss: 113.95316 Accuracy: 0.94892\n",
      "Epoch: 633 Batch:   0 Loss: 113.86160 Accuracy: 0.94892\n",
      "Epoch: 634 Batch:   0 Loss: 113.77052 Accuracy: 0.94892\n",
      "Epoch: 635 Batch:   0 Loss: 113.67993 Accuracy: 0.94892\n",
      "Epoch: 636 Batch:   0 Loss: 113.58986 Accuracy: 0.94945\n",
      "Epoch: 637 Batch:   0 Loss: 113.50024 Accuracy: 0.94945\n",
      "Epoch: 638 Batch:   0 Loss: 113.41113 Accuracy: 0.94945\n",
      "Epoch: 639 Batch:   0 Loss: 113.32246 Accuracy: 0.94997\n",
      "Test Loss: 289.00682 Accuracy: 0.77764\n",
      "Epoch: 640 Batch:   0 Loss: 113.23425 Accuracy: 0.94997\n",
      "Epoch: 641 Batch:   0 Loss: 113.14651 Accuracy: 0.94997\n",
      "Epoch: 642 Batch:   0 Loss: 113.05922 Accuracy: 0.94945\n",
      "Epoch: 643 Batch:   0 Loss: 112.97238 Accuracy: 0.94945\n",
      "Epoch: 644 Batch:   0 Loss: 112.88597 Accuracy: 0.94945\n",
      "Epoch: 645 Batch:   0 Loss: 112.80003 Accuracy: 0.94945\n",
      "Epoch: 646 Batch:   0 Loss: 112.71449 Accuracy: 0.94945\n",
      "Epoch: 647 Batch:   0 Loss: 112.62936 Accuracy: 0.94945\n",
      "Epoch: 648 Batch:   0 Loss: 112.54469 Accuracy: 0.94945\n",
      "Epoch: 649 Batch:   0 Loss: 112.46043 Accuracy: 0.94945\n",
      "Test Loss: 288.00020 Accuracy: 0.77805\n",
      "Epoch: 650 Batch:   0 Loss: 112.37658 Accuracy: 0.94945\n",
      "Epoch: 651 Batch:   0 Loss: 112.29314 Accuracy: 0.94945\n",
      "Epoch: 652 Batch:   0 Loss: 112.21010 Accuracy: 0.94945\n",
      "Epoch: 653 Batch:   0 Loss: 112.12745 Accuracy: 0.94945\n",
      "Epoch: 654 Batch:   0 Loss: 112.04519 Accuracy: 0.94945\n",
      "Epoch: 655 Batch:   0 Loss: 111.96333 Accuracy: 0.94945\n",
      "Epoch: 656 Batch:   0 Loss: 111.88187 Accuracy: 0.94945\n",
      "Epoch: 657 Batch:   0 Loss: 111.80076 Accuracy: 0.94945\n",
      "Epoch: 658 Batch:   0 Loss: 111.72007 Accuracy: 0.94945\n",
      "Epoch: 659 Batch:   0 Loss: 111.63974 Accuracy: 0.94945\n",
      "Test Loss: 286.92827 Accuracy: 0.77737\n",
      "Epoch: 660 Batch:   0 Loss: 111.55974 Accuracy: 0.94945\n",
      "Epoch: 661 Batch:   0 Loss: 111.48012 Accuracy: 0.94945\n",
      "Epoch: 662 Batch:   0 Loss: 111.40086 Accuracy: 0.94945\n",
      "Epoch: 663 Batch:   0 Loss: 111.32199 Accuracy: 0.94945\n",
      "Epoch: 664 Batch:   0 Loss: 111.24345 Accuracy: 0.94945\n",
      "Epoch: 665 Batch:   0 Loss: 111.16528 Accuracy: 0.94945\n",
      "Epoch: 666 Batch:   0 Loss: 111.08739 Accuracy: 0.94945\n",
      "Epoch: 667 Batch:   0 Loss: 111.00990 Accuracy: 0.94945\n",
      "Epoch: 668 Batch:   0 Loss: 110.93272 Accuracy: 0.94945\n",
      "Epoch: 669 Batch:   0 Loss: 110.85588 Accuracy: 0.94945\n",
      "Test Loss: 285.80283 Accuracy: 0.77859\n",
      "Epoch: 670 Batch:   0 Loss: 110.77939 Accuracy: 0.94945\n",
      "Epoch: 671 Batch:   0 Loss: 110.70319 Accuracy: 0.94945\n",
      "Epoch: 672 Batch:   0 Loss: 110.62733 Accuracy: 0.94892\n",
      "Epoch: 673 Batch:   0 Loss: 110.55177 Accuracy: 0.94892\n",
      "Epoch: 674 Batch:   0 Loss: 110.47654 Accuracy: 0.94892\n",
      "Epoch: 675 Batch:   0 Loss: 110.40163 Accuracy: 0.94892\n",
      "Epoch: 676 Batch:   0 Loss: 110.32706 Accuracy: 0.94945\n",
      "Epoch: 677 Batch:   0 Loss: 110.25271 Accuracy: 0.94997\n",
      "Epoch: 678 Batch:   0 Loss: 110.17872 Accuracy: 0.94997\n",
      "Epoch: 679 Batch:   0 Loss: 110.10500 Accuracy: 0.94997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 284.63512 Accuracy: 0.77927\n",
      "Epoch: 680 Batch:   0 Loss: 110.03160 Accuracy: 0.95050\n",
      "Epoch: 681 Batch:   0 Loss: 109.95846 Accuracy: 0.95103\n",
      "Epoch: 682 Batch:   0 Loss: 109.88566 Accuracy: 0.95103\n",
      "Epoch: 683 Batch:   0 Loss: 109.81308 Accuracy: 0.95103\n",
      "Epoch: 684 Batch:   0 Loss: 109.74083 Accuracy: 0.95103\n",
      "Epoch: 685 Batch:   0 Loss: 109.66884 Accuracy: 0.95103\n",
      "Epoch: 686 Batch:   0 Loss: 109.59713 Accuracy: 0.95103\n",
      "Epoch: 687 Batch:   0 Loss: 109.52569 Accuracy: 0.95103\n",
      "Epoch: 688 Batch:   0 Loss: 109.45448 Accuracy: 0.95103\n",
      "Epoch: 689 Batch:   0 Loss: 109.38356 Accuracy: 0.95103\n",
      "Test Loss: 283.43447 Accuracy: 0.77995\n",
      "Epoch: 690 Batch:   0 Loss: 109.31296 Accuracy: 0.95103\n",
      "Epoch: 691 Batch:   0 Loss: 109.24255 Accuracy: 0.95050\n",
      "Epoch: 692 Batch:   0 Loss: 109.17238 Accuracy: 0.95050\n",
      "Epoch: 693 Batch:   0 Loss: 109.10249 Accuracy: 0.95050\n",
      "Epoch: 694 Batch:   0 Loss: 109.03287 Accuracy: 0.95103\n",
      "Epoch: 695 Batch:   0 Loss: 108.96350 Accuracy: 0.95103\n",
      "Epoch: 696 Batch:   0 Loss: 108.89436 Accuracy: 0.95155\n",
      "Epoch: 697 Batch:   0 Loss: 108.82539 Accuracy: 0.95155\n",
      "Epoch: 698 Batch:   0 Loss: 108.75672 Accuracy: 0.95155\n",
      "Epoch: 699 Batch:   0 Loss: 108.68831 Accuracy: 0.95155\n",
      "Test Loss: 282.20961 Accuracy: 0.78225\n",
      "Epoch: 700 Batch:   0 Loss: 108.62009 Accuracy: 0.95155\n",
      "Epoch: 701 Batch:   0 Loss: 108.55208 Accuracy: 0.95155\n",
      "Epoch: 702 Batch:   0 Loss: 108.48432 Accuracy: 0.95155\n",
      "Epoch: 703 Batch:   0 Loss: 108.41683 Accuracy: 0.95155\n",
      "Epoch: 704 Batch:   0 Loss: 108.34946 Accuracy: 0.95155\n",
      "Epoch: 705 Batch:   0 Loss: 108.28236 Accuracy: 0.95155\n",
      "Epoch: 706 Batch:   0 Loss: 108.21548 Accuracy: 0.95155\n",
      "Epoch: 707 Batch:   0 Loss: 108.14883 Accuracy: 0.95155\n",
      "Epoch: 708 Batch:   0 Loss: 108.08236 Accuracy: 0.95155\n",
      "Epoch: 709 Batch:   0 Loss: 108.01610 Accuracy: 0.95155\n",
      "Test Loss: 280.97004 Accuracy: 0.78455\n",
      "Epoch: 710 Batch:   0 Loss: 107.95003 Accuracy: 0.95155\n",
      "Epoch: 711 Batch:   0 Loss: 107.88414 Accuracy: 0.95155\n",
      "Epoch: 712 Batch:   0 Loss: 107.81847 Accuracy: 0.95155\n",
      "Epoch: 713 Batch:   0 Loss: 107.75305 Accuracy: 0.95155\n",
      "Epoch: 714 Batch:   0 Loss: 107.68771 Accuracy: 0.95155\n",
      "Epoch: 715 Batch:   0 Loss: 107.62264 Accuracy: 0.95155\n",
      "Epoch: 716 Batch:   0 Loss: 107.55775 Accuracy: 0.95155\n",
      "Epoch: 717 Batch:   0 Loss: 107.49306 Accuracy: 0.95155\n",
      "Epoch: 718 Batch:   0 Loss: 107.42850 Accuracy: 0.95155\n",
      "Epoch: 719 Batch:   0 Loss: 107.36419 Accuracy: 0.95155\n",
      "Test Loss: 279.72887 Accuracy: 0.78564\n",
      "Epoch: 720 Batch:   0 Loss: 107.30000 Accuracy: 0.95208\n",
      "Epoch: 721 Batch:   0 Loss: 107.23605 Accuracy: 0.95208\n",
      "Epoch: 722 Batch:   0 Loss: 107.17224 Accuracy: 0.95208\n",
      "Epoch: 723 Batch:   0 Loss: 107.10860 Accuracy: 0.95208\n",
      "Epoch: 724 Batch:   0 Loss: 107.04513 Accuracy: 0.95261\n",
      "Epoch: 725 Batch:   0 Loss: 106.98183 Accuracy: 0.95261\n",
      "Epoch: 726 Batch:   0 Loss: 106.91875 Accuracy: 0.95261\n",
      "Epoch: 727 Batch:   0 Loss: 106.85580 Accuracy: 0.95313\n",
      "Epoch: 728 Batch:   0 Loss: 106.79298 Accuracy: 0.95313\n",
      "Epoch: 729 Batch:   0 Loss: 106.73039 Accuracy: 0.95313\n",
      "Test Loss: 278.50058 Accuracy: 0.78713\n",
      "Epoch: 730 Batch:   0 Loss: 106.66796 Accuracy: 0.95313\n",
      "Epoch: 731 Batch:   0 Loss: 106.60566 Accuracy: 0.95313\n",
      "Epoch: 732 Batch:   0 Loss: 106.54351 Accuracy: 0.95261\n",
      "Epoch: 733 Batch:   0 Loss: 106.48154 Accuracy: 0.95261\n",
      "Epoch: 734 Batch:   0 Loss: 106.41974 Accuracy: 0.95261\n",
      "Epoch: 735 Batch:   0 Loss: 106.35811 Accuracy: 0.95261\n",
      "Epoch: 736 Batch:   0 Loss: 106.29662 Accuracy: 0.95261\n",
      "Epoch: 737 Batch:   0 Loss: 106.23526 Accuracy: 0.95261\n",
      "Epoch: 738 Batch:   0 Loss: 106.17409 Accuracy: 0.95261\n",
      "Epoch: 739 Batch:   0 Loss: 106.11307 Accuracy: 0.95261\n",
      "Test Loss: 277.29819 Accuracy: 0.78564\n",
      "Epoch: 740 Batch:   0 Loss: 106.05221 Accuracy: 0.95261\n",
      "Epoch: 741 Batch:   0 Loss: 105.99145 Accuracy: 0.95261\n",
      "Epoch: 742 Batch:   0 Loss: 105.93087 Accuracy: 0.95261\n",
      "Epoch: 743 Batch:   0 Loss: 105.87047 Accuracy: 0.95261\n",
      "Epoch: 744 Batch:   0 Loss: 105.81020 Accuracy: 0.95261\n",
      "Epoch: 745 Batch:   0 Loss: 105.75008 Accuracy: 0.95313\n",
      "Epoch: 746 Batch:   0 Loss: 105.69005 Accuracy: 0.95313\n",
      "Epoch: 747 Batch:   0 Loss: 105.63023 Accuracy: 0.95313\n",
      "Epoch: 748 Batch:   0 Loss: 105.57054 Accuracy: 0.95366\n",
      "Epoch: 749 Batch:   0 Loss: 105.51100 Accuracy: 0.95366\n",
      "Test Loss: 276.13065 Accuracy: 0.78645\n",
      "Epoch: 750 Batch:   0 Loss: 105.45159 Accuracy: 0.95366\n",
      "Epoch: 751 Batch:   0 Loss: 105.39233 Accuracy: 0.95419\n",
      "Epoch: 752 Batch:   0 Loss: 105.33320 Accuracy: 0.95419\n",
      "Epoch: 753 Batch:   0 Loss: 105.27423 Accuracy: 0.95419\n",
      "Epoch: 754 Batch:   0 Loss: 105.21535 Accuracy: 0.95419\n",
      "Epoch: 755 Batch:   0 Loss: 105.15665 Accuracy: 0.95419\n",
      "Epoch: 756 Batch:   0 Loss: 105.09806 Accuracy: 0.95471\n",
      "Epoch: 757 Batch:   0 Loss: 105.03960 Accuracy: 0.95471\n",
      "Epoch: 758 Batch:   0 Loss: 104.98135 Accuracy: 0.95524\n",
      "Epoch: 759 Batch:   0 Loss: 104.92316 Accuracy: 0.95524\n",
      "Test Loss: 275.00455 Accuracy: 0.78753\n",
      "Epoch: 760 Batch:   0 Loss: 104.86514 Accuracy: 0.95524\n",
      "Epoch: 761 Batch:   0 Loss: 104.80727 Accuracy: 0.95524\n",
      "Epoch: 762 Batch:   0 Loss: 104.74948 Accuracy: 0.95524\n",
      "Epoch: 763 Batch:   0 Loss: 104.69182 Accuracy: 0.95524\n",
      "Epoch: 764 Batch:   0 Loss: 104.63435 Accuracy: 0.95524\n",
      "Epoch: 765 Batch:   0 Loss: 104.57693 Accuracy: 0.95524\n",
      "Epoch: 766 Batch:   0 Loss: 104.51972 Accuracy: 0.95524\n",
      "Epoch: 767 Batch:   0 Loss: 104.46258 Accuracy: 0.95524\n",
      "Epoch: 768 Batch:   0 Loss: 104.40562 Accuracy: 0.95524\n",
      "Epoch: 769 Batch:   0 Loss: 104.34874 Accuracy: 0.95524\n",
      "Test Loss: 273.92459 Accuracy: 0.78875\n",
      "Epoch: 770 Batch:   0 Loss: 104.29199 Accuracy: 0.95524\n",
      "Epoch: 771 Batch:   0 Loss: 104.23540 Accuracy: 0.95524\n",
      "Epoch: 772 Batch:   0 Loss: 104.17892 Accuracy: 0.95524\n",
      "Epoch: 773 Batch:   0 Loss: 104.12254 Accuracy: 0.95524\n",
      "Epoch: 774 Batch:   0 Loss: 104.06632 Accuracy: 0.95524\n",
      "Epoch: 775 Batch:   0 Loss: 104.01018 Accuracy: 0.95524\n",
      "Epoch: 776 Batch:   0 Loss: 103.95420 Accuracy: 0.95524\n",
      "Epoch: 777 Batch:   0 Loss: 103.89833 Accuracy: 0.95524\n",
      "Epoch: 778 Batch:   0 Loss: 103.84255 Accuracy: 0.95524\n",
      "Epoch: 779 Batch:   0 Loss: 103.78691 Accuracy: 0.95524\n",
      "Test Loss: 272.89481 Accuracy: 0.79079\n",
      "Epoch: 780 Batch:   0 Loss: 103.73141 Accuracy: 0.95524\n",
      "Epoch: 781 Batch:   0 Loss: 103.67601 Accuracy: 0.95524\n",
      "Epoch: 782 Batch:   0 Loss: 103.62071 Accuracy: 0.95524\n",
      "Epoch: 783 Batch:   0 Loss: 103.56553 Accuracy: 0.95524\n",
      "Epoch: 784 Batch:   0 Loss: 103.51051 Accuracy: 0.95524\n",
      "Epoch: 785 Batch:   0 Loss: 103.45557 Accuracy: 0.95524\n",
      "Epoch: 786 Batch:   0 Loss: 103.40075 Accuracy: 0.95524\n",
      "Epoch: 787 Batch:   0 Loss: 103.34602 Accuracy: 0.95524\n",
      "Epoch: 788 Batch:   0 Loss: 103.29146 Accuracy: 0.95524\n",
      "Epoch: 789 Batch:   0 Loss: 103.23695 Accuracy: 0.95524\n",
      "Test Loss: 271.91896 Accuracy: 0.79268\n",
      "Epoch: 790 Batch:   0 Loss: 103.18259 Accuracy: 0.95524\n",
      "Epoch: 791 Batch:   0 Loss: 103.12834 Accuracy: 0.95524\n",
      "Epoch: 792 Batch:   0 Loss: 103.07418 Accuracy: 0.95524\n",
      "Epoch: 793 Batch:   0 Loss: 103.02016 Accuracy: 0.95524\n",
      "Epoch: 794 Batch:   0 Loss: 102.96624 Accuracy: 0.95524\n",
      "Epoch: 795 Batch:   0 Loss: 102.91240 Accuracy: 0.95524\n",
      "Epoch: 796 Batch:   0 Loss: 102.85870 Accuracy: 0.95524\n",
      "Epoch: 797 Batch:   0 Loss: 102.80511 Accuracy: 0.95524\n",
      "Epoch: 798 Batch:   0 Loss: 102.75161 Accuracy: 0.95524\n",
      "Epoch: 799 Batch:   0 Loss: 102.69822 Accuracy: 0.95524\n",
      "Test Loss: 271.00038 Accuracy: 0.79363\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, dataDimension], name='X')\n",
    "Y = tf.placeholder(tf.float32, [None, numClasses], name='Y')\n",
    "protoNN = ProtoNN(dataDimension, PROJECTION_DIM,\n",
    "                  NUM_PROTOTYPES, numClasses,\n",
    "                  gamma, W=W, B=B)\n",
    "trainer = ProtoNNTrainer(protoNN,  REG_W, REG_B, REG_Z,\n",
    "                         SPAR_W, SPAR_B, SPAR_Z,\n",
    "                        LEARNING_RATE, X, Y, lossType='l2')\n",
    "sess = tf.Session()\n",
    "\n",
    "trainer.train(2048, 800, sess, x_train, x_test, y_train, y_test,\n",
    "              printStep=600, valStep=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy 0.79363143\n",
      "Model size constraint (Bytes):  9580\n",
      "Number of non-zeros:  2395\n"
     ]
    }
   ],
   "source": [
    "acc = sess.run(protoNN.accuracy, feed_dict={X: x_test, Y: y_test})\n",
    "pred = sess.run(protoNN.predictions, feed_dict={X: x_test, Y: y_test})\n",
    "# W, B, Z are tensorflow graph nodes\n",
    "W, B, Z, _ = protoNN.getModelMatrices()\n",
    "matrixList = sess.run([W, B, Z])\n",
    "sparcityList = [SPAR_W, SPAR_B, SPAR_Z]                       \n",
    "nnz, size, sparse = getModelSize(matrixList, sparcityList)\n",
    "print(\"Final test accuracy\", acc)\n",
    "print(\"Model size constraint (Bytes): \", size)\n",
    "print(\"Number of non-zeros: \", nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3016  674]\n",
      " [ 849 2841]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.78034   0.81734   0.79841      3690\n",
      "           1    0.80825   0.76992   0.78862      3690\n",
      "\n",
      "    accuracy                        0.79363      7380\n",
      "   macro avg    0.79429   0.79363   0.79352      7380\n",
      "weighted avg    0.79429   0.79363   0.79352      7380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "y_test = np.argmax(y_test,axis=1)\n",
    "print (confusion_matrix(y_test,pred))\n",
    "print (classification_report(y_test,pred,digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7699186991869919"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivity = confusion_matrix(y_test,pred)[1][1]/(confusion_matrix(y_test,pred)[1][1] + confusion_matrix(y_test,pred)[1][0])\n",
    "sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8173441734417344"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specificity = confusion_matrix(y_test,pred)[0][0]/(confusion_matrix(y_test,pred)[0][0] + confusion_matrix(y_test,pred)[0][1])\n",
    "specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "protoNN_example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ProtoNN",
   "language": "python",
   "name": "protonn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
