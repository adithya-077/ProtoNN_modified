{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RGFRB9ZVHWDY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nadit\\AppData\\Local\\Temp\\ipykernel_35224\\2844498513.py:17: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import helpermethods\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "# sys.path.insert(0, '../../')\n",
    "\n",
    "#Provide the GPU number to be used\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] =''\n",
    "\n",
    "#Bonsai imports\n",
    "# from edgeml.trainer.bonsaiTrainer import BonsaiTrainer\n",
    "# from edgeml.graph.bonsai import Bonsai\n",
    "\n",
    "# Fixing seeds for reproducibility\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "sys.path.append(r\"E:\\programming\\practice\\research\\EdgeML\\examples\\tf\\Bonsai\")\n",
    "import helpermethods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rg1vqMfVHvls"
   },
   "source": [
    "# Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "caCzClmoHthe"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def checkIntPos(value):\n",
    "    ivalue = int(value)\n",
    "    if ivalue <= 0:\n",
    "        raise argparse.ArgumentTypeError(\n",
    "            \"%s is an invalid positive int value\" % value)\n",
    "    return ivalue\n",
    "\n",
    "\n",
    "def checkIntNneg(value):\n",
    "    ivalue = int(value)\n",
    "    if ivalue < 0:\n",
    "        raise argparse.ArgumentTypeError(\n",
    "            \"%s is an invalid non-neg int value\" % value)\n",
    "    return ivalue\n",
    "\n",
    "\n",
    "def checkFloatNneg(value):\n",
    "    fvalue = float(value)\n",
    "    if fvalue < 0:\n",
    "        raise argparse.ArgumentTypeError(\n",
    "            \"%s is an invalid non-neg float value\" % value)\n",
    "    return fvalue\n",
    "\n",
    "\n",
    "def checkFloatPos(value):\n",
    "    fvalue = float(value)\n",
    "    if fvalue <= 0:\n",
    "        raise argparse.ArgumentTypeError(\n",
    "            \"%s is an invalid positive float value\" % value)\n",
    "    return fvalue\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "\n",
    "def getArgs():\n",
    "    '''\n",
    "    Function to parse arguments for Bonsai Algorithm\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='HyperParams for Bonsai Algorithm')\n",
    "    parser.add_argument('-dir', '--data-dir', required=True,\n",
    "                        help='Data directory containing' +\n",
    "                        'train.npy and test.npy')\n",
    "\n",
    "    parser.add_argument('-d', '--depth', type=checkIntNneg, default=2,\n",
    "                        help='Depth of Bonsai Tree ' +\n",
    "                        '(default: 2 try: [0, 1, 3])')\n",
    "    parser.add_argument('-p', '--proj-dim', type=checkIntPos, default=10,\n",
    "                        help='Projection Dimension ' +\n",
    "                        '(default: 20 try: [5, 10, 30])')\n",
    "    parser.add_argument('-s', '--sigma', type=float, default=1.0,\n",
    "                        help='Parameter for sigmoid sharpness ' +\n",
    "                        '(default: 1.0 try: [3.0, 0.05, 0.1]')\n",
    "    parser.add_argument('-e', '--epochs', type=checkIntPos, default=42,\n",
    "                        help='Total Epochs (default: 42 try:[100, 150, 60])')\n",
    "    parser.add_argument('-b', '--batch-size', type=checkIntPos,\n",
    "                        help='Batch Size to be used ' +\n",
    "                        '(default: max(100, sqrt(train_samples)))')\n",
    "    parser.add_argument('-lr', '--learning-rate', type=checkFloatPos,\n",
    "                        default=0.01, help='Initial Learning rate for ' +\n",
    "                        'Adam Optimizer (default: 0.01)')\n",
    "\n",
    "    parser.add_argument('-rW', type=float, default=0.0001,\n",
    "                        help='Regularizer for predictor parameter W  ' +\n",
    "                        '(default: 0.0001 try: [0.01, 0.001, 0.00001])')\n",
    "    parser.add_argument('-rV', type=float, default=0.0001,\n",
    "                        help='Regularizer for predictor parameter V  ' +\n",
    "                        '(default: 0.0001 try: [0.01, 0.001, 0.00001])')\n",
    "    parser.add_argument('-rT', type=float, default=0.0001,\n",
    "                        help='Regularizer for branching parameter Theta  ' +\n",
    "                        '(default: 0.0001 try: [0.01, 0.001, 0.00001])')\n",
    "    parser.add_argument('-rZ', type=float, default=0.00001,\n",
    "                        help='Regularizer for projection parameter Z  ' +\n",
    "                        '(default: 0.00001 try: [0.001, 0.0001, 0.000001])')\n",
    "\n",
    "    parser.add_argument('-sW', type=checkFloatPos,\n",
    "                        help='Sparsity for predictor parameter W  ' +\n",
    "                        '(default: For Binary classification 1.0 else 0.2 ' +\n",
    "                        'try: [0.1, 0.3, 0.5])')\n",
    "    parser.add_argument('-sV', type=checkFloatPos,\n",
    "                        help='Sparsity for predictor parameter V  ' +\n",
    "                        '(default: For Binary classification 1.0 else 0.2 ' +\n",
    "                        'try: [0.1, 0.3, 0.5])')\n",
    "    parser.add_argument('-sT', type=checkFloatPos,\n",
    "                        help='Sparsity for branching parameter Theta  ' +\n",
    "                        '(default: For Binary classification 1.0 else 0.2 ' +\n",
    "                        'try: [0.1, 0.3, 0.5])')\n",
    "    parser.add_argument('-sZ', type=checkFloatPos, default=0.2,\n",
    "                        help='Sparsity for projection parameter Z  ' +\n",
    "                        '(default: 0.2 try: [0.1, 0.3, 0.5])')\n",
    "    parser.add_argument('-oF', '--output-file', default=None,\n",
    "                        help='Output file for dumping the program output, ' +\n",
    "                        '(default: stdout)')\n",
    "\n",
    "    parser.add_argument('-regression', type=str2bool, default=False,\n",
    "                        help='boolean argument which controls whether to perform ' +\n",
    "                        'regression or classification.' +\n",
    "                        'default : False (Classification) values: [True, False]')\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def getQuantArgs():\n",
    "    '''\n",
    "    Function to parse arguments for Model Quantisation\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Arguments for quantizing Fast models. ' +\n",
    "        'Works only for piece-wise linear non-linearities, ' +\n",
    "        'like relu, quantTanh, quantSigm (check rnn.py for the definitions)')\n",
    "    parser.add_argument('-dir', '--model-dir', required=True,\n",
    "                        help='model directory containing' +\n",
    "                        '*.npy weight files dumped from the trained model')\n",
    "    parser.add_argument('-m', '--max-val', type=checkIntNneg, default=127,\n",
    "                        help='this represents the maximum possible value ' +\n",
    "                        'in model, essentially the byte complexity, ' +\n",
    "                        '127=> 1 byte is default')\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def createTimeStampDir(dataDir):\n",
    "    '''\n",
    "    Creates a Directory with timestamp as it's name\n",
    "    '''\n",
    "    if os.path.isdir(dataDir + '/TFBonsaiResults') is False:\n",
    "        try:\n",
    "            os.mkdir(dataDir + '/TFBonsaiResults')\n",
    "        except OSError:\n",
    "            print(\"Creation of the directory %s failed\" %\n",
    "                  dataDir + '/TFBonsaiResults')\n",
    "\n",
    "    currDir = 'TFBonsaiResults/' + datetime.datetime.now().strftime(\"%H_%M_%S_%d_%m_%y\")\n",
    "    if os.path.isdir(dataDir + '/' + currDir) is False:\n",
    "        try:\n",
    "            os.mkdir(dataDir + '/' + currDir)\n",
    "        except OSError:\n",
    "            print(\"Creation of the directory %s failed\" %\n",
    "                  dataDir + '/' + currDir)\n",
    "        else:\n",
    "            return (dataDir + '/' + currDir)\n",
    "    return None\n",
    "\n",
    "\n",
    "def preProcessData(dataDir, w,isRegression=False):\n",
    "    '''\n",
    "    Function to pre-process input data\n",
    "    Expects a .npy file of form [lbl feats] for each datapoint\n",
    "    Outputs a train and test set datapoints appended with 1 for Bias induction\n",
    "    dataDimension, numClasses are inferred directly\n",
    "    '''\n",
    "    train = np.load(dataDir + '/train_'+str(w)+'.npy')\n",
    "    test = np.load(dataDir + '/test_'+str(w)+'.npy')\n",
    "\n",
    "    dataDimension = int(train.shape[1]) - 1\n",
    "\n",
    "    Xtrain = train[:, 1:dataDimension + 1]\n",
    "    Ytrain_ = train[:, 0]\n",
    "\n",
    "    Xtest = test[:, 1:dataDimension + 1]\n",
    "    Ytest_ = test[:, 0]\n",
    "\n",
    "    # Mean Var Normalisation\n",
    "    mean = np.mean(Xtrain, 0)\n",
    "    std = np.std(Xtrain, 0)\n",
    "    std[std[:] < 0.000001] = 1\n",
    "    Xtrain = (Xtrain - mean) / std\n",
    "    Xtest = (Xtest - mean) / std\n",
    "    # End Mean Var normalisation\n",
    "\n",
    "    # Classification.\n",
    "    if (isRegression == False):\n",
    "        numClasses = max(Ytrain_) - min(Ytrain_) + 1\n",
    "        numClasses = int(max(numClasses, max(Ytest_) - min(Ytest_) + 1))\n",
    "\n",
    "        lab = Ytrain_.astype('uint8')\n",
    "        lab = np.array(lab) - min(lab)\n",
    "\n",
    "        lab_ = np.zeros((Xtrain.shape[0], numClasses))\n",
    "        lab_[np.arange(Xtrain.shape[0]), lab] = 1\n",
    "        if (numClasses == 2):\n",
    "            Ytrain = np.reshape(lab, [-1, 1])\n",
    "        else:\n",
    "            Ytrain = lab_\n",
    "\n",
    "        lab = Ytest_.astype('uint8')\n",
    "        lab = np.array(lab) - min(lab)\n",
    "\n",
    "        lab_ = np.zeros((Xtest.shape[0], numClasses))\n",
    "        lab_[np.arange(Xtest.shape[0]), lab] = 1\n",
    "        if (numClasses == 2):\n",
    "            Ytest = np.reshape(lab, [-1, 1])\n",
    "        else:\n",
    "            Ytest = lab_\n",
    "\n",
    "    elif (isRegression == True):\n",
    "        # The number of classes is always 1, for regression.\n",
    "        numClasses = 1\n",
    "        Ytrain = Ytrain_\n",
    "        Ytest = Ytest_\n",
    "\n",
    "    trainBias = np.ones([Xtrain.shape[0], 1])\n",
    "    Xtrain = np.append(Xtrain, trainBias, axis=1)\n",
    "    testBias = np.ones([Xtest.shape[0], 1])\n",
    "    Xtest = np.append(Xtest, testBias, axis=1)\n",
    "\n",
    "    mean = np.append(mean, np.array([0]))\n",
    "    std = np.append(std, np.array([1]))\n",
    "    \n",
    "    if (isRegression == False):\n",
    "        return dataDimension + 1, numClasses, Xtrain, Ytrain, Xtest, Ytest, mean, std\n",
    "    elif (isRegression == True):\n",
    "        return dataDimension + 1, numClasses, Xtrain, Ytrain.reshape((-1, 1)), Xtest, Ytest.reshape((-1, 1)), mean, std\n",
    "\n",
    "\n",
    "def dumpCommand(list, currDir):\n",
    "    '''\n",
    "    Dumps the current command to a file for further use\n",
    "    '''\n",
    "    commandFile = open(currDir + '/command.txt', 'w')\n",
    "    command = \"python\"\n",
    "\n",
    "    command = command + \" \" + ' '.join(list)\n",
    "    commandFile.write(command)\n",
    "\n",
    "    commandFile.flush()\n",
    "    commandFile.close()\n",
    "\n",
    "\n",
    "def saveMeanStd(mean, std, currDir):\n",
    "    '''\n",
    "    Function to save Mean and Std vectors\n",
    "    '''\n",
    "    np.save(currDir + '/mean.npy', mean)\n",
    "    np.save(currDir + '/std.npy', std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LjQNpiM_H95f"
   },
   "source": [
    "#utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNGyxDb6H1uB"
   },
   "outputs": [],
   "source": [
    "#utils\n",
    "import scipy.cluster\n",
    "import scipy.spatial\n",
    "import os\n",
    "\n",
    "\n",
    "def medianHeuristic(data, projectionDimension, numPrototypes, W_init=None):\n",
    "    '''\n",
    "    This method can be used to estimate gamma for ProtoNN. An approximation to\n",
    "    median heuristic is used here.\n",
    "    1. First the data is collapsed into the projectionDimension by W_init. If\n",
    "    W_init is not provided, it is initialized from a random normal(0, 1). Hence\n",
    "    data normalization is essential.\n",
    "    2. Prototype are computed by running a  k-means clustering on the projected\n",
    "    data.\n",
    "    3. The median distance is then estimated by calculating median distance\n",
    "    between prototypes and projected data points.\n",
    "\n",
    "    data needs to be [-1, numFeats]\n",
    "    If using this method to initialize gamma, please use the W and B as well.\n",
    "\n",
    "    TODO: Return estimate of Z (prototype labels) based on cluster centroids\n",
    "    andand labels\n",
    "\n",
    "    TODO: Clustering fails due to singularity error if projecting upwards\n",
    "\n",
    "    W [dxd_cap]\n",
    "    B [d_cap, m]\n",
    "    returns gamma, W, B\n",
    "    '''\n",
    "    assert data.ndim == 2\n",
    "    X = data\n",
    "    featDim = data.shape[1]\n",
    "    if projectionDimension > featDim:\n",
    "        print(\"Warning: Projection dimension > feature dimension. Gamma\")\n",
    "        print(\"\\t estimation due to median heuristic could fail.\")\n",
    "        print(\"\\tTo retain the projection dataDimension, provide\")\n",
    "        print(\"\\ta value for gamma.\")\n",
    "\n",
    "    if W_init is None:\n",
    "        W_init = np.random.normal(size=[featDim, projectionDimension])\n",
    "    W = W_init\n",
    "    XW = np.matmul(X, W)\n",
    "    assert XW.shape[1] == projectionDimension\n",
    "    assert XW.shape[0] == len(X)\n",
    "    # Requires [N x d_cap] data matrix of N observations of d_cap-dimension and\n",
    "    # the number of centroids m. Returns, [n x d_cap] centroids and\n",
    "    # elementwise center information.\n",
    "    B, centers = scipy.cluster.vq.kmeans2(XW, numPrototypes)\n",
    "    # Requires two matrices. Number of observations x dimension of observation\n",
    "    # space. Distances[i,j] is the distance between XW[i] and B[j]\n",
    "    distances = scipy.spatial.distance.cdist(XW, B, metric='euclidean')\n",
    "    distances = np.reshape(distances, [-1])\n",
    "    gamma = np.median(distances)\n",
    "    gamma = 1 / (2.5 * gamma)\n",
    "    return gamma.astype('float32'), W.astype('float32'), B.T.astype('float32')\n",
    "\n",
    "\n",
    "def multiClassHingeLoss(logits, label, batch_th):\n",
    "    '''\n",
    "    MultiClassHingeLoss to match C++ Version - No TF internal version\n",
    "    '''\n",
    "    flatLogits = tf.reshape(logits, [-1, ])\n",
    "    label_ = tf.argmax(label, 1)\n",
    "\n",
    "    correctId = tf.range(0, batch_th) * label.shape[1] + label_\n",
    "    correctLogit = tf.gather(flatLogits, correctId)\n",
    "\n",
    "    maxLabel = tf.argmax(logits, 1)\n",
    "    top2, _ = tf.nn.top_k(logits, k=2, sorted=True)\n",
    "\n",
    "    wrongMaxLogit = tf.where(\n",
    "        tf.equal(maxLabel, label_), top2[:, 1], top2[:, 0])\n",
    "\n",
    "    return tf.reduce_mean(tf.nn.relu(1. + wrongMaxLogit - correctLogit))\n",
    "\n",
    "\n",
    "def crossEntropyLoss(logits, label):\n",
    "    '''\n",
    "    Cross Entropy loss for MultiClass case in joint training for\n",
    "    faster convergence\n",
    "    '''\n",
    "    return tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                   labels=tf.stop_gradient(label)))\n",
    "\n",
    "\n",
    "def mean_absolute_error(logits, label):\n",
    "    '''\n",
    "    Function to compute the mean absolute error.\n",
    "    '''\n",
    "    return tf.reduce_mean(tf.abs(tf.subtract(logits, label)))\n",
    "\n",
    "\n",
    "def hardThreshold(A, s):\n",
    "    '''\n",
    "    Hard thresholding function on Tensor A with sparsity s\n",
    "    '''\n",
    "    A_ = np.copy(A)\n",
    "    A_ = A_.ravel()\n",
    "    if len(A_) > 0:\n",
    "        th = np.percentile(np.abs(A_), (1 - s) * 100.0, interpolation='higher')\n",
    "        A_[np.abs(A_) < th] = 0.0\n",
    "    A_ = A_.reshape(A.shape)\n",
    "    return A_\n",
    "\n",
    "\n",
    "def copySupport(src, dest):\n",
    "    '''\n",
    "    copy support of src tensor to dest tensor\n",
    "    '''\n",
    "    support = np.nonzero(src)\n",
    "    dest_ = dest\n",
    "    dest = np.zeros(dest_.shape)\n",
    "    dest[support] = dest_[support]\n",
    "    return dest\n",
    "\n",
    "\n",
    "def countnnZ(A, s, bytesPerVar=4):\n",
    "    '''\n",
    "    Returns # of non-zeros and representative size of the tensor\n",
    "    Uses dense for s >= 0.5 - 4 byte\n",
    "    Else uses sparse - 8 byte\n",
    "    '''\n",
    "    params = 1\n",
    "    hasSparse = False\n",
    "    for i in range(0, len(A.shape)):\n",
    "        params *= int(A.shape[i])\n",
    "    if s < 0.5:\n",
    "        nnZ = np.ceil(params * s)\n",
    "        hasSparse = True\n",
    "        return nnZ, nnZ * 2 * bytesPerVar, hasSparse\n",
    "    else:\n",
    "        nnZ = params\n",
    "        return nnZ, nnZ * bytesPerVar, hasSparse\n",
    "\n",
    "\n",
    "def getConfusionMatrix(predicted, target, numClasses):\n",
    "    '''\n",
    "    Returns a confusion matrix for a multiclass classification\n",
    "    problem. `predicted` is a 1-D array of integers representing\n",
    "    the predicted classes and `target` is the target classes.\n",
    "\n",
    "    confusion[i][j]: Number of elements of class j\n",
    "        predicted as class i\n",
    "    Labels are assumed to be in range(0, numClasses)\n",
    "    Use`printFormattedConfusionMatrix` to echo the confusion matrix\n",
    "    in a user friendly form.\n",
    "    '''\n",
    "    assert(predicted.ndim == 1)\n",
    "    assert(target.ndim == 1)\n",
    "    arr = np.zeros([numClasses, numClasses])\n",
    "\n",
    "    for i in range(len(predicted)):\n",
    "        arr[predicted[i]][target[i]] += 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "def printFormattedConfusionMatrix(matrix):\n",
    "    '''\n",
    "    Given a 2D confusion matrix, prints it in a human readable way.\n",
    "    The confusion matrix is expected to be a 2D numpy array with\n",
    "    square dimensions\n",
    "    '''\n",
    "    assert(matrix.ndim == 2)\n",
    "    assert(matrix.shape[0] == matrix.shape[1])\n",
    "    RECALL = 'Recall'\n",
    "    PRECISION = 'PRECISION'\n",
    "    print(\"|%s|\" % ('True->'), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%7d|\" % i, end='')\n",
    "    print(\"%s|\" % 'Precision')\n",
    "\n",
    "    print(\"|%s|\" % ('-' * len(RECALL)), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%s|\" % ('-' * 7), end='')\n",
    "    print(\"%s|\" % ('-' * len(PRECISION)))\n",
    "\n",
    "    precisionlist = np.sum(matrix, axis=1)\n",
    "    recalllist = np.sum(matrix, axis=0)\n",
    "    precisionlist = [matrix[i][i] / x if x !=\n",
    "                     0 else -1 for i, x in enumerate(precisionlist)]\n",
    "    recalllist = [matrix[i][i] / x if x !=\n",
    "                  0 else -1 for i, x in enumerate(recalllist)]\n",
    "    for i in range(matrix.shape[0]):\n",
    "        # len recall = 6\n",
    "        print(\"|%6d|\" % (i), end='')\n",
    "        for j in range(matrix.shape[0]):\n",
    "            print(\"%7d|\" % (matrix[i][j]), end='')\n",
    "        print(\"%s\" % (\" \" * (len(PRECISION) - 7)), end='')\n",
    "        if precisionlist[i] != -1:\n",
    "            print(\"%1.5f|\" % precisionlist[i])\n",
    "        else:\n",
    "            print(\"%7s|\" % \"nan\")\n",
    "\n",
    "    print(\"|%s|\" % ('-' * len(RECALL)), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%s|\" % ('-' * 7), end='')\n",
    "    print(\"%s|\" % ('-' * len(PRECISION)))\n",
    "    print(\"|%s|\" % ('Recall'), end='')\n",
    "\n",
    "    for i in range(matrix.shape[0]):\n",
    "        if recalllist[i] != -1:\n",
    "            print(\"%1.5f|\" % (recalllist[i]), end='')\n",
    "        else:\n",
    "            print(\"%7s|\" % \"nan\", end='')\n",
    "\n",
    "    print('%s|' % (' ' * len(PRECISION)))\n",
    "\n",
    "\n",
    "def getPrecisionRecall(cmatrix, label=1):\n",
    "    trueP = cmatrix[label][label]\n",
    "    denom = np.sum(cmatrix, axis=0)[label]\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    recall = trueP / denom\n",
    "    denom = np.sum(cmatrix, axis=1)[label]\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    precision = trueP / denom\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMacroPrecisionRecall(cmatrix):\n",
    "    # TP + FP\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    # TP + FN\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    precisionlist__ = [cmatrix[i][i] / x if x !=\n",
    "                       0 else 0 for i, x in enumerate(precisionlist)]\n",
    "    recalllist__ = [cmatrix[i][i] / x if x !=\n",
    "                    0 else 0 for i, x in enumerate(recalllist)]\n",
    "    precision = np.sum(precisionlist__)\n",
    "    precision /= len(precisionlist__)\n",
    "    recall = np.sum(recalllist__)\n",
    "    recall /= len(recalllist__)\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMicroPrecisionRecall(cmatrix):\n",
    "    # TP + FP\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    # TP + FN\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    num = 0.0\n",
    "    for i in range(len(cmatrix)):\n",
    "        num += cmatrix[i][i]\n",
    "\n",
    "    precision = num / np.sum(precisionlist)\n",
    "    recall = num / np.sum(recalllist)\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMacroMicroFScore(cmatrix):\n",
    "    '''\n",
    "    Returns macro and micro f-scores.\n",
    "    Refer: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.104.8244&rep=rep1&type=pdf\n",
    "    '''\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    precisionlist__ = [cmatrix[i][i] / x if x !=\n",
    "                       0 else 0 for i, x in enumerate(precisionlist)]\n",
    "    recalllist__ = [cmatrix[i][i] / x if x !=\n",
    "                    0 else 0 for i, x in enumerate(recalllist)]\n",
    "    macro = 0.0\n",
    "    for i in range(len(precisionlist)):\n",
    "        denom = precisionlist__[i] + recalllist__[i]\n",
    "        numer = precisionlist__[i] * recalllist__[i] * 2\n",
    "        if denom == 0:\n",
    "            denom = 1\n",
    "        macro += numer / denom\n",
    "    macro /= len(precisionlist)\n",
    "\n",
    "    num = 0.0\n",
    "    for i in range(len(precisionlist)):\n",
    "        num += cmatrix[i][i]\n",
    "\n",
    "    denom1 = np.sum(precisionlist)\n",
    "    denom2 = np.sum(recalllist)\n",
    "    pi = num / denom1\n",
    "    rho = num / denom2\n",
    "    denom = pi + rho\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    micro = 2 * pi * rho / denom\n",
    "    return macro, micro\n",
    "\n",
    "\n",
    "class GraphManager:\n",
    "    '''\n",
    "    Manages saving and restoring graphs. Designed to be used with EMI-RNN\n",
    "    though is general enough to be useful otherwise as well.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def checkpointModel(self, saver, sess, modelPrefix,\n",
    "                        globalStep=1000, redirFile=None):\n",
    "        saver.save(sess, modelPrefix, global_step=globalStep)\n",
    "        print('Model saved to %s, global_step %d' % (modelPrefix, globalStep),\n",
    "              file=redirFile)\n",
    "\n",
    "    def loadCheckpoint(self, sess, modelPrefix, globalStep,\n",
    "                       redirFile=None):\n",
    "        metaname = modelPrefix + '-%d.meta' % globalStep\n",
    "        basename = os.path.basename(metaname)\n",
    "        fileList = os.listdir(os.path.dirname(modelPrefix))\n",
    "        fileList = [x for x in fileList if x.startswith(basename)]\n",
    "        assert len(fileList) > 0, 'Checkpoint file not found'\n",
    "        msg = 'Too many or too few checkpoint files for globalStep: %d' % globalStep\n",
    "        assert len(fileList) is 1, msg\n",
    "        chkpt = basename + '/' + fileList[0]\n",
    "        saver = tf.train.import_meta_graph(metaname)\n",
    "        metaname = metaname[:-5]\n",
    "        saver.restore(sess, metaname)\n",
    "        graph = tf.get_default_graph()\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "usCaGutdHWDk"
   },
   "source": [
    "# Bonsai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-jpAffm_HWDo"
   },
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT license.\n",
    "import warnings\n",
    "\n",
    "\n",
    "class Bonsai:\n",
    "    def __init__(self, numClasses, dataDimension, projectionDimension,\n",
    "                 treeDepth, sigma,\n",
    "                 isRegression=False, W=None, T=None, V=None, Z=None):\n",
    "        '''\n",
    "        Expected Dimensions:\n",
    "\n",
    "        Bonsai Params // Optional\n",
    "        W [numClasses*totalNodes, projectionDimension]\n",
    "        V [numClasses*totalNodes, projectionDimension]\n",
    "        Z [projectionDimension, dataDimension + 1]\n",
    "        T [internalNodes, projectionDimension]\n",
    "\n",
    "        internalNodes = 2**treeDepth - 1\n",
    "        totalNodes = 2*internalNodes + 1\n",
    "\n",
    "        sigma - tanh non-linearity\n",
    "        sigmaI - Indicator function for node probabilities\n",
    "        sigmaI - has to be set to infinity(1e9 for practicality)\n",
    "        while doing testing/inference\n",
    "        numClasses will be reset to 1 in binary case\n",
    "        '''\n",
    "        self.dataDimension = dataDimension\n",
    "        self.projectionDimension = projectionDimension\n",
    "        self.isRegression = isRegression\n",
    "\n",
    "        if ((self.isRegression == True) & (numClasses != 1)):\n",
    "            warnings.warn(\"Number of classes cannot be greater than 1 for regression\")\n",
    "            self.numClasses = 1\n",
    "\n",
    "        if numClasses == 2:\n",
    "            self.numClasses = 1\n",
    "        else:\n",
    "            self.numClasses = numClasses\n",
    "\n",
    "        self.treeDepth = treeDepth\n",
    "        self.sigma = sigma\n",
    "\n",
    "        self.internalNodes = 2**self.treeDepth - 1\n",
    "        self.totalNodes = 2 * self.internalNodes + 1\n",
    "\n",
    "        self.W = self.initW(W)\n",
    "        self.V = self.initV(V)\n",
    "        self.T = self.initT(T)\n",
    "        self.Z = self.initZ(Z)\n",
    "\n",
    "        self.assertInit()\n",
    "\n",
    "        self.score = None\n",
    "        self.X_ = None\n",
    "        self.prediction = None\n",
    "\n",
    "    def initZ(self, Z):\n",
    "        if Z is None:\n",
    "            Z = tf.random_normal(\n",
    "                [self.projectionDimension, self.dataDimension])\n",
    "        Z = tf.Variable(Z, name='Z', dtype=tf.float32)\n",
    "        return Z\n",
    "\n",
    "    def initW(self, W):\n",
    "        if W is None:\n",
    "            W = tf.random_normal(\n",
    "                [self.numClasses * self.totalNodes, self.projectionDimension])\n",
    "        W = tf.Variable(W, name='W', dtype=tf.float32)\n",
    "        return W\n",
    "\n",
    "    def initV(self, V):\n",
    "        if V is None:\n",
    "            V = tf.random_normal(\n",
    "                [self.numClasses * self.totalNodes, self.projectionDimension])\n",
    "        V = tf.Variable(V, name='V', dtype=tf.float32)\n",
    "        return V\n",
    "\n",
    "    def initT(self, T):\n",
    "        if T is None:\n",
    "            T = tf.random_normal(\n",
    "                [self.internalNodes, self.projectionDimension])\n",
    "        T = tf.Variable(T, name='T', dtype=tf.float32)\n",
    "        return T\n",
    "\n",
    "    def __call__(self, X, sigmaI):\n",
    "        '''\n",
    "        Function to build the Bonsai Tree graph\n",
    "        Expected Dimensions\n",
    "\n",
    "        X is [_, self.dataDimension]\n",
    "        '''\n",
    "        errmsg = \"Dimension Mismatch, X is [_, self.dataDimension]\"\n",
    "        assert (len(X.shape) == 2 and int(\n",
    "            X.shape[1]) == self.dataDimension), errmsg\n",
    "        if self.score is not None:\n",
    "            return self.score, self.X_\n",
    "\n",
    "        X_ = tf.divide(tf.matmul(self.Z, X, transpose_b=True),\n",
    "                       self.projectionDimension)\n",
    "\n",
    "        W_ = self.W[0:(self.numClasses)]\n",
    "        V_ = self.V[0:(self.numClasses)]\n",
    "\n",
    "        self.__nodeProb = []\n",
    "        self.__nodeProb.append(1)\n",
    "\n",
    "        score_ = self.__nodeProb[0] * tf.multiply(\n",
    "            tf.matmul(W_, X_), tf.tanh(self.sigma * tf.matmul(V_, X_)))\n",
    "        for i in range(1, self.totalNodes):\n",
    "            W_ = self.W[i * self.numClasses:((i + 1) * self.numClasses)]\n",
    "            V_ = self.V[i * self.numClasses:((i + 1) * self.numClasses)]\n",
    "\n",
    "            T_ = tf.reshape(self.T[int(np.ceil(i / 2.0) - 1.0)],\n",
    "                            [-1, self.projectionDimension])\n",
    "            prob = (1 + ((-1)**(i + 1)) *\n",
    "                    tf.tanh(tf.multiply(sigmaI, tf.matmul(T_, X_))))\n",
    "\n",
    "            prob = tf.divide(prob, 2.0)\n",
    "            prob = self.__nodeProb[int(np.ceil(i / 2.0) - 1.0)] * prob\n",
    "            self.__nodeProb.append(prob)\n",
    "            score_ += self.__nodeProb[i] * tf.multiply(\n",
    "                tf.matmul(W_, X_), tf.tanh(self.sigma * tf.matmul(V_, X_)))\n",
    "\n",
    "        self.score = score_\n",
    "        self.X_ = X_\n",
    "        return self.score, self.X_\n",
    "\n",
    "    def getPrediction(self):\n",
    "        '''\n",
    "        Takes in a score tensor and outputs a integer class for each data point\n",
    "        '''\n",
    "\n",
    "        # Classification.\n",
    "        if (self.isRegression == False):\n",
    "            if self.prediction is not None:\n",
    "                return self.prediction\n",
    "\n",
    "            if self.numClasses > 2:\n",
    "                self.prediction = tf.argmax(tf.transpose(self.score), 1)\n",
    "            else:\n",
    "                self.prediction = tf.argmax(\n",
    "                    tf.concat([tf.transpose(self.score),\n",
    "                               0 * tf.transpose(self.score)], 1), 1)\n",
    "        # Regression.\n",
    "        elif (self.isRegression == True):\n",
    "            # For regression , scores are the actual predictions, just return them.\n",
    "            self.prediction = self.score    \n",
    "        return self.prediction\n",
    "\n",
    "    def assertInit(self):\n",
    "        errmsg = \"Number of Classes for regression can only be 1.\"\n",
    "        if (self.isRegression == True):\n",
    "            assert (self.numClasses == 1), errmsg\n",
    "        errRank = \"All Parameters must has only two dimensions shape = [a, b]\"\n",
    "        assert len(self.W.shape) == len(self.Z.shape), errRank\n",
    "        assert len(self.W.shape) == len(self.T.shape), errRank\n",
    "        assert len(self.W.shape) == 2, errRank\n",
    "        msg = \"W and V should be of same Dimensions\"\n",
    "        assert self.W.shape == self.V.shape, msg\n",
    "        errW = \"W and V are [numClasses*totalNodes, projectionDimension]\"\n",
    "        assert self.W.shape[0] == self.numClasses * self.totalNodes, errW\n",
    "        assert self.W.shape[1] == self.projectionDimension, errW\n",
    "        errZ = \"Z is [projectionDimension, dataDimension]\"\n",
    "        assert self.Z.shape[0] == self.projectionDimension, errZ\n",
    "        assert self.Z.shape[1] == self.dataDimension, errZ\n",
    "        errT = \"T is [internalNodes, projectionDimension]\"\n",
    "        assert self.T.shape[0] == self.internalNodes, errT\n",
    "        assert self.T.shape[1] == self.projectionDimension, errT\n",
    "        assert int(self.numClasses) > 0, \"numClasses should be > 1\"\n",
    "        msg = \"# of features in data should be > 0\"\n",
    "        assert int(self.dataDimension) > 0, msg\n",
    "        msg = \"Projection should be  > 0 dims\"\n",
    "        assert int(self.projectionDimension) > 0, msg\n",
    "        msg = \"treeDepth should be >= 0\"\n",
    "        assert int(self.treeDepth) >= 0, msg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32LpTZCDHWDw"
   },
   "source": [
    "# bonsaiTrainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TjW-WXGuHWDy"
   },
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT license.\n",
    "\n",
    "from __future__ import print_function\n",
    "# import edgeml.utils as utils\n",
    "\n",
    "\n",
    "class BonsaiTrainer:\n",
    "    def __init__(self, bonsaiObj, lW, lT, lV, lZ, sW, sT, sV, sZ,\n",
    "                 learningRate, X, Y, useMCHLoss=False, outFile=None, regLoss='huber'):\n",
    "        '''\n",
    "        bonsaiObj - Initialised Bonsai Object and Graph\n",
    "        lW, lT, lV and lZ are regularisers to Bonsai Params\n",
    "        sW, sT, sV and sZ are sparsity factors to Bonsai Params\n",
    "        learningRate - learningRate fro optimizer\n",
    "        X is the Data Placeholder - Dims [_, dataDimension]\n",
    "        Y - Label placeholder for loss computation\n",
    "        useMCHLoss - For choice between HingeLoss vs CrossEntropy\n",
    "        useMCHLoss - True - MultiClass - multiClassHingeLoss\n",
    "        useMCHLoss - False - MultiClass - crossEntropyLoss\n",
    "        '''\n",
    "        self.testAcc1 = 0\n",
    "        self.testLoss1 = 0\n",
    "        self.regTestLoss1= 0\n",
    "        self.pred1 = 0\n",
    "        \n",
    "        self.bonsaiObj = bonsaiObj\n",
    "        self.regressionLoss = regLoss\n",
    "\n",
    "        self.lW = lW\n",
    "        self.lV = lV\n",
    "        self.lT = lT\n",
    "        self.lZ = lZ\n",
    "\n",
    "        self.sW = sW\n",
    "        self.sV = sV\n",
    "        self.sT = sT\n",
    "        self.sZ = sZ\n",
    "\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "\n",
    "        self.useMCHLoss = useMCHLoss\n",
    "\n",
    "        if outFile is not None:\n",
    "            print(\"Outfile : \", outFile)\n",
    "            self.outFile = open(outFile, 'w')\n",
    "        else:\n",
    "            self.outFile = sys.stdout\n",
    "\n",
    "        self.learningRate = learningRate\n",
    "\n",
    "        self.assertInit()\n",
    "\n",
    "        self.sigmaI = tf.placeholder(tf.float32, name='sigmaI')\n",
    "\n",
    "        self.score, self.X_ = self.bonsaiObj(self.X, self.sigmaI)\n",
    "\n",
    "        self.loss, self.marginLoss, self.regLoss = self.lossGraph()\n",
    "\n",
    "        self.trainStep = self.trainGraph()\n",
    "        '''\n",
    "        self.accuracy -> 'MAE' for Regression.\n",
    "        self.accuracy -> 'Accuracy' for Classification.\n",
    "        '''\n",
    "        self.accuracy = self.accuracyGraph()\n",
    "        self.prediction = self.bonsaiObj.getPrediction()\n",
    "\n",
    "        if self.sW > 0.99 and self.sV > 0.99 and self.sZ > 0.99 and self.sT > 0.99:\n",
    "            self.isDenseTraining = True\n",
    "        else:\n",
    "            self.isDenseTraining = False\n",
    "\n",
    "        self.hardThrsd()\n",
    "        self.sparseTraining()\n",
    "\n",
    "    def lossGraph(self):\n",
    "        '''\n",
    "        Loss Graph for given Bonsai Obj\n",
    "        '''\n",
    "        self.regLoss = 0.5 * (self.lZ * tf.square(tf.norm(self.bonsaiObj.Z)) +\n",
    "                              self.lW * tf.square(tf.norm(self.bonsaiObj.W)) +\n",
    "                              self.lV * tf.square(tf.norm(self.bonsaiObj.V)) +\n",
    "                              self.lT * tf.square(tf.norm(self.bonsaiObj.T)))\n",
    "\n",
    "        # Loss functions for classification.\n",
    "        if (self.bonsaiObj.isRegression == False):\n",
    "            if (self.bonsaiObj.numClasses > 2):\n",
    "                if self.useMCHLoss is True:\n",
    "                    self.batch_th = tf.placeholder(tf.int64, name='batch_th')\n",
    "                    self.marginLoss = utils.multiClassHingeLoss(\n",
    "                        tf.transpose(self.score), self.Y,\n",
    "                        self.batch_th)\n",
    "                else:\n",
    "                    self.marginLoss = utils.crossEntropyLoss(\n",
    "                        tf.transpose(self.score), self.Y)\n",
    "                self.loss = self.marginLoss + self.regLoss\n",
    "            else:\n",
    "                self.marginLoss = tf.reduce_mean(tf.nn.relu(\n",
    "                    1.0 - (2 * self.Y - 1) * tf.transpose(self.score)))\n",
    "                self.loss = self.marginLoss + self.regLoss\n",
    "\n",
    "        # Loss functions for regression.\n",
    "        elif (self.bonsaiObj.isRegression == True):\n",
    "            if(self.regressionLoss == 'huber'):\n",
    "                # Use of Huber Loss , because it is more robust to outliers.\n",
    "                self.marginLoss = tf.losses.huber_loss(self.Y, tf.transpose(self.score))\n",
    "                self.loss = self.marginLoss + self.regLoss\n",
    "            elif (self.regressionLoss == 'l2'):\n",
    "                # L2 loss function.\n",
    "                self.marginLoss = tf.nn.l2_loss(self.Y - tf.transpose(self.score))\n",
    "                self.loss = self.marginLoss + self.regLoss\n",
    "\n",
    "        return self.loss, self.marginLoss, self.regLoss\n",
    "\n",
    "    def trainGraph(self):\n",
    "        '''\n",
    "        Train Graph for the loss generated by Bonsai\n",
    "        '''\n",
    "        self.bonsaiObj.TrainStep = tf.train.AdamOptimizer(\n",
    "            self.learningRate).minimize(self.loss)\n",
    "\n",
    "        return self.bonsaiObj.TrainStep\n",
    "\n",
    "    def accuracyGraph(self):\n",
    "        '''\n",
    "        Accuracy Graph to evaluate accuracy when needed\n",
    "        '''\n",
    "        if(self.bonsaiObj.isRegression == False):\n",
    "            if (self.bonsaiObj.numClasses > 2):\n",
    "                correctPrediction = tf.equal(\n",
    "                    tf.argmax(tf.transpose(self.score), 1), tf.argmax(self.Y, 1))\n",
    "                self.accuracy = tf.reduce_mean(tf.cast(correctPrediction, tf.float32))\n",
    "            else:\n",
    "                y_ = self.Y * 2 - 1\n",
    "                correctPrediction = tf.multiply(tf.transpose(self.score), y_)\n",
    "                correctPrediction = tf.nn.relu(correctPrediction)\n",
    "                correctPrediction = tf.ceil(tf.tanh(correctPrediction))\n",
    "                self.accuracy = tf.reduce_mean(tf.cast(correctPrediction, tf.float32))\n",
    "\n",
    "        elif (self.bonsaiObj.isRegression == True):\n",
    "            # Accuracy for regression , in terms of mean absolute error.\n",
    "            self.accuracy = mean_absolute_error(tf.reshape(\n",
    "                self.score, [-1, 1]), tf.reshape(self.Y, [-1, 1]))\n",
    "        return self.accuracy\n",
    "\n",
    "    def hardThrsd(self):\n",
    "        '''\n",
    "        Set up for hard Thresholding Functionality\n",
    "        '''\n",
    "        self.__Wth = tf.placeholder(tf.float32, name='Wth')\n",
    "        self.__Vth = tf.placeholder(tf.float32, name='Vth')\n",
    "        self.__Zth = tf.placeholder(tf.float32, name='Zth')\n",
    "        self.__Tth = tf.placeholder(tf.float32, name='Tth')\n",
    "\n",
    "        self.__Woph = self.bonsaiObj.W.assign(self.__Wth)\n",
    "        self.__Voph = self.bonsaiObj.V.assign(self.__Vth)\n",
    "        self.__Toph = self.bonsaiObj.T.assign(self.__Tth)\n",
    "        self.__Zoph = self.bonsaiObj.Z.assign(self.__Zth)\n",
    "\n",
    "        self.hardThresholdGroup = tf.group(\n",
    "            self.__Woph, self.__Voph, self.__Toph, self.__Zoph)\n",
    "\n",
    "    def sparseTraining(self):\n",
    "        '''\n",
    "        Set up for Sparse Retraining Functionality\n",
    "        '''\n",
    "        self.__Wops = self.bonsaiObj.W.assign(self.__Wth)\n",
    "        self.__Vops = self.bonsaiObj.V.assign(self.__Vth)\n",
    "        self.__Zops = self.bonsaiObj.Z.assign(self.__Zth)\n",
    "        self.__Tops = self.bonsaiObj.T.assign(self.__Tth)\n",
    "\n",
    "        self.sparseRetrainGroup = tf.group(\n",
    "            self.__Wops, self.__Vops, self.__Tops, self.__Zops)\n",
    "\n",
    "    def runHardThrsd(self, sess):\n",
    "        '''\n",
    "        Function to run the IHT routine on Bonsai Obj\n",
    "        '''\n",
    "        currW = self.bonsaiObj.W.eval()\n",
    "        currV = self.bonsaiObj.V.eval()\n",
    "        currZ = self.bonsaiObj.Z.eval()\n",
    "        currT = self.bonsaiObj.T.eval()\n",
    "\n",
    "        self.__thrsdW = hardThreshold(currW, self.sW)\n",
    "        self.__thrsdV = hardThreshold(currV, self.sV)\n",
    "        self.__thrsdZ = hardThreshold(currZ, self.sZ)\n",
    "        self.__thrsdT = hardThreshold(currT, self.sT)\n",
    "\n",
    "        fd_thrsd = {self.__Wth: self.__thrsdW, self.__Vth: self.__thrsdV,\n",
    "                    self.__Zth: self.__thrsdZ, self.__Tth: self.__thrsdT}\n",
    "        sess.run(self.hardThresholdGroup, feed_dict=fd_thrsd)\n",
    "\n",
    "    def runSparseTraining(self, sess):\n",
    "        '''\n",
    "        Function to run the Sparse Retraining routine on Bonsai Obj\n",
    "        '''\n",
    "        currW = self.bonsaiObj.W.eval()\n",
    "        currV = self.bonsaiObj.V.eval()\n",
    "        currZ = self.bonsaiObj.Z.eval()\n",
    "        currT = self.bonsaiObj.T.eval()\n",
    "\n",
    "        newW = copySupport(self.__thrsdW, currW)\n",
    "        newV = copySupport(self.__thrsdV, currV)\n",
    "        newZ = copySupport(self.__thrsdZ, currZ)\n",
    "        newT = copySupport(self.__thrsdT, currT)\n",
    "\n",
    "        fd_st = {self.__Wth: newW, self.__Vth: newV,\n",
    "                 self.__Zth: newZ, self.__Tth: newT}\n",
    "        sess.run(self.sparseRetrainGroup, feed_dict=fd_st)\n",
    "\n",
    "    def assertInit(self):\n",
    "        err = \"sparsity must be between 0 and 1\"\n",
    "        assert self.sW >= 0 and self.sW <= 1, \"W \" + err\n",
    "        assert self.sV >= 0 and self.sV <= 1, \"V \" + err\n",
    "        assert self.sZ >= 0 and self.sZ <= 1, \"Z \" + err\n",
    "        assert self.sT >= 0 and self.sT <= 1, \"T \" + err\n",
    "        errMsg = \"Dimension Mismatch, Y has to be [_, \" + \\\n",
    "            str(self.bonsaiObj.numClasses) + \"]\"\n",
    "        errCont = \" numClasses are 1 in case of Binary case by design\"\n",
    "        assert (len(self.Y.shape) == 2 and\n",
    "                self.Y.shape[1] == self.bonsaiObj.numClasses), errMsg + errCont\n",
    "\n",
    "    def saveParams(self, currDir):\n",
    "        '''\n",
    "        Function to save Parameter matrices into a given folder\n",
    "        '''\n",
    "        paramDir = currDir + '/'\n",
    "        np.save(paramDir + \"W.npy\", self.bonsaiObj.W.eval())\n",
    "        np.save(paramDir + \"V.npy\", self.bonsaiObj.V.eval())\n",
    "        np.save(paramDir + \"T.npy\", self.bonsaiObj.T.eval())\n",
    "        np.save(paramDir + \"Z.npy\", self.bonsaiObj.Z.eval())\n",
    "        hyperParamDict = {'dataDim': self.bonsaiObj.dataDimension,\n",
    "                          'projDim': self.bonsaiObj.projectionDimension,\n",
    "                          'numClasses': self.bonsaiObj.numClasses,\n",
    "                          'depth': self.bonsaiObj.treeDepth,\n",
    "                          'sigma': self.bonsaiObj.sigma}\n",
    "        hyperParamFile = paramDir + 'hyperParam.npy'\n",
    "        np.save(hyperParamFile, hyperParamDict)\n",
    "\n",
    "    def loadModel(self, currDir):\n",
    "        '''\n",
    "        Load the Saved model and load it to the model using constructor\n",
    "        Returns two dict one for params and other for hyperParams\n",
    "        '''\n",
    "        paramDir = currDir + '/'\n",
    "        paramDict = {}\n",
    "        paramDict['W'] = np.load(paramDir + \"W.npy\")\n",
    "        paramDict['V'] = np.load(paramDir + \"V.npy\")\n",
    "        paramDict['T'] = np.load(paramDir + \"T.npy\")\n",
    "        paramDict['Z'] = np.load(paramDir + \"Z.npy\")\n",
    "        hyperParamDict = np.load(paramDir + \"hyperParam.npy\").item()\n",
    "        return paramDict, hyperParamDict\n",
    "\n",
    "    # Function to get aimed model size\n",
    "    def getModelSize(self):\n",
    "        '''\n",
    "        Function to get aimed model size\n",
    "        '''\n",
    "        nnzZ, sizeZ, sparseZ = countnnZ(self.bonsaiObj.Z, self.sZ)\n",
    "        nnzW, sizeW, sparseW = countnnZ(self.bonsaiObj.W, self.sW)\n",
    "        nnzV, sizeV, sparseV = countnnZ(self.bonsaiObj.V, self.sV)\n",
    "        nnzT, sizeT, sparseT = countnnZ(self.bonsaiObj.T, self.sT)\n",
    "\n",
    "        totalnnZ = (nnzZ + nnzT + nnzV + nnzW)\n",
    "        totalSize = (sizeZ + sizeW + sizeV + sizeT)\n",
    "        hasSparse = (sparseW or sparseV or sparseT or sparseZ)\n",
    "        return totalnnZ, totalSize, hasSparse\n",
    "    \n",
    "    \n",
    "    def train(self, batchSize, totalEpochs, sess,\n",
    "              Xtrain, Xtest, Ytrain, Ytest, dataDir, currDir):\n",
    "        '''\n",
    "        The Dense - IHT - Sparse Retrain Routine for Bonsai Training\n",
    "        '''\n",
    "        resultFile = open(dataDir + '/TFBonsaiResults.txt', 'a+')\n",
    "        numIters = Xtrain.shape[0] / batchSize\n",
    "\n",
    "        totalBatches = numIters * totalEpochs\n",
    "\n",
    "        bonsaiObjSigmaI = 1\n",
    "\n",
    "        counter = 0\n",
    "        if self.bonsaiObj.numClasses > 2:\n",
    "            trimlevel = 15\n",
    "        else:\n",
    "            trimlevel = 5\n",
    "        ihtDone = 0\n",
    "        if (self.bonsaiObj.isRegression == True):\n",
    "            maxTestAcc = 100000007\n",
    "        else:\n",
    "            maxTestAcc = -10000\n",
    "        if self.isDenseTraining is True:\n",
    "            ihtDone = 1\n",
    "            bonsaiObjSigmaI = 1\n",
    "            itersInPhase = 0\n",
    "\n",
    "        header = '*' * 20\n",
    "        for i in range(totalEpochs):\n",
    "            print(\"\\nEpoch Number: \" + str(i), file=self.outFile)\n",
    "\n",
    "            '''\n",
    "            trainAcc -> For Regression, it is 'Mean Absolute Error'.\n",
    "            trainAcc -> For Classification, it is 'Accuracy'.\n",
    "            '''\n",
    "            trainAcc = 0.0\n",
    "            trainLoss = 0.0\n",
    "\n",
    "            numIters = int(numIters)\n",
    "            for j in range(numIters):\n",
    "\n",
    "                if counter == 0:\n",
    "                    msg = \" Dense Training Phase Started \"\n",
    "                    print(\"\\n%s%s%s\\n\" %\n",
    "                          (header, msg, header), file=self.outFile)\n",
    "\n",
    "                # Updating the indicator sigma\n",
    "                if ((counter == 0) or (counter == int(totalBatches / 3.0)) or\n",
    "                        (counter == int(2 * totalBatches / 3.0))) and (self.isDenseTraining is False):\n",
    "                    bonsaiObjSigmaI = 1\n",
    "                    itersInPhase = 0\n",
    "\n",
    "                elif (itersInPhase % 100 == 0):\n",
    "                    indices = np.random.choice(Xtrain.shape[0], 100)\n",
    "                    batchX = Xtrain[indices, :]\n",
    "                    batchY = Ytrain[indices, :]\n",
    "                    batchY = np.reshape(\n",
    "                        batchY, [-1, self.bonsaiObj.numClasses])\n",
    "\n",
    "                    _feed_dict = {self.X: batchX}\n",
    "                    Xcapeval = self.X_.eval(feed_dict=_feed_dict)\n",
    "                    Teval = self.bonsaiObj.T.eval()\n",
    "\n",
    "                    sum_tr = 0.0\n",
    "                    for k in range(0, self.bonsaiObj.internalNodes):\n",
    "                        sum_tr += (np.sum(np.abs(np.dot(Teval[k], Xcapeval))))\n",
    "\n",
    "                    if(self.bonsaiObj.internalNodes > 0):\n",
    "                        sum_tr /= (100 * self.bonsaiObj.internalNodes)\n",
    "                        sum_tr = 0.1 / sum_tr\n",
    "                    else:\n",
    "                        sum_tr = 0.1\n",
    "                    sum_tr = min(\n",
    "                        1000, sum_tr * (2**(float(itersInPhase) /\n",
    "                                            (float(totalBatches) / 30.0))))\n",
    "\n",
    "                    bonsaiObjSigmaI = sum_tr\n",
    "\n",
    "                itersInPhase += 1\n",
    "                batchX = Xtrain[j * batchSize:(j + 1) * batchSize]\n",
    "                batchY = Ytrain[j * batchSize:(j + 1) * batchSize]\n",
    "                batchY = np.reshape(\n",
    "                    batchY, [-1, self.bonsaiObj.numClasses])\n",
    "\n",
    "                if self.bonsaiObj.numClasses > 2:\n",
    "                    if self.useMCHLoss is True:\n",
    "                        _feed_dict = {self.X: batchX, self.Y: batchY,\n",
    "                                      self.batch_th: batchY.shape[0],\n",
    "                                      self.sigmaI: bonsaiObjSigmaI}\n",
    "                    else:\n",
    "                        _feed_dict = {self.X: batchX, self.Y: batchY,\n",
    "                                      self.sigmaI: bonsaiObjSigmaI}\n",
    "                else:\n",
    "                    _feed_dict = {self.X: batchX, self.Y: batchY,\n",
    "                                  self.sigmaI: bonsaiObjSigmaI}\n",
    "\n",
    "                # Mini-batch training\n",
    "                _, batchLoss, batchAcc = sess.run(\n",
    "                    [self.trainStep, self.loss, self.accuracy],\n",
    "                    feed_dict=_feed_dict)\n",
    "\n",
    "                # Classification.\n",
    "                if (self.bonsaiObj.isRegression == False):\n",
    "                    trainAcc += batchAcc\n",
    "                    trainLoss += batchLoss\n",
    "                # Regression.\n",
    "                else:\n",
    "                    trainAcc += np.mean(batchAcc)\n",
    "                    trainLoss += np.mean(batchLoss)\n",
    "\n",
    "                # Training routine involving IHT and sparse retraining\n",
    "                if (counter >= int(totalBatches / 3.0) and\n",
    "                    (counter < int(2 * totalBatches / 3.0)) and\n",
    "                    counter % trimlevel == 0 and\n",
    "                        self.isDenseTraining is False):\n",
    "                    self.runHardThrsd(sess)\n",
    "                    if ihtDone == 0:\n",
    "                        msg = \" IHT Phase Started \"\n",
    "                        print(\"\\n%s%s%s\\n\" %\n",
    "                              (header, msg, header), file=self.outFile)\n",
    "                    ihtDone = 1\n",
    "                elif ((ihtDone == 1 and counter >= int(totalBatches / 3.0) and\n",
    "                       (counter < int(2 * totalBatches / 3.0)) and\n",
    "                       counter % trimlevel != 0 and\n",
    "                       self.isDenseTraining is False) or\n",
    "                        (counter >= int(2 * totalBatches / 3.0) and\n",
    "                            self.isDenseTraining is False)):\n",
    "                    self.runSparseTraining(sess)\n",
    "                    if counter == int(2 * totalBatches / 3.0):\n",
    "                        msg = \" Sparse Retraining Phase Started \"\n",
    "                        print(\"\\n%s%s%s\\n\" %\n",
    "                              (header, msg, header), file=self.outFile)\n",
    "                counter += 1\n",
    "            try:\n",
    "                if (self.bonsaiObj.isRegression == True):\n",
    "                    print(\"\\nRegression Train Loss: \" + str(trainLoss / numIters) +\n",
    "                          \"\\nTraining MAE (Regression): \" + str(trainAcc / numIters),\n",
    "                          file=self.outFile)\n",
    "                else:\n",
    "                    print(\"\\nClassification Train Loss: \" + str(trainLoss / numIters) +\n",
    "                          \"\\nTraining accuracy (Classification): \" + str(trainAcc / numIters),\n",
    "                          file=self.outFile)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            oldSigmaI = bonsaiObjSigmaI\n",
    "            bonsaiObjSigmaI = 1e9\n",
    "\n",
    "            if self.bonsaiObj.numClasses > 2:\n",
    "                if self.useMCHLoss is True:\n",
    "                    _feed_dict = {self.X: Xtest, self.Y: Ytest,\n",
    "                                  self.batch_th: Ytest.shape[0],\n",
    "                                  self.sigmaI: bonsaiObjSigmaI}\n",
    "                else:\n",
    "                    _feed_dict = {self.X: Xtest, self.Y: Ytest,\n",
    "                                  self.sigmaI: bonsaiObjSigmaI}\n",
    "            else:\n",
    "                _feed_dict = {self.X: Xtest, self.Y: Ytest,\n",
    "                              self.sigmaI: bonsaiObjSigmaI}\n",
    "\n",
    "            # This helps in direct testing instead of extracting the model out\n",
    "\n",
    "            testAcc, testLoss, regTestLoss, pred = sess.run(\n",
    "                [self.accuracy, self.loss, self.regLoss, self.prediction], feed_dict=_feed_dict)\n",
    "            self.testAcc1, self.testLoss1, self.regTestLoss1, self.pred1 = sess.run(\n",
    "                [self.accuracy, self.loss, self.regLoss, self.prediction], feed_dict=_feed_dict)\n",
    "\n",
    "            if ihtDone == 0:\n",
    "                if (self.bonsaiObj.isRegression == False):\n",
    "                    maxTestAcc = -10000\n",
    "                    maxTestAccEpoch = i\n",
    "                elif (self.bonsaiObj.isRegression == True):\n",
    "                    maxTestAcc = testAcc\n",
    "                    maxTestAccEpoch = i\n",
    "\n",
    "            else:\n",
    "                if (self.bonsaiObj.isRegression == False):\n",
    "                    if maxTestAcc <= testAcc:\n",
    "                        maxTestAccEpoch = i\n",
    "                        maxTestAcc = testAcc\n",
    "                        self.saveParams(currDir)\n",
    "                elif (self.bonsaiObj.isRegression == True):\n",
    "                    print(\"Minimum Training MAE : \", np.mean(maxTestAcc))\n",
    "                    if maxTestAcc >= testAcc:\n",
    "                        # For regression , we're more interested in the minimum MAE.\n",
    "                        maxTestAccEpoch = i\n",
    "                        maxTestAcc = testAcc\n",
    "                        self.saveParams(currDir)\n",
    "\n",
    "            if (self.bonsaiObj.isRegression == True):\n",
    "                print(\"Testing MAE %g\" % np.mean(testAcc), file=self.outFile)\n",
    "            else:\n",
    "                print(\"Test accuracy %g\" % np.mean(testAcc), file=self.outFile)\n",
    "#                 print(\"Prediction %g\" %pred ,file=self.outFile)\n",
    "\n",
    "            if (self.bonsaiObj.isRegression == True):\n",
    "                testAcc = np.mean(testAcc)\n",
    "            else:\n",
    "                testAcc = testAcc\n",
    "                maxTestAcc = maxTestAcc\n",
    "\n",
    "            print(\"MarginLoss + RegLoss: \" + str(testLoss - regTestLoss) +\n",
    "                  \" + \" + str(regTestLoss) + \" = \" + str(testLoss) + \"\\n\",\n",
    "                  file=self.outFile)\n",
    "            self.outFile.flush()\n",
    "\n",
    "            bonsaiObjSigmaI = oldSigmaI\n",
    "\n",
    "        # sigmaI has to be set to infinity to ensure\n",
    "        # only a single path is used in inference\n",
    "        bonsaiObjSigmaI = 1e9\n",
    "        print(\"\\nNon-Zero : \" + str(self.getModelSize()[0]) + \" Model Size: \" +\n",
    "              str(float(self.getModelSize()[1]) / 1024.0) + \" KB hasSparse: \" +\n",
    "              str(self.getModelSize()[2]) + \"\\n\", file=self.outFile)\n",
    "\n",
    "        if (self.bonsaiObj.isRegression == True):\n",
    "            maxTestAcc = np.mean(maxTestAcc)\n",
    "\n",
    "        if (self.bonsaiObj.isRegression == True):\n",
    "            print(\"For Regression, Minimum MAE at compressed\" +\n",
    "                  \" model size(including early stopping): \" +\n",
    "                  str(maxTestAcc) + \" at Epoch: \" +\n",
    "                  str(maxTestAccEpoch + 1) + \"\\nFinal Test\" +\n",
    "                  \" MAE: \" + str(testAcc), file=self.outFile)\n",
    "\n",
    "            resultFile.write(\"MinTestMAE: \" + str(maxTestAcc) +\n",
    "                             \" at Epoch(totalEpochs): \" +\n",
    "                             str(maxTestAccEpoch + 1) +\n",
    "                             \"(\" + str(totalEpochs) + \")\" + \" ModelSize: \" +\n",
    "                             str(float(self.getModelSize()[1]) / 1024.0) +\n",
    "                             \" KB hasSparse: \" + str(self.getModelSize()[2]) +\n",
    "                             \" Param Directory: \" +\n",
    "                             str(os.path.abspath(currDir)) + \"\\n\")\n",
    "\n",
    "        elif (self.bonsaiObj.isRegression == False):\n",
    "            print(\"For Classification, Maximum Test accuracy at compressed\" +\n",
    "                  \" model size(including early stopping): \" +\n",
    "                  str(maxTestAcc) + \" at Epoch: \" +\n",
    "                  str(maxTestAccEpoch + 1) + \"\\nFinal Test\" +\n",
    "                  \" Accuracy: \" + str(testAcc), file=self.outFile)\n",
    "\n",
    "            resultFile.write(\"MaxTestAcc: \" + str(maxTestAcc) +\n",
    "                             \" at Epoch(totalEpochs): \" +\n",
    "                             str(maxTestAccEpoch + 1) +\n",
    "                             \"(\" + str(totalEpochs) + \")\" + \" ModelSize: \" +\n",
    "                             str(float(self.getModelSize()[1]) / 1024.0) +\n",
    "                             \" KB hasSparse: \" + str(self.getModelSize()[2]) +\n",
    "                             \" Param Directory: \" +\n",
    "                             str(os.path.abspath(currDir)) + \"\\n\")\n",
    "        print(\"The Model Directory: \" + currDir + \"\\n\")\n",
    "\n",
    "        resultFile.close()\n",
    "        self.outFile.flush()\n",
    "\n",
    "        if self.outFile is not sys.stdout:\n",
    "            self.outFile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r_CSgAOxJU8z"
   },
   "source": [
    "# Obtain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i_QU1xakLjTm"
   },
   "outputs": [],
   "source": [
    "\n",
    "# os.chdir(\"/home/iot/Documents/dataset_fog_release/dataset/ML\")\n",
    "# x_train = pd.read_csv(\"final_x.csv\")\n",
    "# x_train.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "# y_list = pd.read_csv(\"final_y.csv\")\n",
    "# y_list.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "# y_list = y_list-1\n",
    "# x =pd.concat([x_train,y_list],axis=1)\n",
    "# x= x[['0','mean1', 'mean2', 'mean9', 'rms2', 'abovemn2', 'belowmn2', 'soc4',\n",
    "#        'PSD2', 'FI1', 'FI2', 'FI3', 'FI5', 'FI9', 'FImag2']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 830,
     "status": "ok",
     "timestamp": 1567659429748,
     "user": {
      "displayName": "Gokul Hari",
      "photoUrl": "",
      "userId": "16159457985484250305"
     },
     "user_tz": -330
    },
    "id": "27pKEISHHWEK",
    "outputId": "ca747bfd-360b-4dbb-9f8b-b242fd01f213"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Dimension:  424\n",
      "Num classes:  2\n"
     ]
    }
   ],
   "source": [
    "#Loading and Pre-processing dataset for Bonsai\n",
    "dataDir = r\"E:\\programming\\practice\\research\\bonsai\\experiments\"\n",
    "(dataDimension, numClasses, Xtrain, Ytrain, Xtest, Ytest, mean, std) = helpermethods.preProcessData(dataDir, isRegression=False)\n",
    "print(\"Feature Dimension: \", dataDimension)\n",
    "print(\"Num classes: \", numClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2_ZfsH8GHWEP"
   },
   "source": [
    "# Model Parameters\n",
    "\n",
    "Note that Bonsai is designed for low-memory setting and the best results are obtained when operating in that setting. Use the sparsity, projection dimension and tree depth to vary the model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ZMc-pm_HWEU"
   },
   "outputs": [],
   "source": [
    "sigma = 1.0 #Sigmoid parameter for tanh\n",
    "depth = 10 #Depth of Bonsai Tree\n",
    "projectionDimension = 48 #Lower Dimensional space for Bonsai to work on\n",
    "\n",
    "#Regularizers for Bonsai Parameters\n",
    "regZ = 0.0001\n",
    "regW = 0.001\n",
    "regV = 0.001\n",
    "regT = 0.001\n",
    "\n",
    "totalEpochs = 100\n",
    "\n",
    "learningRate = 0.01\n",
    "\n",
    "outFile = None\n",
    "\n",
    "#Sparsity for Bonsai Parameters. x => 100*x % are non-zeros\n",
    "sparZ = 0.2\n",
    "sparW = 0.3\n",
    "sparV = 0.3\n",
    "sparT = 0.62\n",
    "\n",
    "batchSize = np.maximum(100, int(np.ceil(np.sqrt(Ytrain.shape[0]))))\n",
    "\n",
    "useMCHLoss = True #only for Multiclass cases True: Multiclass-Hing Loss, False: Cross Entropy. \n",
    "\n",
    "#Bonsai uses one classier for Binary, thus this condition\n",
    "if numClasses == 2:\n",
    "    numClasses = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ob350Z_jHWEZ"
   },
   "source": [
    "Placeholders for Data feeding during training and infernece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-cDEwJXLHWEa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nadit\\AppData\\Local\\Temp\\ipykernel_35224\\1897015113.py:1: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(\"float32\", [None, dataDimension])\n",
    "Y = tf.placeholder(\"float32\", [None, numClasses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g9AWWX7SHWEh"
   },
   "source": [
    "Creating a directory for current model in the datadirectory using timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QekJmbhZHWEk"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = r\"E:\\programming\\practice\\research\\bonsai\\experiments\"\n",
    "currDir = createTimeStampDir(DATA_DIR)\n",
    "dumpCommand(sys.argv, currDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Si3zazw0HWEo"
   },
   "source": [
    "# Bonsai Graph Object\n",
    "\n",
    "Instantiating the Bonsai Graph which will be used for training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X7336O8mHWEq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nadit\\AppData\\Local\\Temp\\ipykernel_35224\\868221364.py:67: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bonsaiObj = Bonsai(numClasses, dataDimension, projectionDimension, depth, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qCqKPxJVHWEv"
   },
   "source": [
    "# Bonsai Trainer Object\n",
    "\n",
    "Instantiating the Bonsai Trainer which will be used for 3 phase training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TqjoQ9-lHWEw"
   },
   "outputs": [],
   "source": [
    "bonsaiTrainer = BonsaiTrainer(bonsaiObj, regW, regT, regV, regZ, sparW, sparT, sparV, sparZ,\n",
    "                              learningRate, X, Y, useMCHLoss, outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0b1xQdpdHWE4"
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9HtKGhLLHWE9"
   },
   "source": [
    "# Bonsai Training Routine\n",
    "\n",
    "The method to to run the 3 phase training, followed by giving out the best early stopping model, accuracy along with saving of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "tDlywVhoHWE-",
    "outputId": "0e248cbe-efa8-43d9-80e6-3d6ba57fc17b"
   },
   "outputs": [],
   "source": [
    "bonsaiTrainer.train(batchSize, totalEpochs, sess,\n",
    "                    Xtrain, Xtest, Ytrain, Ytest, DATA_DIR, currDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HvAmO9DpHWFE"
   },
   "outputs": [],
   "source": [
    "pred = bonsaiTrainer.pred1\n",
    "y_pred=[]\n",
    "for i in pred:\n",
    "    if i == 0:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xEXln5MsM7or"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "# Ytest = np.argmax(Ytest,axis=1)\n",
    "print (confusion_matrix(Ytest,y_pred))\n",
    "print (classification_report(Ytest,y_pred,digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "icMCp5EQRfwR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "BonsaiNotebook.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
